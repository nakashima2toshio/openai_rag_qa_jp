# python a42_qdrant_registration.py --recreate --include-answer

#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
a42_qdrant_registration.py â€” 3ã¤ã®ç•°ãªã‚‹ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã¸ã®åˆ†é›¢ç™»éŒ²ç‰ˆ
--------------------------------------------------------------------------------
cc_newsãƒ‰ãƒ¡ã‚¤ãƒ³ã®Q&Aãƒ‡ãƒ¼ã‚¿ã‚’ã€ç”Ÿæˆæ–¹æ³•ã”ã¨ã«åˆ¥ã€…ã®ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã«ç™»éŒ²ã—ã¾ã™ã€‚

ã€ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³æ§‹æˆã€‘
- qa_cc_news_a02_llm    : a02_qa_pairs_cc_news.csv (LLMç”Ÿæˆæ–¹å¼)
- qa_cc_news_a03_rule   : a03_qa_pairs_cc_news.csv (ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹ç”Ÿæˆæ–¹å¼)
- qa_cc_news_a10_hybrid : a10_qa_pairs_cc_news.csv (ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ç”Ÿæˆæ–¹å¼)

ã€ä¸»ãªå¤‰æ›´ç‚¹ï¼ˆa30ã¨ã®é•ã„ï¼‰ã€‘
- å˜ä¸€ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ â†’ 3ã¤ã®ç‹¬ç«‹ã—ãŸã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³
- å„CSVãƒ•ã‚¡ã‚¤ãƒ«ãŒå°‚ç”¨ã®ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã‚’æŒã¤
- domainã¨generation_methodãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’ä¿æŒï¼ˆäº’æ›æ€§ã®ãŸã‚ï¼‰

ä½¿ã„æ–¹ï¼š
  # 1. CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç”Ÿæˆï¼ˆa20_output_qa_csv.pyã‚’å®Ÿè¡Œï¼‰
  python a20_output_qa_csv.py

  # 2. Qdrantã‚µãƒ¼ãƒãƒ¼ã‚’èµ·å‹•
  export OPENAI_API_KEY=sk-...
  docker run -p 6333:6333 -p 6334:6334 qdrant/qdrant

  # -------------------------------------------------------------
  # 3. 3ã¤ã®ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã«ãƒ‡ãƒ¼ã‚¿ã‚’ç™»éŒ²
  python a42_qdrant_registration.py --recreate --include-answer
  # -------------------------------------------------------------

  # 4. ç‰¹å®šã®ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã®ã¿ç™»éŒ²
  python a42_qdrant_registration.py --collection qa_cc_news_a02_llm --recreate

  # 5. æ¤œç´¢ãƒ†ã‚¹ãƒˆï¼ˆç‰¹å®šã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ï¼‰
  python a42_qdrant_registration.py --search "æ°—å€™å¤‰å‹•" --collection qa_cc_news_a02_llm

ä¸»è¦å¼•æ•°ï¼š
  --recreate          : ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³å‰Šé™¤â†’æ–°è¦ä½œæˆ
  --collection        : ç‰¹å®šã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã®ã¿å‡¦ç†ï¼ˆæŒ‡å®šãªã—ã§å…¨ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ï¼‰
  --qdrant-url        : æ—¢å®šã¯ http://localhost:6333
  --batch-size        : Embeddings/Upsert ãƒãƒƒãƒã‚µã‚¤ã‚ºï¼ˆæ—¢å®š 32ï¼‰
  --limit             : ãƒ‡ãƒ¼ã‚¿ä»¶æ•°ä¸Šé™ï¼ˆé–‹ç™ºç”¨ã€0=ç„¡åˆ¶é™ï¼‰
  --include-answer    : åŸ‹ã‚è¾¼ã¿å…¥åŠ›ã« answer ã‚‚çµåˆï¼ˆquestion + "\\n" + answerï¼‰
  --search            : ã‚¯ã‚¨ãƒªæŒ‡å®šã§æ¤œç´¢ã®ã¿å®Ÿè¡Œ
  --topk              : ä¸Šä½ä»¶æ•°ï¼ˆæ—¢å®š5ï¼‰
"""
import argparse
import os
import json
import glob
from datetime import datetime, timezone
from typing import Dict, Iterable, List, Tuple, Optional, Any
from pathlib import Path

import pandas as pd

try:
    import yaml  # PyYAML
except Exception:
    yaml = None

try:
    import helper_api as hapi
except Exception:
    hapi = None

try:
    import helper_rag as hrag
except Exception:
    hrag = None

from qdrant_client import QdrantClient
from qdrant_client.http import models
from openai import OpenAI

# ------------------ ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®š ------------------
DEFAULTS = {
    "rag": {
        "include_answer_in_embedding": False,
    },
    "embeddings": {
        "primary": {"provider": "openai", "model": "text-embedding-3-small", "dims": 1536},
    },
    "qdrant": {"url": "http://localhost:6333"},
}

# ------------------ ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³å®šç¾© ------------------
# CSVãƒ•ã‚¡ã‚¤ãƒ«/TXTãƒ•ã‚¡ã‚¤ãƒ« â†’ ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³å â†’ ç”Ÿæˆæ–¹æ³•ã®ãƒãƒƒãƒ”ãƒ³ã‚°
COLLECTION_MAPPINGS = [
    {
        "csv_file": "qa_output/a02_qa_pairs_cc_news.csv",
        "collection": "qa_cc_news_a02_llm",
        "generation_method": "a02_make_qa",
        "domain": "cc_news",
        "description": "LLMç”Ÿæˆæ–¹å¼ï¼ˆa02_make_qa.pyï¼‰",
        "type": "qa"
    },
    {
        "csv_file": "qa_output/a03_qa_pairs_cc_news.csv",
        "collection": "qa_cc_news_a03_rule",
        "generation_method": "a03_coverage",
        "domain": "cc_news",
        "description": "ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹ç”Ÿæˆæ–¹å¼ï¼ˆa03_rag_qa_coverage_improved.pyï¼‰",
        "type": "qa"
    },
    {
        "csv_file": "qa_output/a10_qa_pairs_cc_news.csv",
        "collection": "qa_cc_news_a10_hybrid",
        "generation_method": "a10_hybrid",
        "domain": "cc_news",
        "description": "ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ç”Ÿæˆæ–¹å¼ï¼ˆa10_qa_optimized_hybrid_batch.pyï¼‰",
        "type": "qa"
    },
    {
        "csv_file": "qa_output/a02_qa_pairs_livedoor.csv",
        "collection": "qa_livedoor_a02_20_llm",
        "generation_method": "a02_make_qa",
        "domain": "livedoor",
        "description": "LLMç”Ÿæˆæ–¹å¼ï¼ˆa02_make_qa.pyï¼‰- livedoorãƒ‡ãƒ¼ã‚¿",
        "type": "qa"
    },
    {
        "csv_file": "qa_output/a03_qa_pairs_livedoor.csv",
        "collection": "qa_livedoor_a03_rule",
        "generation_method": "a03_coverage",
        "domain": "livedoor",
        "description": "ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹ç”Ÿæˆæ–¹å¼ï¼ˆa03_rag_qa_coverage_improved.pyï¼‰- livedoorãƒ‡ãƒ¼ã‚¿",
        "type": "qa"
    },
    {
        "csv_file": "qa_output/a10_qa_pairs_livedoor.csv",
        "collection": "qa_livedoor_a10_hybrid",
        "generation_method": "a10_hybrid",
        "domain": "livedoor",
        "description": "ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ç”Ÿæˆæ–¹å¼ï¼ˆa10_qa_optimized_hybrid_batch.pyï¼‰- livedoorãƒ‡ãƒ¼ã‚¿",
        "type": "qa"
    },
    {
        "csv_file": "OUTPUT/cc_news.txt",
        "collection": "raw_cc_news",
        "generation_method": "raw_text",
        "domain": "cc_news",
        "description": "ç”Ÿãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ï¼ˆcc_newsï¼‰",
        "type": "raw"
    },
    {
        "csv_file": "OUTPUT/livedoor.txt",
        "collection": "raw_livedoor",
        "generation_method": "raw_text",
        "domain": "livedoor",
        "description": "ç”Ÿãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ï¼ˆlivedoorï¼‰",
        "type": "raw"
    }
]


# ------------------ è¨­å®šãƒ­ãƒ¼ãƒ‰ ------------------
def load_config(path: str = "config.yml") -> Dict[str, Any]:
    cfg = {}
    if yaml and os.path.exists(path):
        with open(path, "r", encoding="utf-8") as f:
            cfg = yaml.safe_load(f) or {}
    # ãƒãƒ¼ã‚¸ï¼ˆæµ…ã„ãƒãƒ¼ã‚¸ã§ååˆ†ã€‚å¿…è¦ãªã‚‰æ·±ã„ãƒãƒ¼ã‚¸ã«å·®ã—æ›¿ãˆï¼‰
    def merge(dst, src):
        for k, v in src.items():
            if isinstance(v, dict) and isinstance(dst.get(k), dict):
                merge(dst[k], v)
            else:
                dst.setdefault(k, v)
    full = {}
    merge(full, DEFAULTS)
    merge(full, cfg)
    return full

# ------------------ å°é“å…· ------------------
def batched(seq: Iterable, size: int):
    buf = []
    for x in seq:
        buf.append(x)
        if len(buf) >= size:
            yield buf
            buf = []
    if buf:
        yield buf

# ------------------ OpenAIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ ------------------
def get_openai_client():
    if hapi and hasattr(hapi, "get_openai_client"):
        return hapi.get_openai_client()
    return OpenAI()

# ------------------ åŸ‹ã‚è¾¼ã¿å®Ÿè£…ï¼ˆhelperå„ªå…ˆï¼‰ ------------------
def embed_texts_openai(texts: List[str], model: str, client: Optional[OpenAI] = None) -> List[List[float]]:
    client = client or get_openai_client()
    resp = client.embeddings.create(model=model, input=texts)
    return [d.embedding for d in resp.data]

def embed_texts(texts: List[str], model: str, batch_size: int = 128) -> List[List[float]]:
    """
    ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒãƒƒãƒå‡¦ç†ã§Embeddingã«å¤‰æ›
    OpenAIã®8192ãƒˆãƒ¼ã‚¯ãƒ³åˆ¶é™ã‚’è€ƒæ…®ã—ã¦å‹•çš„ã«ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’èª¿æ•´

    Note: batch_sizeå¼•æ•°ã¯å¾Œæ–¹äº’æ›æ€§ã®ãŸã‚ã«æ®‹ã—ã¦ã„ã‚‹ãŒã€å®Ÿéš›ã«ã¯ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã®ã¿ã§åˆ¶å¾¡
    """
    if hrag and hasattr(hrag, "embed_texts"):
        return hrag.embed_texts(texts, model=model, batch_size=batch_size)

    import tiktoken

    enc = tiktoken.get_encoding("cl100k_base")
    client = get_openai_client()

    # OpenAI Embeddingã®æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³æ•°ï¼ˆå®‰å…¨ãƒãƒ¼ã‚¸ãƒ³è¾¼ã¿ï¼‰
    MAX_TOKENS_PER_REQUEST = 8000  # 8192ã‹ã‚‰ä½™è£•ã‚’æŒãŸã›ã¦8000

    # â˜… ç©ºæ–‡å­—åˆ—ãƒ»ç©ºç™½ã®ã¿ã®æ–‡å­—åˆ—ã‚’é™¤å¤–ã—ã€ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãƒãƒƒãƒ”ãƒ³ã‚°ã‚’ä¿æŒ
    valid_texts = []
    valid_indices = []
    for i, text in enumerate(texts):
        if text and text.strip():  # ç©ºæ–‡å­—åˆ—ã¨ç©ºç™½ã®ã¿ã‚’é™¤å¤–
            valid_texts.append(text)
            valid_indices.append(i)

    # å…¨ã¦ç©ºæ–‡å­—åˆ—ã®å ´åˆã¯ãƒ€ãƒŸãƒ¼ãƒ™ã‚¯ãƒˆãƒ«ã‚’è¿”ã™
    if not valid_texts:
        print(f"\r   [WARN] å…¨ã¦ã®ãƒ†ã‚­ã‚¹ãƒˆãŒç©ºæ–‡å­—åˆ—ã§ã™ã€‚ãƒ€ãƒŸãƒ¼ãƒ™ã‚¯ãƒˆãƒ«ã‚’è¿”ã—ã¾ã™ã€‚", flush=True)
        return [[0.0] * 1536] * len(texts)

    # æœ‰åŠ¹ãªãƒ†ã‚­ã‚¹ãƒˆã®ã¿ã§åŸ‹ã‚è¾¼ã¿ç”Ÿæˆ
    valid_vecs: List[List[float]] = []
    current_batch = []
    current_tokens = 0
    batch_count = 0

    for i, text in enumerate(valid_texts):
        text_tokens = len(enc.encode(text))

        # å˜ä¸€ãƒãƒ£ãƒ³ã‚¯ãŒåˆ¶é™ã‚’è¶…ãˆã‚‹å ´åˆã¯ã‚¨ãƒ©ãƒ¼
        if text_tokens > MAX_TOKENS_PER_REQUEST:
            raise ValueError(
                f"Single text at index {valid_indices[i]} has {text_tokens} tokens, "
                f"which exceeds MAX_TOKENS_PER_REQUEST ({MAX_TOKENS_PER_REQUEST}). "
                f"Please reduce chunk_size when creating chunks."
            )

        # è¿½åŠ å‰ã«ãƒã‚§ãƒƒã‚¯ï¼šåˆ¶é™ã‚’è¶…ãˆã‚‹å ´åˆã¯å…ˆã«ãƒãƒƒãƒã‚’é€ä¿¡
        if current_tokens + text_tokens > MAX_TOKENS_PER_REQUEST:
            # ç¾åœ¨ã®ãƒãƒƒãƒãŒã‚ã‚Œã°é€ä¿¡
            if current_batch:
                batch_count += 1
                print(f"\r   åŸ‹ã‚è¾¼ã¿ç”Ÿæˆä¸­: ãƒãƒƒãƒ{batch_count} ({len(current_batch)}ä»¶, {current_tokens}ãƒˆãƒ¼ã‚¯ãƒ³)... ", end="", flush=True)
                valid_vecs.extend(embed_texts_openai(current_batch, model=model, client=client))
                current_batch = []
                current_tokens = 0

        # ãƒãƒƒãƒã«è¿½åŠ 
        current_batch.append(text)
        current_tokens += text_tokens

    # æ®‹ã‚Šã®ãƒãƒƒãƒã‚’å‡¦ç†
    if current_batch:
        batch_count += 1
        print(f"\r   åŸ‹ã‚è¾¼ã¿ç”Ÿæˆä¸­: ãƒãƒƒãƒ{batch_count} ({len(current_batch)}ä»¶, {current_tokens}ãƒˆãƒ¼ã‚¯ãƒ³)... ", end="", flush=True)
        valid_vecs.extend(embed_texts_openai(current_batch, model=model, client=client))

    # å…ƒã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã«åˆã‚ã›ã¦ãƒ™ã‚¯ãƒˆãƒ«ã‚’å†é…ç½®ï¼ˆç©ºæ–‡å­—åˆ—ã¯ã‚¼ãƒ­ãƒ™ã‚¯ãƒˆãƒ«ï¼‰
    vecs: List[List[float]] = []
    valid_vec_idx = 0
    for i in range(len(texts)):
        if i in valid_indices:
            vecs.append(valid_vecs[valid_vec_idx])
            valid_vec_idx += 1
        else:
            # ç©ºæ–‡å­—åˆ—ã®å ´åˆã¯ã‚¼ãƒ­ãƒ™ã‚¯ãƒˆãƒ«
            vecs.append([0.0] * 1536)

    return vecs

# ------------------ å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆæ§‹ç¯‰ ------------------
def build_inputs(df: pd.DataFrame, include_answer: bool) -> List[str]:
    if include_answer:
        return (df["question"].astype(str) + "\n" + df["answer"].astype(str)).tolist()
    return df["question"].astype(str).tolist()

# ------------------ CSVãƒ­ãƒ¼ãƒ‰ ------------------
def load_csv(path: str, required=("question", "answer"), limit: int = 0) -> pd.DataFrame:
    if not os.path.exists(path):
        raise FileNotFoundError(f"CSV not found: {path}")
    df = pd.read_csv(path)
    # åˆ—åãƒãƒƒãƒ”ãƒ³ã‚°ãŒå¿…è¦ãªã‚‰ã“ã“ã§èª¿æ•´ï¼ˆä¾‹: 'Question'->'question'ï¼‰
    column_mappings = {
        'Question': 'question',
        'Response': 'answer',
        'Answer': 'answer',
        'correct_answer': 'answer'
    }
    df = df.rename(columns=column_mappings)
    for col in required:
        if col not in df.columns:
            raise ValueError(f"{path} ã«ã¯ '{col}' åˆ—ãŒå¿…è¦ã§ã™ï¼ˆåˆ—: {list(df.columns)}ï¼‰")
    df = df.fillna("").drop_duplicates(subset=list(required)).reset_index(drop=True)
    if limit and limit > 0:
        df = df.head(limit).copy()
    return df

# ------------------ TXTãƒ­ãƒ¼ãƒ‰ï¼ˆç”Ÿãƒ†ã‚­ã‚¹ãƒˆï¼‰ ------------------
def detect_language(text: str) -> str:
    """
    ãƒ†ã‚­ã‚¹ãƒˆã®è¨€èªã‚’åˆ¤å®šï¼ˆæ—¥æœ¬èª or è‹±èªï¼‰

    Args:
        text: åˆ¤å®šã™ã‚‹ãƒ†ã‚­ã‚¹ãƒˆ

    Returns:
        "ja" (æ—¥æœ¬èª) or "en" (è‹±èª)
    """
    # æ—¥æœ¬èªæ–‡å­—ï¼ˆã²ã‚‰ãŒãªã€ã‚«ã‚¿ã‚«ãƒŠã€æ¼¢å­—ï¼‰ã®æ¤œå‡º
    japanese_chars = len([c for c in text if '\u3040' <= c <= '\u309F' or  # ã²ã‚‰ãŒãª
                                            '\u30A0' <= c <= '\u30FF' or  # ã‚«ã‚¿ã‚«ãƒŠ
                                            '\u4E00' <= c <= '\u9FFF'])    # æ¼¢å­—

    total_chars = len([c for c in text if not c.isspace()])

    # æ—¥æœ¬èªæ–‡å­—ãŒ30%ä»¥ä¸Šãªã‚‰æ—¥æœ¬èªã¨åˆ¤å®š
    if total_chars > 0 and japanese_chars / total_chars > 0.3:
        return "ja"
    return "en"

def chunk_japanese_text(text: str, max_tokens: int = 500) -> List[str]:
    """
    æ—¥æœ¬èªãƒ†ã‚­ã‚¹ãƒˆã‚’ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯ã«ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²
    æ®µè½â†’æ–‡å˜ä½ã§æ„å‘³ã‚’ä¿æŒã—ãªãŒã‚‰åˆ†å‰²
    å˜ä¸€æ–‡ãŒå¤§ãã™ãã‚‹å ´åˆã¯å¼·åˆ¶çš„ã«å›ºå®šé•·åˆ†å‰²

    Args:
        text: åˆ†å‰²ã™ã‚‹æ—¥æœ¬èªãƒ†ã‚­ã‚¹ãƒˆ
        max_tokens: ãƒãƒ£ãƒ³ã‚¯ã®æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³æ•°

    Returns:
        ãƒãƒ£ãƒ³ã‚¯ã®ãƒªã‚¹ãƒˆ
    """
    import tiktoken
    import re

    enc = tiktoken.get_encoding("cl100k_base")

    # æ®µè½ã§åˆ†å‰²ï¼ˆç©ºè¡ŒåŒºåˆ‡ã‚Šï¼‰
    paragraphs = re.split(r'\n\s*\n', text)

    chunks = []
    current_chunk = ""
    current_tokens = 0

    for para in paragraphs:
        para = para.strip()
        if not para:
            continue

        para_tokens = len(enc.encode(para))

        # ç¾åœ¨ã®ãƒãƒ£ãƒ³ã‚¯ã«è¿½åŠ ã§ãã‚‹ã‹ç¢ºèª
        if current_tokens + para_tokens <= max_tokens:
            if current_chunk:
                current_chunk += "\n\n"
            current_chunk += para
            current_tokens += para_tokens
        else:
            # ç¾åœ¨ã®ãƒãƒ£ãƒ³ã‚¯ã‚’ä¿å­˜
            if current_chunk:
                chunks.append(current_chunk)

            # æ®µè½ãŒå¤§ãã™ãã‚‹å ´åˆã¯æ–‡å˜ä½ã§åˆ†å‰²
            if para_tokens > max_tokens:
                # æ—¥æœ¬èªã®å¥ç‚¹ã§æ–‡åˆ†å‰²
                sentences = re.split(r'([ã€‚ï¼ï¼Ÿ\n])', para)
                temp_chunk = ""
                temp_tokens = 0

                for i in range(0, len(sentences), 2):
                    sent = sentences[i]
                    if i + 1 < len(sentences):
                        sent += sentences[i + 1]

                    sent_tokens = len(enc.encode(sent))

                    # å˜ä¸€æ–‡ãŒå¤§ãã™ãã‚‹å ´åˆã¯å¼·åˆ¶åˆ†å‰²
                    if sent_tokens > max_tokens:
                        # ç¾åœ¨ã®temp_chunkã‚’ä¿å­˜
                        if temp_chunk:
                            chunks.append(temp_chunk.strip())
                            temp_chunk = ""
                            temp_tokens = 0

                        # é•·æ–‡ã‚’å›ºå®šé•·ã§å¼·åˆ¶åˆ†å‰²
                        sent_enc = enc.encode(sent)
                        for chunk_start in range(0, len(sent_enc), max_tokens):
                            chunk_tokens = sent_enc[chunk_start:chunk_start + max_tokens]
                            chunk_text = enc.decode(chunk_tokens).strip()
                            # ç©ºæ–‡å­—åˆ—ãƒã‚§ãƒƒã‚¯
                            if chunk_text:
                                chunks.append(chunk_text)
                    elif temp_tokens + sent_tokens <= max_tokens:
                        temp_chunk += sent
                        temp_tokens += sent_tokens
                    else:
                        if temp_chunk:
                            chunks.append(temp_chunk.strip())
                        temp_chunk = sent
                        temp_tokens = sent_tokens

                if temp_chunk:
                    current_chunk = temp_chunk
                    current_tokens = temp_tokens
                else:
                    current_chunk = ""
                    current_tokens = 0
            else:
                current_chunk = para
                current_tokens = para_tokens

    # æœ€å¾Œã®ãƒãƒ£ãƒ³ã‚¯
    if current_chunk:
        chunks.append(current_chunk)

    return chunks

def chunk_english_text(text: str, max_tokens: int = 500) -> List[str]:
    """
    è‹±èªãƒ†ã‚­ã‚¹ãƒˆã‚’ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯ã«ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²
    æ®µè½â†’æ–‡å˜ä½ã§æ„å‘³ã‚’ä¿æŒã—ãªãŒã‚‰åˆ†å‰²
    å˜ä¸€æ–‡ãŒå¤§ãã™ãã‚‹å ´åˆã¯å¼·åˆ¶çš„ã«å›ºå®šé•·åˆ†å‰²

    Args:
        text: åˆ†å‰²ã™ã‚‹è‹±èªãƒ†ã‚­ã‚¹ãƒˆ
        max_tokens: ãƒãƒ£ãƒ³ã‚¯ã®æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³æ•°

    Returns:
        ãƒãƒ£ãƒ³ã‚¯ã®ãƒªã‚¹ãƒˆ
    """
    import tiktoken
    import re

    enc = tiktoken.get_encoding("cl100k_base")

    # æ®µè½ã§åˆ†å‰²ï¼ˆç©ºè¡ŒåŒºåˆ‡ã‚Šï¼‰
    paragraphs = re.split(r'\n\s*\n', text)

    chunks = []
    current_chunk = ""
    current_tokens = 0

    for para in paragraphs:
        para = para.strip()
        if not para:
            continue

        para_tokens = len(enc.encode(para))

        # ç¾åœ¨ã®ãƒãƒ£ãƒ³ã‚¯ã«è¿½åŠ ã§ãã‚‹ã‹ç¢ºèª
        if current_tokens + para_tokens <= max_tokens:
            if current_chunk:
                current_chunk += "\n\n"
            current_chunk += para
            current_tokens += para_tokens
        else:
            # ç¾åœ¨ã®ãƒãƒ£ãƒ³ã‚¯ã‚’ä¿å­˜
            if current_chunk:
                chunks.append(current_chunk)

            # æ®µè½ãŒå¤§ãã™ãã‚‹å ´åˆã¯æ–‡å˜ä½ã§åˆ†å‰²
            if para_tokens > max_tokens:
                # è‹±èªã®æ–‡åˆ†å‰²ï¼ˆãƒ”ãƒªã‚ªãƒ‰+ã‚¹ãƒšãƒ¼ã‚¹+å¤§æ–‡å­—ã§åˆ†å‰²ã€ç•¥èªã‚’è€ƒæ…®ï¼‰
                sentences = re.split(r'(?<=[.!?])\s+(?=[A-Z])', para)
                temp_chunk = ""
                temp_tokens = 0

                for sent in sentences:
                    sent_tokens = len(enc.encode(sent))

                    # å˜ä¸€æ–‡ãŒå¤§ãã™ãã‚‹å ´åˆã¯å¼·åˆ¶åˆ†å‰²
                    if sent_tokens > max_tokens:
                        # ç¾åœ¨ã®temp_chunkã‚’ä¿å­˜
                        if temp_chunk:
                            chunks.append(temp_chunk.strip())
                            temp_chunk = ""
                            temp_tokens = 0

                        # é•·æ–‡ã‚’å›ºå®šé•·ã§å¼·åˆ¶åˆ†å‰²
                        sent_enc = enc.encode(sent)
                        for chunk_start in range(0, len(sent_enc), max_tokens):
                            chunk_tokens = sent_enc[chunk_start:chunk_start + max_tokens]
                            chunk_text = enc.decode(chunk_tokens).strip()
                            # ç©ºæ–‡å­—åˆ—ãƒã‚§ãƒƒã‚¯
                            if chunk_text:
                                chunks.append(chunk_text)
                    elif temp_tokens + sent_tokens <= max_tokens:
                        if temp_chunk:
                            temp_chunk += " "
                        temp_chunk += sent
                        temp_tokens += sent_tokens
                    else:
                        if temp_chunk:
                            chunks.append(temp_chunk.strip())
                        temp_chunk = sent
                        temp_tokens = sent_tokens

                if temp_chunk:
                    current_chunk = temp_chunk
                    current_tokens = temp_tokens
                else:
                    current_chunk = ""
                    current_tokens = 0
            else:
                current_chunk = para
                current_tokens = para_tokens

    # æœ€å¾Œã®ãƒãƒ£ãƒ³ã‚¯
    if current_chunk:
        chunks.append(current_chunk)

    return chunks

def load_txt(path: str, limit: int = 0, chunk_size: int = 500) -> pd.DataFrame:
    """
    ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿ã€ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯ãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰²ã—ã¦DataFrameã«å¤‰æ›
    è¨€èªã‚’è‡ªå‹•åˆ¤å®šã—ã€é©åˆ‡ãªæ–¹æ³•ã§ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²ã™ã‚‹

    Args:
        path: ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
        limit: ãƒãƒ£ãƒ³ã‚¯æ•°ã®ä¸Šé™ï¼ˆ0=ç„¡åˆ¶é™ï¼‰
        chunk_size: ãƒãƒ£ãƒ³ã‚¯ã®æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³æ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ500ï¼‰

    Returns:
        ãƒãƒ£ãƒ³ã‚¯ã‚’å«ã‚€DataFrame
    """
    if not os.path.exists(path):
        raise FileNotFoundError(f"TXT not found: {path}")

    with open(path, "r", encoding="utf-8") as f:
        full_text = f.read()

    # è¨€èªåˆ¤å®š
    lang = detect_language(full_text[:1000])  # å…ˆé ­1000æ–‡å­—ã§åˆ¤å®š
    print(f"   è¨€èªåˆ¤å®š: {'æ—¥æœ¬èª' if lang == 'ja' else 'è‹±èª'}")

    # è¨€èªã«å¿œã˜ãŸãƒãƒ£ãƒ³ã‚¯åˆ†å‰²
    if lang == "ja":
        chunks = chunk_japanese_text(full_text, max_tokens=chunk_size)
    else:
        chunks = chunk_english_text(full_text, max_tokens=chunk_size)

    print(f"   ãƒãƒ£ãƒ³ã‚¯æ•°: {len(chunks):,}ä»¶ï¼ˆæœ€å¤§{chunk_size}ãƒˆãƒ¼ã‚¯ãƒ³/ãƒãƒ£ãƒ³ã‚¯ï¼‰")

    if limit and limit > 0:
        chunks = chunks[:limit]
        print(f"   åˆ¶é™é©ç”¨å¾Œ: {len(chunks):,}ä»¶")

    # DataFrameã«å¤‰æ›ï¼ˆtextã‚«ãƒ©ãƒ ã®ã¿ï¼‰
    df = pd.DataFrame({"text": chunks})
    return df

# ------------------ Qdrant: ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ä½œæˆï¼ˆNamed Vectorså¯¾å¿œï¼‰ ------------------
def create_or_recreate_collection(client: QdrantClient, name: str, recreate: bool,
                                  embeddings_cfg: Dict[str, Dict[str, Any]]):
    # embeddings_cfg: dict[name] = {"model": "...", "dims": int}
    # Named Vectorsï¼šè¤‡æ•°ã‚­ãƒ¼ãªã‚‰ dict ã‚’ã€å˜ä¸€ãªã‚‰ VectorParams ã‚’ä½¿ã†
    if len(embeddings_cfg) == 1:
        dims = list(embeddings_cfg.values())[0]["dims"]
        vectors_config = models.VectorParams(size=dims, distance=models.Distance.COSINE)
    else:
        # Named vectors
        vectors_config = {
            k: models.VectorParams(size=v["dims"], distance=models.Distance.COSINE)
            for k, v in embeddings_cfg.items()
        }
    if recreate:
        client.recreate_collection(collection_name=name, vectors_config=vectors_config)
    else:
        # ç„¡ã‘ã‚Œã°ä½œæˆ
        try:
            client.get_collection(name)
        except Exception:
            client.create_collection(collection_name=name, vectors_config=vectors_config)
    # ã‚ˆãä½¿ã†payloadã®ç´¢å¼•ï¼ˆä»»æ„ï¼‰
    try:
        client.create_payload_index(name, field_name="domain", field_schema=models.PayloadSchemaType.KEYWORD)
    except Exception:
        pass
    try:
        client.create_payload_index(name, field_name="generation_method", field_schema=models.PayloadSchemaType.KEYWORD)
    except Exception:
        pass

# ------------------ ãƒã‚¤ãƒ³ãƒˆæ§‹ç¯‰ï¼ˆNamed Vectorså¯¾å¿œï¼‰ ------------------
def build_points(df: pd.DataFrame, vectors_by_name: Dict[str, List[List[float]]], domain: str, source_file: str,
                 generation_method: str = None, data_type: str = "qa") -> List[models.PointStruct]:
    # vectors_by_name: name -> list[vec]
    n = len(df)
    for name, vecs in vectors_by_name.items():
        if len(vecs) != n:
            raise ValueError(f"vectors length mismatch for '{name}': df={n}, vecs={len(vecs)}")
    now_iso = datetime.now(timezone.utc).isoformat()
    points: List[models.PointStruct] = []

    for i, row in enumerate(df.itertuples(index=False)):
        if data_type == "qa":
            # Q&Aãƒšã‚¢ç”¨ã®ãƒšã‚¤ãƒ­ãƒ¼ãƒ‰
            payload = {
                "domain": domain,
                "generation_method": generation_method or "unknown",
                "question": getattr(row, "question"),
                "answer": getattr(row, "answer"),
                "source": os.path.basename(source_file),
                "created_at": now_iso,
                "schema": "qa:v1",
            }
        else:  # raw text
            # ç”Ÿãƒ†ã‚­ã‚¹ãƒˆç”¨ã®ãƒšã‚¤ãƒ­ãƒ¼ãƒ‰
            payload = {
                "domain": domain,
                "generation_method": generation_method or "unknown",
                "text": getattr(row, "text"),
                "source": os.path.basename(source_file),
                "created_at": now_iso,
                "schema": "raw:v1",
            }

        # Qdrant requires point IDs to be UUID or unsigned integer
        # ã‚½ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«åã‚‚å«ã‚ã¦ãƒ¦ãƒ‹ãƒ¼ã‚¯ãªIDã‚’ç”Ÿæˆ
        pid = abs(hash(f"{domain}-{generation_method}-{source_file}-{i}")) & 0x7FFFFFFFFFFFFFFF  # Convert to positive 64-bit integer
        if len(vectors_by_name) == 1:
            # å˜ä¸€ãƒ™ã‚¯ãƒˆãƒ«
            vec = list(vectors_by_name.values())[0][i]
            points.append(models.PointStruct(id=pid, vector=vec, payload=payload))
        else:
            # Named Vectorsï¼ˆdictæ¸¡ã—ï¼‰
            vecs_dict = {name: vecs[i] for name, vecs in vectors_by_name.items()}
            points.append(models.PointStruct(id=pid, vector=vecs_dict, payload=payload))
    return points

def upsert_points(client: QdrantClient, collection: str, points: List[models.PointStruct], batch_size: int = 128) -> int:
    count = 0
    for chunk in batched(points, batch_size):
        client.upsert(collection_name=collection, points=chunk)
        count += len(chunk)
    return count

# ------------------ æ¤œç´¢ï¼ˆNamed Vectorså¯¾å¿œï¼‰ ------------------
def embed_one(text: str, model: str) -> List[float]:
    return embed_texts([text], model=model, batch_size=1)[0]

def search(client: QdrantClient, collection: str, query: str, using_vec: str, model_for_using: str,
           topk: int = 5, domain: Optional[str] = None, generation_method: Optional[str] = None):
    qvec = embed_one(query, model=model_for_using)
    qfilter = None
    filter_conditions = []
    if domain:
        filter_conditions.append(models.FieldCondition(key="domain", match=models.MatchValue(value=domain)))
    if generation_method:
        filter_conditions.append(models.FieldCondition(key="generation_method", match=models.MatchValue(value=generation_method)))
    if filter_conditions:
        qfilter = models.Filter(must=filter_conditions)
    
    # ãƒ™ã‚¯ãƒˆãƒ«è¨­å®šã‚’ç¢ºèªã—ã¦é©åˆ‡ãªæ¤œç´¢æ–¹æ³•ã‚’é¸æŠ
    try:
        collection_info = client.get_collection(collection)
        # Named Vectorsã‹ã©ã†ã‹ç¢ºèª
        has_named_vectors = hasattr(collection_info.config.params.vectors, '__iter__') and not isinstance(
            collection_info.config.params.vectors, (str, models.VectorParams))
        
        if has_named_vectors and using_vec:
            # Named Vectorsã®å ´åˆã€usingå¼•æ•°ã‚’è©¦ã™
            try:
                hits = client.search(collection_name=collection, query_vector=qvec, limit=topk,
                                   query_filter=qfilter, using=using_vec)
            except (TypeError, Exception):
                # usingå¼•æ•°ãŒã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ãªã„ã€ã¾ãŸã¯ä»–ã®ã‚¨ãƒ©ãƒ¼ã®å ´åˆ
                hits = client.search(collection_name=collection, query_vector=qvec, limit=topk,
                                   query_filter=qfilter)
        else:
            # å˜ä¸€ãƒ™ã‚¯ãƒˆãƒ«ã®å ´åˆã€usingå¼•æ•°ãªã—ã§æ¤œç´¢
            hits = client.search(collection_name=collection, query_vector=qvec, limit=topk,
                               query_filter=qfilter)
    except Exception:
        # ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³æƒ…å ±ãŒå–å¾—ã§ããªã„å ´åˆã€usingå¼•æ•°ãªã—ã§æ¤œç´¢
        hits = client.search(collection_name=collection, query_vector=qvec, limit=topk,
                           query_filter=qfilter)
    
    return hits

# ------------------ ãƒ¡ã‚¤ãƒ³ ------------------
def main():
    cfg = load_config("config.yml")
    rag_cfg = cfg.get("rag", {})
    embeddings_cfg: Dict[str, Dict[str, Any]] = cfg.get("embeddings", {})
    qdrant_url = (cfg.get("qdrant", {}) or {}).get("url", "http://localhost:6333")

    ap = argparse.ArgumentParser(
        description="3ã¤ã®Q&Aãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ãã‚Œãã‚Œç‹¬ç«‹ã—ãŸQdrantã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã«ç™»éŒ²"
    )
    ap.add_argument("--recreate", action="store_true",
                    help="Drop & create collection before upsert.")
    ap.add_argument("--collection", default=None,
                    help="ç‰¹å®šã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã®ã¿å‡¦ç†ï¼ˆæŒ‡å®šãªã—ã§å…¨ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ï¼‰")
    ap.add_argument("--qdrant-url", default=qdrant_url)
    ap.add_argument("--batch-size", type=int, default=32)
    ap.add_argument("--limit", type=int, default=0,
                    help="Row limit per CSV for development (0=all)")
    ap.add_argument("--include-answer", action="store_true",
                    default=rag_cfg.get("include_answer_in_embedding", False),
                    help="Use 'question\\nanswer' as embedding input.")
    ap.add_argument("--search", default=None, help="Run search only.")
    ap.add_argument("--topk", type=int, default=5)
    args = ap.parse_args()

    if not embeddings_cfg:
        embeddings_cfg = DEFAULTS["embeddings"]

    # Qdrant client
    client = QdrantClient(url=args.qdrant_url, timeout=300)

    # æ¤œç´¢ã®ã¿
    if args.search:
        if not args.collection:
            print("[ERROR] æ¤œç´¢ã«ã¯ --collection ã®æŒ‡å®šãŒå¿…è¦ã§ã™")
            return

        model = embeddings_cfg["primary"]["model"]
        hits = search(client, args.collection, args.search, "primary", model, topk=args.topk)

        print(f"\n[Search] collection={args.collection} query={args.search!r}")
        for h in hits:
            method = h.payload.get('generation_method', 'unknown')
            question = h.payload.get('question', '')[:80]
            answer = h.payload.get('answer', '')[:80]
            print(f"score={h.score:.4f}  method={method}  Q: {question}  A: {answer}...")
        return

    # å‡¦ç†å¯¾è±¡ã®ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã‚’æ±ºå®š
    if args.collection:
        # ç‰¹å®šã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã®ã¿
        target_mappings = [m for m in COLLECTION_MAPPINGS if m["collection"] == args.collection]
        if not target_mappings:
            print(f"[ERROR] ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ '{args.collection}' ã¯å®šç¾©ã•ã‚Œã¦ã„ã¾ã›ã‚“")
            print(f"åˆ©ç”¨å¯èƒ½ãªã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³: {[m['collection'] for m in COLLECTION_MAPPINGS]}")
            return
    else:
        # å…¨ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³
        target_mappings = COLLECTION_MAPPINGS

    # ã‚¤ãƒ³ã‚¸ã‚§ã‚¹ãƒˆå‡¦ç†
    print(f"\n[INFO] å‡¦ç†å¯¾è±¡: {len(target_mappings)} ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³")
    print("=" * 80)

    total = 0
    for mapping in target_mappings:
        csv_file = mapping["csv_file"]
        collection_name = mapping["collection"]
        generation_method = mapping["generation_method"]
        domain = mapping["domain"]
        description = mapping["description"]

        print(f"\nğŸ“¦ ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³: {collection_name}")
        print(f"   èª¬æ˜: {description}")
        print(f"   ã‚½ãƒ¼ã‚¹: {csv_file}")
        print("-" * 80)

        if not os.path.exists(csv_file):
            print(f"[WARN] ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {csv_file} (ã‚¹ã‚­ãƒƒãƒ—)")
            continue

        # ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ä½œæˆ
        create_or_recreate_collection(client, collection_name, args.recreate, embeddings_cfg)

        # ãƒ‡ãƒ¼ã‚¿ã‚¿ã‚¤ãƒ—ã«å¿œã˜ã¦ãƒ­ãƒ¼ãƒ‰
        data_type = mapping.get("type", "qa")
        if data_type == "raw":
            # ç”Ÿãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰
            df = load_txt(csv_file, limit=args.limit)
            print(f"   ãƒ‡ãƒ¼ã‚¿ä»¶æ•°: {len(df):,}ä»¶")
            texts = df["text"].tolist()
        else:
            # Q&A CSVã‚’ãƒ­ãƒ¼ãƒ‰
            df = load_csv(csv_file, limit=args.limit)
            print(f"   ãƒ‡ãƒ¼ã‚¿ä»¶æ•°: {len(df):,}ä»¶")
            texts = build_inputs(df, include_answer=args.include_answer)

        # åŸ‹ã‚è¾¼ã¿ç”Ÿæˆ
        vectors_by_name: Dict[str, List[List[float]]] = {}
        for name, vcfg in embeddings_cfg.items():
            print(f"   åŸ‹ã‚è¾¼ã¿ç”Ÿæˆä¸­: {name} (model={vcfg['model']})... ", end="", flush=True)
            vectors_by_name[name] = embed_texts(texts, model=vcfg["model"], batch_size=args.batch_size)
            print("âœ“")

        # ãƒã‚¤ãƒ³ãƒˆæ§‹ç¯‰ã¨ã‚¢ãƒƒãƒ—ã‚µãƒ¼ãƒˆ
        points = build_points(df, vectors_by_name, domain, csv_file, generation_method, data_type=data_type)
        print(f"   ã‚¢ãƒƒãƒ—ã‚µãƒ¼ãƒˆä¸­... ", end="", flush=True)
        n = upsert_points(client, collection_name, points, batch_size=args.batch_size)
        print(f"âœ“ {n:,}ä»¶")

        total += n

    print("\n" + "=" * 80)
    print(f"âœ… å®Œäº†: ç·ç™»éŒ²ä»¶æ•° {total:,}ä»¶")

    # ç™»éŒ²ã•ã‚ŒãŸã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ä¸€è¦§ã‚’è¡¨ç¤º
    print(f"\n[INFO] ç™»éŒ²ã•ã‚ŒãŸã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ä¸€è¦§:")
    print("-" * 80)
    all_collections = client.get_collections()
    for col in all_collections.collections:
        print(f"  â€¢ {col.name}")
    print("-" * 80)

    # æ¤œè¨¼æ¤œç´¢
    print(f"\n[INFO] æ¤œè¨¼æ¤œç´¢ã‚’å®Ÿè¡Œä¸­...")
    model = embeddings_cfg["primary"]["model"]

    for mapping in target_mappings:
        collection_name = mapping["collection"]
        try:
            # ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³æƒ…å ±ã‚’å–å¾—
            info = client.get_collection(collection_name)
            print(f"\n  {collection_name}: {info.points_count:,}ä»¶ç™»éŒ²æ¸ˆã¿")

            # ã‚µãƒ³ãƒ—ãƒ«æ¤œç´¢
            hits = search(client, collection_name, "æ°—å€™å¤‰å‹•", "primary", model, topk=2)
            if hits:
                for h in hits[:1]:
                    q = h.payload.get('question', '')[:50]
                    print(f"    ã‚µãƒ³ãƒ—ãƒ«æ¤œç´¢çµæœ: score={h.score:.4f}  Q: {q}...")
        except Exception as e:
            print(f"    æ¤œè¨¼ã‚¨ãƒ©ãƒ¼: {e}")

if __name__ == "__main__":
    main()
