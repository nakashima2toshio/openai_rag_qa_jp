#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
a02_make_qa.py - preprocessedãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰Q/Aãƒšã‚¢ç”Ÿæˆ
=====================================================
OUTPUTãƒ•ã‚©ãƒ«ãƒ€å†…ã®preprocessedãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰è‡ªå‹•çš„ã«Q/Aãƒšã‚¢ã‚’ç”Ÿæˆ

å¯¾å¿œãƒ•ã‚¡ã‚¤ãƒ«:
- OUTPUT/preprocessed_cc_news.csv (è‹±èªãƒ‹ãƒ¥ãƒ¼ã‚¹)
- OUTPUT/preprocessed_japanese_text.csv (æ—¥æœ¬èªWebãƒ†ã‚­ã‚¹ãƒˆ)
- OUTPUT/preprocessed_wikipedia_ja.csv (Wikipediaæ—¥æœ¬èªç‰ˆ)

ä½¿ç”¨æ–¹æ³•:
    python a02_make_qa.py [--dataset DATASET_TYPE] [--model MODEL_NAME] [--output OUTPUT_DIR]

ä¾‹:
    python a02_make_qa.py --dataset cc_news --model gpt-5-mini  --analyze-coverage --max-docs 10
    python a02_make_qa.py --dataset wikipedia_ja --model gpt-5-mini  --analyze-coverage --max-docs 10
    python a02_make_qa.py --dataset japanese_text --model gpt-5-mini  --analyze-coverage --max-docs 10
"""

import os
import sys
import json
import time
import argparse
import pandas as pd
import numpy as np
from pathlib import Path
from typing import List, Dict, Optional, Tuple
from datetime import datetime
import tiktoken
from openai import OpenAI
from pydantic import BaseModel
from dotenv import load_dotenv
import logging

# ãƒ­ãƒ¼ã‚«ãƒ«ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
from a03_rag_qa_coverage import SemanticCoverage

# ç’°å¢ƒå¤‰æ•°èª­ã¿è¾¼ã¿
load_dotenv()

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


# ==========================================
# Pydantic ãƒ¢ãƒ‡ãƒ«å®šç¾©
# ==========================================

class QAPair(BaseModel):
    """Q/Aãƒšã‚¢ã®ãƒ‡ãƒ¼ã‚¿ãƒ¢ãƒ‡ãƒ«"""
    question: str
    answer: str
    question_type: str
    source_chunk_id: Optional[str] = None
    dataset_type: Optional[str] = None
    auto_generated: bool = False


class QAPairsResponse(BaseModel):
    """Q/Aãƒšã‚¢ç”Ÿæˆãƒ¬ã‚¹ãƒãƒ³ã‚¹"""
    qa_pairs: List[QAPair]


# ==========================================
# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆè¨­å®š
# ==========================================

DATASET_CONFIGS = {
    "cc_news": {
        "name": "CC-Newsè‹±èªãƒ‹ãƒ¥ãƒ¼ã‚¹",
        "file": "OUTPUT/preprocessed_cc_news.csv",
        "text_column": "Combined_Text",
        "title_column": "title",
        "lang": "en",
        "chunk_size": 300,  # ãƒˆãƒ¼ã‚¯ãƒ³æ•°
        "qa_per_chunk": 3,  # ãƒãƒ£ãƒ³ã‚¯ã‚ãŸã‚Šã®Q/Aæ•°
    },
    "japanese_text": {
        "name": "æ—¥æœ¬èªWebãƒ†ã‚­ã‚¹ãƒˆ",
        "file": "OUTPUT/preprocessed_japanese_text.csv",
        "text_column": "Combined_Text",
        "title_column": None,
        "lang": "ja",
        "chunk_size": 200,
        "qa_per_chunk": 2,
    },
    "wikipedia_ja": {
        "name": "Wikipediaæ—¥æœ¬èªç‰ˆ",
        "file": "OUTPUT/preprocessed_wikipedia_ja.csv",
        "text_column": "Combined_Text",
        "title_column": "title",
        "lang": "ja",
        "chunk_size": 250,
        "qa_per_chunk": 3,
    }
}


# ==========================================
# ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ãƒ»å‰å‡¦ç†
# ==========================================

def load_preprocessed_data(dataset_type: str) -> pd.DataFrame:
    """preprocessedãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿

    Args:
        dataset_type: ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚¿ã‚¤ãƒ—

    Returns:
        èª­ã¿è¾¼ã‚“ã DataFrame
    """
    config = DATASET_CONFIGS.get(dataset_type)
    if not config:
        raise ValueError(f"æœªå¯¾å¿œã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ: {dataset_type}")

    file_path = config["file"]
    if not Path(file_path).exists():
        raise FileNotFoundError(f"ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {file_path}")

    logger.info(f"ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ä¸­: {file_path}")
    df = pd.read_csv(file_path)

    # å¿…è¦ãªã‚«ãƒ©ãƒ ã®ç¢ºèª
    text_col = config["text_column"]
    if text_col not in df.columns:
        raise ValueError(f"ãƒ†ã‚­ã‚¹ãƒˆã‚«ãƒ©ãƒ  '{text_col}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")

    # ç©ºã®ãƒ†ã‚­ã‚¹ãƒˆã‚’é™¤å¤–
    df = df[df[text_col].notna() & (df[text_col].str.strip() != '')]

    logger.info(f"èª­ã¿è¾¼ã¿å®Œäº†: {len(df)}ä»¶ã®ãƒ‡ãƒ¼ã‚¿")
    return df


def create_document_chunks(df: pd.DataFrame, dataset_type: str, max_docs: Optional[int] = None) -> List[Dict]:
    """DataFrameã‹ã‚‰æ–‡æ›¸ãƒãƒ£ãƒ³ã‚¯ã‚’ä½œæˆ
    Args:
        df: ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ 
        dataset_type: ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚¿ã‚¤ãƒ—
        max_docs: å‡¦ç†ã™ã‚‹æœ€å¤§æ–‡æ›¸æ•°
    Returns:
        ãƒãƒ£ãƒ³ã‚¯ã®ãƒªã‚¹ãƒˆ
    """
    config = DATASET_CONFIGS[dataset_type]
    text_col = config["text_column"]
    title_col = config.get("title_column")
    chunk_size = config["chunk_size"]

    analyzer = SemanticCoverage()
    all_chunks = []

    # å‡¦ç†ã™ã‚‹æ–‡æ›¸æ•°ã‚’åˆ¶é™
    docs_to_process = df.head(max_docs) if max_docs else df

    logger.info(f"ãƒãƒ£ãƒ³ã‚¯ä½œæˆé–‹å§‹: {len(docs_to_process)}ä»¶ã®æ–‡æ›¸")

    for idx, row in docs_to_process.iterrows():
        # row[text_col]ã¯Seriesã‚„ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®å¯èƒ½æ€§ãŒã‚ã‚‹ãŸã‚ã€æ˜ç¤ºçš„ã«strã«å¤‰æ›
        text = str(row[text_col]) if pd.notna(row[text_col]) else ""

        # ã‚¿ã‚¤ãƒˆãƒ«ãŒã‚ã‚‹å ´åˆã¯å«ã‚ã‚‹
        if title_col and title_col in row and pd.notna(row[title_col]):
            doc_id = f"{dataset_type}_{idx}_{str(row[title_col])[:30]}"
        else:
            doc_id = f"{dataset_type}_{idx}"

        # SemanticCoverageã‚’ä½¿ç”¨ã—ã¦ãƒãƒ£ãƒ³ã‚¯ä½œæˆ
        # æ³¨: create_semantic_chunksã¯max_tokensãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æŒãŸãªã„ãŸã‚ã€
        # å†…éƒ¨ã®ãƒãƒ¼ãƒ‰ã‚³ãƒ¼ãƒ‰å€¤(200ãƒˆãƒ¼ã‚¯ãƒ³)ãŒä½¿ç”¨ã•ã‚Œã‚‹
        try:
            chunks = analyzer.create_semantic_chunks(text, verbose=False)

            # å„ãƒãƒ£ãƒ³ã‚¯ã«ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’è¿½åŠ 
            for i, chunk in enumerate(chunks):
                chunk['doc_id'] = doc_id
                chunk['doc_idx'] = idx
                chunk['chunk_idx'] = i
                chunk['dataset_type'] = dataset_type
                all_chunks.append(chunk)

        except Exception as e:
            logger.warning(f"ãƒãƒ£ãƒ³ã‚¯ä½œæˆã‚¨ãƒ©ãƒ¼ (doc {idx}): {e}")
            continue

    logger.info(f"ãƒãƒ£ãƒ³ã‚¯ä½œæˆå®Œäº†: {len(all_chunks)}å€‹ã®ãƒãƒ£ãƒ³ã‚¯")
    return all_chunks


def merge_small_chunks(chunks: List[Dict], min_tokens: int = 150, max_tokens: int = 400) -> List[Dict]:
    """å°ã•ã„ãƒãƒ£ãƒ³ã‚¯ã‚’çµ±åˆã—ã¦é©åˆ‡ãªã‚µã‚¤ã‚ºã«ã™ã‚‹

    Args:
        chunks: ãƒãƒ£ãƒ³ã‚¯ã®ãƒªã‚¹ãƒˆ
        min_tokens: ã“ã®ãƒˆãƒ¼ã‚¯ãƒ³æ•°æœªæº€ã®ãƒãƒ£ãƒ³ã‚¯ã¯çµ±åˆå¯¾è±¡
        max_tokens: çµ±åˆå¾Œã®æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³æ•°

    Returns:
        çµ±åˆã•ã‚ŒãŸãƒãƒ£ãƒ³ã‚¯ã®ãƒªã‚¹ãƒˆ
    """
    tokenizer = tiktoken.get_encoding("cl100k_base")
    merged_chunks = []
    current_merge = None

    for chunk in chunks:
        chunk_tokens = len(tokenizer.encode(chunk['text']))

        # å¤§ãã„ãƒãƒ£ãƒ³ã‚¯ã¯ãã®ã¾ã¾è¿½åŠ 
        if chunk_tokens >= min_tokens:
            if current_merge:
                merged_chunks.append(current_merge)
                current_merge = None
            merged_chunks.append(chunk)
        else:
            # å°ã•ã„ãƒãƒ£ãƒ³ã‚¯ã¯çµ±åˆå€™è£œ
            if current_merge is None:
                current_merge = chunk.copy()
                current_merge['merged'] = True
                current_merge['original_chunks'] = [chunk['id']]
            else:
                # çµ±åˆå¯èƒ½ã‹ãƒã‚§ãƒƒã‚¯
                merge_tokens = len(tokenizer.encode(current_merge['text']))
                if merge_tokens + chunk_tokens <= max_tokens:
                    # åŒã˜æ–‡æ›¸ã‹ã‚‰ã®ãƒãƒ£ãƒ³ã‚¯ã®ã¿çµ±åˆ
                    if current_merge.get('doc_id') == chunk.get('doc_id'):
                        current_merge['text'] += "\n\n" + chunk['text']
                        current_merge['original_chunks'].append(chunk['id'])
                        if 'chunk_idx' in current_merge:
                            current_merge['chunk_idx'] = f"{current_merge['chunk_idx']}-{chunk['chunk_idx']}"
                    else:
                        # ç•°ãªã‚‹æ–‡æ›¸ã®å ´åˆã¯åˆ¥ã€…ã«
                        merged_chunks.append(current_merge)
                        current_merge = chunk.copy()
                        current_merge['merged'] = True
                        current_merge['original_chunks'] = [chunk['id']]
                else:
                    # ã‚µã‚¤ã‚ºã‚ªãƒ¼ãƒãƒ¼ã®å ´åˆã¯ç¾åœ¨ã®çµ±åˆã‚’è¿½åŠ ã—ã¦æ–°è¦é–‹å§‹
                    merged_chunks.append(current_merge)
                    current_merge = chunk.copy()
                    current_merge['merged'] = True
                    current_merge['original_chunks'] = [chunk['id']]

    # æœ€å¾Œã®çµ±åˆãƒãƒ£ãƒ³ã‚¯ã‚’è¿½åŠ 
    if current_merge:
        merged_chunks.append(current_merge)

    logger.info(f"ãƒãƒ£ãƒ³ã‚¯çµ±åˆ: {len(chunks)}å€‹ â†’ {len(merged_chunks)}å€‹ ({100*(1-len(merged_chunks)/len(chunks)):.1f}%å‰Šæ¸›)")
    return merged_chunks


# ==========================================
# Q/Aãƒšã‚¢ç”Ÿæˆ
# ==========================================

def determine_qa_count(chunk: Dict, config: Dict) -> int:
    """ãƒãƒ£ãƒ³ã‚¯ã«æœ€é©ãªQ/Aæ•°ã‚’æ±ºå®š

    Args:
        chunk: ãƒãƒ£ãƒ³ã‚¯ãƒ‡ãƒ¼ã‚¿
        config: ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆè¨­å®š

    Returns:
        Q/Aãƒšã‚¢æ•°
    """
    base_count = config["qa_per_chunk"]

    # ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã«åŸºã¥ãèª¿æ•´
    tokenizer = tiktoken.get_encoding("cl100k_base")
    token_count = len(tokenizer.encode(chunk['text']))

    if token_count < 50:
        return min(base_count, 1)
    elif token_count < 100:
        return min(base_count, 2)
    elif token_count < 200:
        return base_count
    else:
        return min(base_count + 1, 5)


def generate_qa_pairs_for_batch(
    chunks: List[Dict],
    config: Dict,
    model: str = "gpt-5-mini",
    client: Optional[OpenAI] = None
) -> List[Dict]:
    """è¤‡æ•°ãƒãƒ£ãƒ³ã‚¯ã‹ã‚‰ä¸€åº¦ã«Q/Aãƒšã‚¢ã‚’ç”Ÿæˆï¼ˆ3ãƒãƒ£ãƒ³ã‚¯ãƒãƒƒãƒå‡¦ç†å¯¾å¿œï¼‰

    Args:
        chunks: ãƒãƒ£ãƒ³ã‚¯ãƒ‡ãƒ¼ã‚¿ã®ãƒªã‚¹ãƒˆï¼ˆæœ€å¤§3å€‹ï¼‰
        config: ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆè¨­å®š
        model: ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«
        client: OpenAIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ

    Returns:
        ç”Ÿæˆã•ã‚ŒãŸQ/Aãƒšã‚¢ã®ãƒªã‚¹ãƒˆ
    """
    if client is None:
        client = OpenAI()

    if len(chunks) == 0:
        return []

    # å˜ä¸€ãƒãƒ£ãƒ³ã‚¯ã®å ´åˆã¯å¾“æ¥ã®å‡¦ç†
    if len(chunks) == 1:
        return generate_qa_pairs_for_chunk(chunks[0], config, model, client)

    lang = config["lang"]
    all_qa_pairs = []

    # è¨€èªåˆ¥ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆè¨­å®š
    if lang == "ja":
        system_prompt = """ã‚ãªãŸã¯æ•™è‚²ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ä½œæˆã®å°‚é–€å®¶ã§ã™ã€‚
è¤‡æ•°ã®æ—¥æœ¬èªãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ã€å­¦ç¿’åŠ¹æœã®é«˜ã„Q&Aãƒšã‚¢ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚

ç”Ÿæˆãƒ«ãƒ¼ãƒ«:
1. è³ªå•ã¯æ˜ç¢ºã§å…·ä½“çš„ã«
2. å›ç­”ã¯ç°¡æ½”ã§æ­£ç¢ºã«ï¼ˆ1-2æ–‡ç¨‹åº¦ï¼‰
3. ãƒ†ã‚­ã‚¹ãƒˆã®å†…å®¹ã«å¿ å®Ÿã«
4. å¤šæ§˜ãªè¦³ç‚¹ã‹ã‚‰è³ªå•ã‚’ä½œæˆ"""

        # è¤‡æ•°ãƒãƒ£ãƒ³ã‚¯ã‚’çµåˆã—ã¦ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ§‹ç¯‰
        combined_text = ""
        chunks_data = {}
        total_pairs = 0

        for i, chunk in enumerate(chunks, 1):
            num_pairs = determine_qa_count(chunk, config)
            total_pairs += num_pairs
            chunk_text = chunk['text']

            # é•·ã™ãã‚‹å ´åˆã¯çŸ­ç¸®
            if len(chunk_text) > 1000:
                chunk_text = chunk_text[:1000] + "..."

            combined_text += f"\n\nã€ãƒ†ã‚­ã‚¹ãƒˆ{i}ã€‘\n{chunk_text}"
            chunks_data[f"chunk_{i}"] = {"num_pairs": num_pairs, "chunk": chunk}

        user_prompt = f"""ä»¥ä¸‹ã®{len(chunks)}å€‹ã®ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ã€åˆè¨ˆ{total_pairs}å€‹ã®Q&Aãƒšã‚¢ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚
{combined_text}

è³ªå•ã‚¿ã‚¤ãƒ—:
- fact: äº‹å®Ÿç¢ºèªå‹ï¼ˆã€œã¯ä½•ã§ã™ã‹ï¼Ÿï¼‰
- reason: ç†ç”±èª¬æ˜å‹ï¼ˆãªãœã€œã§ã™ã‹ï¼Ÿï¼‰
- comparison: æ¯”è¼ƒå‹ï¼ˆã€œã¨ã€œã®é•ã„ã¯ï¼Ÿï¼‰
- application: å¿œç”¨å‹ï¼ˆã€œã¯ã©ã®ã‚ˆã†ã«æ´»ç”¨ã•ã‚Œã¾ã™ã‹ï¼Ÿï¼‰

JSONå½¢å¼ã§å‡ºåŠ›:
{{
  "qa_pairs": [
    {{
      "question": "è³ªå•æ–‡",
      "answer": "å›ç­”æ–‡",
      "question_type": "fact/reason/comparison/application"
    }}
  ]
}}"""

    else:
        system_prompt = """You are an expert in educational content creation.
Generate high-quality Q&A pairs from multiple English texts.

Generation rules:
1. Questions should be clear and specific
2. Answers should be concise and accurate (1-2 sentences)
3. Stay faithful to the text content
4. Create questions from diverse perspectives"""

        # è¤‡æ•°ãƒãƒ£ãƒ³ã‚¯ã‚’çµåˆã—ã¦ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ§‹ç¯‰
        combined_text = ""
        chunks_data = {}
        total_pairs = 0

        for i, chunk in enumerate(chunks, 1):
            num_pairs = determine_qa_count(chunk, config)
            total_pairs += num_pairs
            chunk_text = chunk['text']

            # é•·ã™ãã‚‹å ´åˆã¯çŸ­ç¸®
            if len(chunk_text) > 1000:
                chunk_text = chunk_text[:1000] + "..."

            combined_text += f"\n\nã€Text {i}ã€‘\n{chunk_text}"
            chunks_data[f"chunk_{i}"] = {"num_pairs": num_pairs, "chunk": chunk}

        user_prompt = f"""Generate {total_pairs} Q&A pairs from the following {len(chunks)} texts.
{combined_text}

Question types:
- fact: Factual questions (What is...?)
- reason: Explanatory questions (Why...?)
- comparison: Comparative questions (What's the difference...?)
- application: Application questions (How is... used?)

Output in JSON format:
{{
  "qa_pairs": [
    {{
      "question": "question text",
      "answer": "answer text",
      "question_type": "fact/reason/comparison/application"
    }}
  ]
}}"""

    try:
        # æœ€æ–°ã®OpenAI Responses API (client.responses.parse) ã‚’ä½¿ç”¨
        # ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¨ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’çµ±åˆ
        combined_input = f"{system_prompt}\n\n{user_prompt}"

        response = client.responses.parse(
            input=combined_input,
            model=model,
            text_format=QAPairsResponse,  # Pydanticãƒ¢ãƒ‡ãƒ«ã‚’ç›´æ¥æŒ‡å®š
            max_output_tokens=4000  # ãƒãƒƒãƒå‡¦ç†ã®ãŸã‚å¢—åŠ ï¼ˆ3ãƒãƒ£ãƒ³ã‚¯å¯¾å¿œï¼‰
        )

        # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã®è§£æ
        for output in response.output:
            if output.type == "message":
                for item in output.content:
                    if item.type == "output_text" and item.parsed:
                        # ãƒ‘ãƒ¼ã‚¹æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
                        parsed_data = item.parsed

                        # ç”Ÿæˆã•ã‚ŒãŸQ/Aãƒšã‚¢ã‚’å„ãƒãƒ£ãƒ³ã‚¯ã«åˆ†é…
                        # å„ãƒãƒ£ãƒ³ã‚¯ã«æœŸå¾…ã•ã‚Œã‚‹æ•°ã ã‘Q/Aã‚’å‰²ã‚Šå½“ã¦
                        qa_index = 0
                        for i, chunk in enumerate(chunks, 1):
                            chunk_key = f"chunk_{i}"
                            expected_pairs = chunks_data[chunk_key]["num_pairs"]

                            # ã“ã®ãƒãƒ£ãƒ³ã‚¯ã«å‰²ã‚Šå½“ã¦ã‚‹Q/Aãƒšã‚¢ã‚’å–å¾—
                            for _ in range(expected_pairs):
                                if qa_index < len(parsed_data.qa_pairs):
                                    qa_data = parsed_data.qa_pairs[qa_index]
                                    qa = {
                                        "question": qa_data.question,
                                        "answer": qa_data.answer,
                                        "question_type": qa_data.question_type,
                                        "source_chunk_id": chunk.get('id', ''),
                                        "doc_id": chunk.get('doc_id', ''),
                                        "dataset_type": chunk.get('dataset_type', ''),
                                        "chunk_idx": chunk.get('chunk_idx', 0)
                                    }
                                    all_qa_pairs.append(qa)
                                    qa_index += 1

        return all_qa_pairs

    except Exception as e:
        logger.error(f"ãƒãƒƒãƒQ/Aç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: å€‹åˆ¥å‡¦ç†
        logger.info("ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ãƒãƒ£ãƒ³ã‚¯ã‚’å€‹åˆ¥å‡¦ç†ã—ã¾ã™")
        for chunk in chunks:
            try:
                qa_pairs = generate_qa_pairs_for_chunk(chunk, config, model, client)
                all_qa_pairs.extend(qa_pairs)
            except Exception as chunk_error:
                logger.error(f"ãƒãƒ£ãƒ³ã‚¯å€‹åˆ¥å‡¦ç†ã‚¨ãƒ©ãƒ¼: {chunk_error}")
        return all_qa_pairs


def generate_qa_pairs_for_chunk(
    chunk: Dict,
    config: Dict,
    model: str = "gpt-5-mini",
    client: Optional[OpenAI] = None
) -> List[Dict]:
    """å˜ä¸€ãƒãƒ£ãƒ³ã‚¯ã‹ã‚‰Q/Aãƒšã‚¢ã‚’ç”Ÿæˆï¼ˆå¾Œæ–¹äº’æ›æ€§ã®ãŸã‚ç¶­æŒï¼‰

    Args:
        chunk: ãƒãƒ£ãƒ³ã‚¯ãƒ‡ãƒ¼ã‚¿
        config: ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆè¨­å®š
        model: ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«
        client: OpenAIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ
    Returns:
        ç”Ÿæˆã•ã‚ŒãŸQ/Aãƒšã‚¢ã®ãƒªã‚¹ãƒˆ
    """
    if client is None:
        client = OpenAI()

    num_pairs = determine_qa_count(chunk, config)
    lang = config["lang"]

    # è¨€èªåˆ¥ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆè¨­å®š
    if lang == "ja":
        system_prompt = """ã‚ãªãŸã¯æ•™è‚²ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ä½œæˆã®å°‚é–€å®¶ã§ã™ã€‚
ä¸ãˆã‚‰ã‚ŒãŸæ—¥æœ¬èªãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ã€å­¦ç¿’åŠ¹æœã®é«˜ã„Q&Aãƒšã‚¢ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚

ç”Ÿæˆãƒ«ãƒ¼ãƒ«:
1. è³ªå•ã¯æ˜ç¢ºã§å…·ä½“çš„ã«
2. å›ç­”ã¯ç°¡æ½”ã§æ­£ç¢ºã«ï¼ˆ1-2æ–‡ç¨‹åº¦ï¼‰
3. ãƒ†ã‚­ã‚¹ãƒˆã®å†…å®¹ã«å¿ å®Ÿã«
4. å¤šæ§˜ãªè¦³ç‚¹ã‹ã‚‰è³ªå•ã‚’ä½œæˆ"""

        question_types_desc = """
- fact: äº‹å®Ÿç¢ºèªå‹ï¼ˆã€œã¯ä½•ã§ã™ã‹ï¼Ÿï¼‰
- reason: ç†ç”±èª¬æ˜å‹ï¼ˆãªãœã€œã§ã™ã‹ï¼Ÿï¼‰
- comparison: æ¯”è¼ƒå‹ï¼ˆã€œã¨ã€œã®é•ã„ã¯ï¼Ÿï¼‰
- application: å¿œç”¨å‹ï¼ˆã€œã¯ã©ã®ã‚ˆã†ã«æ´»ç”¨ã•ã‚Œã¾ã™ã‹ï¼Ÿï¼‰"""
    else:
        system_prompt = """You are an expert in educational content creation.
Generate high-quality Q&A pairs from the given English text.

Generation rules:
1. Questions should be clear and specific
2. Answers should be concise and accurate (1-2 sentences)
3. Stay faithful to the text content
4. Create questions from diverse perspectives"""

        question_types_desc = """
- fact: Factual questions (What is...?)
- reason: Explanatory questions (Why...?)
- comparison: Comparative questions (What's the difference...?)
- application: Application questions (How is... used?)"""

    # è¨€èªã«å¿œã˜ãŸãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
    if lang == "ja":
        user_prompt = f"""ä»¥ä¸‹ã®ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰{num_pairs}å€‹ã®Q&Aãƒšã‚¢ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚

è³ªå•ã‚¿ã‚¤ãƒ—:
{question_types_desc}

ãƒ†ã‚­ã‚¹ãƒˆ:
{chunk['text']}

JSONå½¢å¼ã§å‡ºåŠ›:
{{
  "qa_pairs": [
    {{
      "question": "è³ªå•æ–‡",
      "answer": "å›ç­”æ–‡",
      "question_type": "fact/reason/comparison/application"
    }}
  ]
}}"""
    else:
        user_prompt = f"""Generate {num_pairs} Q&A pairs from the following text.

Question types:
{question_types_desc}

Text:
{chunk['text']}

Output in JSON format:
{{
  "qa_pairs": [
    {{
      "question": "question text",
      "answer": "answer text",
      "question_type": "fact/reason/comparison/application"
    }}
  ]
}}"""

    try:
        # ãƒãƒ£ãƒ³ã‚¯ãŒé•·ã™ãã‚‹å ´åˆã¯çŸ­ç¸®ï¼ˆæ—¥æœ¬èªãƒ†ã‚­ã‚¹ãƒˆã¯é•·ã„å‚¾å‘ãŒã‚ã‚‹ãŸã‚ï¼‰
        max_chunk_length = 2000  # æ–‡å­—æ•°åˆ¶é™
        chunk_text = chunk['text']
        if len(chunk_text) > max_chunk_length:
            chunk_text = chunk_text[:max_chunk_length] + "..."
            logger.debug(f"ãƒãƒ£ãƒ³ã‚¯ã‚’{max_chunk_length}æ–‡å­—ã«çŸ­ç¸®")

        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®å†æ§‹ç¯‰ï¼ˆçŸ­ç¸®ã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã‚’ä½¿ç”¨ï¼‰
        if lang == "ja":
            user_prompt = f"""ä»¥ä¸‹ã®ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰{num_pairs}å€‹ã®Q&Aãƒšã‚¢ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚

è³ªå•ã‚¿ã‚¤ãƒ—:
{question_types_desc}

ãƒ†ã‚­ã‚¹ãƒˆ:
{chunk_text}

JSONå½¢å¼ã§å‡ºåŠ›:
{{
  "qa_pairs": [
    {{
      "question": "è³ªå•æ–‡",
      "answer": "å›ç­”æ–‡",
      "question_type": "fact/reason/comparison/application"
    }}
  ]
}}"""
        else:
            user_prompt = f"""Generate {num_pairs} Q&A pairs from the following text.

Question types:
{question_types_desc}

Text:
{chunk_text}

Output in JSON format:
{{
  "qa_pairs": [
    {{
      "question": "question text",
      "answer": "answer text",
      "question_type": "fact/reason/comparison/application"
    }}
  ]
}}"""

        # æœ€æ–°ã®OpenAI Responses API (client.responses.parse) ã‚’ä½¿ç”¨
        combined_input = f"{system_prompt}\n\n{user_prompt}"

        response = client.responses.parse(
            input=combined_input,
            model=model,
            text_format=QAPairsResponse,  # Pydanticãƒ¢ãƒ‡ãƒ«ã‚’ç›´æ¥æŒ‡å®š
            max_output_tokens=1000  # å‡ºåŠ›ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’åˆ¶é™
        )

        # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã®è§£æ
        qa_pairs = []
        for output in response.output:
            if output.type == "message":
                for item in output.content:
                    if item.type == "output_text" and item.parsed:
                        # ãƒ‘ãƒ¼ã‚¹æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
                        parsed_data = item.parsed

                        for qa_data in parsed_data.qa_pairs:
                            qa = {
                                "question": qa_data.question,
                                "answer": qa_data.answer,
                                "question_type": qa_data.question_type,
                                "source_chunk_id": chunk.get('id', ''),
                                "doc_id": chunk.get('doc_id', ''),
                                "dataset_type": chunk.get('dataset_type', ''),
                                "chunk_idx": chunk.get('chunk_idx', 0)
                            }
                            qa_pairs.append(qa)
        return qa_pairs

    except Exception as e:
        logger.error(f"Q/Aç”Ÿæˆã‚¨ãƒ©ãƒ¼ (chunk {chunk.get('id', 'unknown')}): {e}")
        return []


def generate_qa_for_dataset(
    chunks: List[Dict],
    dataset_type: str,
    model: str = "gpt-5-mini",
    chunk_batch_size: int = 3,
    merge_chunks: bool = True,
    min_tokens: int = 150,
    max_tokens: int = 400
) -> List[Dict]:
    """ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå…¨ä½“ã®Q/Aãƒšã‚¢ç”Ÿæˆï¼ˆæ”¹å–„ç‰ˆï¼‰

    Args:
        chunks: ãƒãƒ£ãƒ³ã‚¯ãƒªã‚¹ãƒˆ
        dataset_type: ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚¿ã‚¤ãƒ—
        model: ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«
        chunk_batch_size: 1å›ã®APIã§å‡¦ç†ã™ã‚‹ãƒãƒ£ãƒ³ã‚¯æ•°ï¼ˆ1-5ï¼‰
        merge_chunks: å°ã•ã„ãƒãƒ£ãƒ³ã‚¯ã‚’çµ±åˆã™ã‚‹ã‹
        min_tokens: çµ±åˆå¯¾è±¡ã®æœ€å°ãƒˆãƒ¼ã‚¯ãƒ³æ•°
        max_tokens: çµ±åˆå¾Œã®æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³æ•°

    Returns:
        ç”Ÿæˆã•ã‚ŒãŸQ/Aãƒšã‚¢ã®ãƒªã‚¹ãƒˆ
    """
    config = DATASET_CONFIGS[dataset_type]
    client = OpenAI()
    all_qa_pairs = []

    # ãƒãƒ£ãƒ³ã‚¯ã®å‰å‡¦ç†ï¼ˆå°ã•ã„ãƒãƒ£ãƒ³ã‚¯ã®çµ±åˆï¼‰
    if merge_chunks:
        processed_chunks = merge_small_chunks(chunks, min_tokens, max_tokens)
    else:
        processed_chunks = chunks

    total_chunks = len(processed_chunks)
    api_calls = (total_chunks + chunk_batch_size - 1) // chunk_batch_size

    logger.info(f"""
    Q/Aãƒšã‚¢ç”Ÿæˆé–‹å§‹:
    - å…ƒãƒãƒ£ãƒ³ã‚¯æ•°: {len(chunks)}
    - å‡¦ç†ãƒãƒ£ãƒ³ã‚¯æ•°: {total_chunks}
    - ãƒãƒƒãƒã‚µã‚¤ã‚º: {chunk_batch_size}
    - APIå‘¼ã³å‡ºã—äºˆå®š: {api_calls}å›
    - ãƒ¢ãƒ‡ãƒ«: {model}
    """)

    # ãƒãƒƒãƒå‡¦ç†
    for i in range(0, total_chunks, chunk_batch_size):
        batch = processed_chunks[i:i+chunk_batch_size]
        batch_num = i // chunk_batch_size + 1
        total_batches = api_calls

        logger.info(f"ãƒãƒƒãƒ {batch_num}/{total_batches} å‡¦ç†ä¸­ ({len(batch)}ãƒãƒ£ãƒ³ã‚¯)...")

        # ãƒªãƒˆãƒ©ã‚¤æ©Ÿèƒ½ä»˜ãQ/Aç”Ÿæˆ
        max_retries = 3
        for attempt in range(max_retries):
            try:
                if chunk_batch_size == 1:
                    # å˜ä¸€ãƒãƒ£ãƒ³ã‚¯å‡¦ç†
                    qa_pairs = generate_qa_pairs_for_chunk(batch[0], config, model, client)
                else:
                    # ãƒãƒƒãƒå‡¦ç†
                    qa_pairs = generate_qa_pairs_for_batch(batch, config, model, client)

                if qa_pairs:
                    all_qa_pairs.extend(qa_pairs)
                    logger.debug(f"ãƒãƒƒãƒ {batch_num}: {len(qa_pairs)}å€‹ã®Q/Aãƒšã‚¢ç”Ÿæˆ")
                break

            except Exception as e:
                if attempt == max_retries - 1:
                    logger.error(f"ãƒãƒƒãƒ {batch_num} ç”Ÿæˆå¤±æ•—: {e}")
                    # æœ€çµ‚è©¦è¡Œå¤±æ•—æ™‚ã¯å€‹åˆ¥å‡¦ç†ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
                    logger.info("å€‹åˆ¥å‡¦ç†ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯...")
                    for chunk in batch:
                        try:
                            qa_pairs = generate_qa_pairs_for_chunk(chunk, config, model, client)
                            if qa_pairs:
                                all_qa_pairs.extend(qa_pairs)
                        except Exception as chunk_error:
                            logger.error(f"ãƒãƒ£ãƒ³ã‚¯å‡¦ç†ã‚¨ãƒ©ãƒ¼: {chunk_error}")
                else:
                    wait_time = 2 ** attempt
                    logger.warning(f"ãƒªãƒˆãƒ©ã‚¤ {attempt + 1}/{max_retries} (å¾…æ©Ÿ: {wait_time}ç§’)")
                    time.sleep(wait_time)

        # APIåˆ¶é™å¯¾ç­–ï¼ˆæœ€å¾Œã®ãƒãƒƒãƒä»¥å¤–ã§å¾…æ©Ÿï¼‰
        if i + chunk_batch_size < total_chunks:
            time.sleep(0.5)  # çŸ­ç¸®ï¼ˆãƒãƒƒãƒå‡¦ç†ã«ã‚ˆã‚Šå‘¼ã³å‡ºã—æ•°ãŒæ¸›ã£ã¦ã„ã‚‹ãŸã‚ï¼‰

    logger.info(f"""
    Q/Aãƒšã‚¢ç”Ÿæˆå®Œäº†:
    - ç”Ÿæˆã•ã‚ŒãŸQ/Aãƒšã‚¢: {len(all_qa_pairs)}å€‹
    - å®Ÿè¡Œã•ã‚ŒãŸAPIå‘¼ã³å‡ºã—: ç´„{api_calls}å›
    """)

    return all_qa_pairs


# ==========================================
# ã‚«ãƒãƒ¬ãƒ¼ã‚¸åˆ†æ
# ==========================================

# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆåˆ¥æœ€é©é–¾å€¤è¨­å®š
OPTIMAL_THRESHOLDS = {
    "cc_news": {
        "strict": 0.80,
        "standard": 0.70,
        "lenient": 0.60
    },
    "japanese_text": {
        "strict": 0.75,
        "standard": 0.65,
        "lenient": 0.55
    },
    "wikipedia_ja": {
        "strict": 0.85,   # å°‚é–€çš„ãªå†…å®¹ â†’ é«˜ã„é¡ä¼¼åº¦è¦æ±‚
        "standard": 0.75,
        "lenient": 0.65
    }
}


def get_optimal_thresholds(dataset_type: str) -> Dict[str, float]:
    """ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆåˆ¥ã®æœ€é©é–¾å€¤ã‚’å–å¾—

    Args:
        dataset_type: ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚¿ã‚¤ãƒ—

    Returns:
        é–¾å€¤è¾æ›¸ {strict, standard, lenient}
    """
    return OPTIMAL_THRESHOLDS.get(dataset_type, {
        "strict": 0.8,
        "standard": 0.7,
        "lenient": 0.6
    })


def multi_threshold_coverage(coverage_matrix: np.ndarray, chunks: List[Dict],
                             qa_pairs: List[Dict], thresholds: Dict[str, float]) -> Dict:
    """è¤‡æ•°é–¾å€¤ã§ã‚«ãƒãƒ¬ãƒ¼ã‚¸ã‚’è©•ä¾¡

    Args:
        coverage_matrix: ã‚«ãƒãƒ¬ãƒ¼ã‚¸è¡Œåˆ—
        chunks: ãƒãƒ£ãƒ³ã‚¯ãƒªã‚¹ãƒˆ
        qa_pairs: Q/Aãƒšã‚¢ãƒªã‚¹ãƒˆ
        thresholds: é–¾å€¤è¾æ›¸

    Returns:
        å¤šæ®µéšã‚«ãƒãƒ¬ãƒ¼ã‚¸çµæœ
    """
    results = {}
    max_similarities = coverage_matrix.max(axis=1)

    for level, threshold in thresholds.items():
        covered = sum(1 for s in max_similarities if s >= threshold)
        uncovered_chunks = [
            {
                "chunk_id": chunks[i].get("id", f"chunk_{i}"),
                "similarity": float(max_similarities[i]),
                "gap": float(threshold - max_similarities[i])
            }
            for i, sim in enumerate(max_similarities)
            if sim < threshold
        ]

        results[level] = {
            "threshold": threshold,
            "covered_chunks": covered,
            "coverage_rate": covered / len(chunks) if chunks else 0,
            "uncovered_count": len(uncovered_chunks),
            "uncovered_chunks": uncovered_chunks
        }

    return results


def analyze_chunk_characteristics_coverage(chunks: List[Dict], coverage_matrix: np.ndarray,
                                          qa_pairs: List[Dict], threshold: float = 0.7) -> Dict:
    """ãƒãƒ£ãƒ³ã‚¯ç‰¹æ€§åˆ¥ã®ã‚«ãƒãƒ¬ãƒ¼ã‚¸åˆ†æ

    Args:
        chunks: ãƒãƒ£ãƒ³ã‚¯ãƒªã‚¹ãƒˆ
        coverage_matrix: ã‚«ãƒãƒ¬ãƒ¼ã‚¸è¡Œåˆ—
        qa_pairs: Q/Aãƒšã‚¢ãƒªã‚¹ãƒˆ
        threshold: åˆ¤å®šé–¾å€¤

    Returns:
        ãƒãƒ£ãƒ³ã‚¯ç‰¹æ€§åˆ¥ã‚«ãƒãƒ¬ãƒ¼ã‚¸çµæœ
    """
    tokenizer = tiktoken.get_encoding("cl100k_base")
    results = {
        "by_length": {},      # é•·ã•åˆ¥
        "by_position": {},    # ä½ç½®åˆ¥
        "summary": {}
    }

    # 1. é•·ã•åˆ¥åˆ†æ
    for i, chunk in enumerate(chunks):
        token_count = len(tokenizer.encode(chunk['text']))
        length_category = (
            "short" if token_count < 100 else
            "medium" if token_count < 200 else
            "long"
        )

        if length_category not in results["by_length"]:
            results["by_length"][length_category] = {
                "count": 0,
                "covered": 0,
                "avg_similarity": 0.0,
                "similarities": []
            }

        max_sim = coverage_matrix[i].max()
        results["by_length"][length_category]["count"] += 1
        results["by_length"][length_category]["similarities"].append(float(max_sim))

        if max_sim >= threshold:
            results["by_length"][length_category]["covered"] += 1

    # å¹³å‡é¡ä¼¼åº¦ã¨ã‚«ãƒãƒ¬ãƒ¼ã‚¸ç‡ã‚’è¨ˆç®—
    for length_cat in results["by_length"]:
        data = results["by_length"][length_cat]
        data["avg_similarity"] = float(np.mean(data["similarities"])) if data["similarities"] else 0.0
        data["coverage_rate"] = data["covered"] / data["count"] if data["count"] > 0 else 0.0
        # similaritiesã¯å¤§ãã„ã®ã§å‰Šé™¤ï¼ˆãƒ¡ãƒ¢ãƒªç¯€ç´„ï¼‰
        del data["similarities"]

    # 2. ä½ç½®åˆ¥åˆ†æï¼ˆæ–‡æ›¸ã®å‰åŠ/ä¸­ç›¤/å¾ŒåŠï¼‰
    total_chunks = len(chunks)
    for i, chunk in enumerate(chunks):
        position = (
            "beginning" if i < total_chunks * 0.33 else
            "middle" if i < total_chunks * 0.67 else
            "end"
        )

        if position not in results["by_position"]:
            results["by_position"][position] = {
                "count": 0,
                "covered": 0,
                "avg_similarity": 0.0,
                "similarities": []
            }

        max_sim = coverage_matrix[i].max()
        results["by_position"][position]["count"] += 1
        results["by_position"][position]["similarities"].append(float(max_sim))

        if max_sim >= threshold:
            results["by_position"][position]["covered"] += 1

    # å¹³å‡é¡ä¼¼åº¦ã¨ã‚«ãƒãƒ¬ãƒ¼ã‚¸ç‡ã‚’è¨ˆç®—
    for position in results["by_position"]:
        data = results["by_position"][position]
        data["avg_similarity"] = float(np.mean(data["similarities"])) if data["similarities"] else 0.0
        data["coverage_rate"] = data["covered"] / data["count"] if data["count"] > 0 else 0.0
        del data["similarities"]

    # 3. ã‚µãƒãƒªãƒ¼æƒ…å ±
    results["summary"] = {
        "total_chunks": len(chunks),
        "total_qa_pairs": len(qa_pairs),
        "threshold_used": threshold,
        "insights": []
    }

    # ã‚¤ãƒ³ã‚µã‚¤ãƒˆç”Ÿæˆ
    for length_cat, data in results["by_length"].items():
        if data["coverage_rate"] < 0.7:
            results["summary"]["insights"].append(
                f"{length_cat}ãƒãƒ£ãƒ³ã‚¯ã®ã‚«ãƒãƒ¬ãƒ¼ã‚¸ãŒä½ã„ï¼ˆ{data['coverage_rate']:.1%}ï¼‰"
            )

    for position, data in results["by_position"].items():
        if data["coverage_rate"] < 0.7:
            results["summary"]["insights"].append(
                f"æ–‡æ›¸{position}éƒ¨åˆ†ã®ã‚«ãƒãƒ¬ãƒ¼ã‚¸ãŒä½ã„ï¼ˆ{data['coverage_rate']:.1%}ï¼‰"
            )

    return results


def analyze_coverage(chunks: List[Dict], qa_pairs: List[Dict], dataset_type: str = "wikipedia_ja") -> Dict:
    """ç”Ÿæˆã•ã‚ŒãŸQ/Aãƒšã‚¢ã®ã‚«ãƒãƒ¬ãƒ¼ã‚¸ã‚’åˆ†æï¼ˆå¤šæ®µéšã‚«ãƒãƒ¬ãƒ¼ã‚¸åˆ†æå¯¾å¿œï¼‰

    Args:
        chunks: ãƒãƒ£ãƒ³ã‚¯ãƒªã‚¹ãƒˆ
        qa_pairs: Q/Aãƒšã‚¢ãƒªã‚¹ãƒˆ
        dataset_type: ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚¿ã‚¤ãƒ—ï¼ˆé–¾å€¤è‡ªå‹•è¨­å®šã«ä½¿ç”¨ï¼‰

    Returns:
        ã‚«ãƒãƒ¬ãƒ¼ã‚¸åˆ†æçµæœï¼ˆå¤šæ®µéšè©•ä¾¡ã€ãƒãƒ£ãƒ³ã‚¯ç‰¹æ€§åˆ†æã‚’å«ã‚€ï¼‰
    """
    analyzer = SemanticCoverage()

    # åŸ‹ã‚è¾¼ã¿ç”Ÿæˆ
    logger.info("åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ç”Ÿæˆä¸­...")
    doc_embeddings = analyzer.generate_embeddings(chunks)

    qa_embeddings = []
    for qa in qa_pairs:
        qa_text = f"{qa['question']} {qa['answer']}"
        embedding = analyzer.generate_embedding(qa_text)
        qa_embeddings.append(embedding)

    qa_embeddings = np.array(qa_embeddings) if qa_embeddings else np.array([])

    if len(qa_embeddings) == 0:
        return {
            "coverage_rate": 0.0,
            "covered_chunks": 0,
            "total_chunks": len(chunks),
            "uncovered_chunks": chunks,
            "multi_threshold": {},
            "chunk_analysis": {}
        }

    # ã‚«ãƒãƒ¬ãƒ¼ã‚¸è¡Œåˆ—è¨ˆç®—
    logger.info("ã‚«ãƒãƒ¬ãƒ¼ã‚¸è¡Œåˆ—è¨ˆç®—ä¸­...")
    coverage_matrix = np.zeros((len(chunks), len(qa_pairs)))
    for i in range(len(doc_embeddings)):
        for j in range(len(qa_embeddings)):
            similarity = analyzer.cosine_similarity(doc_embeddings[i], qa_embeddings[j])
            coverage_matrix[i, j] = similarity

    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆåˆ¥æœ€é©é–¾å€¤ã‚’å–å¾—
    thresholds = get_optimal_thresholds(dataset_type)
    standard_threshold = thresholds["standard"]

    # åŸºæœ¬ã‚«ãƒãƒ¬ãƒ¼ã‚¸ï¼ˆæ¨™æº–é–¾å€¤ï¼‰
    max_similarities = coverage_matrix.max(axis=1)
    covered_count = sum(1 for s in max_similarities if s >= standard_threshold)
    coverage_rate = covered_count / len(chunks) if chunks else 0

    # æœªã‚«ãƒãƒ¼ãƒãƒ£ãƒ³ã‚¯ã®ç‰¹å®š
    uncovered_chunks = []
    for i, (chunk, sim) in enumerate(zip(chunks, max_similarities)):
        if sim < standard_threshold:
            uncovered_chunks.append({
                'chunk': chunk,
                'similarity': float(sim),
                'gap': float(standard_threshold - sim)
            })

    # ææ¡ˆ1ã®æ©Ÿèƒ½: å¤šæ®µéšã‚«ãƒãƒ¬ãƒ¼ã‚¸åˆ†æ
    logger.info("å¤šæ®µéšã‚«ãƒãƒ¬ãƒ¼ã‚¸åˆ†æå®Ÿè¡Œä¸­...")
    multi_threshold_results = multi_threshold_coverage(coverage_matrix, chunks, qa_pairs, thresholds)

    # ææ¡ˆ1ã®æ©Ÿèƒ½: ãƒãƒ£ãƒ³ã‚¯ç‰¹æ€§åˆ¥åˆ†æ
    logger.info("ãƒãƒ£ãƒ³ã‚¯ç‰¹æ€§åˆ¥åˆ†æå®Ÿè¡Œä¸­...")
    chunk_characteristics = analyze_chunk_characteristics_coverage(
        chunks, coverage_matrix, qa_pairs, standard_threshold
    )

    # çµæœã‚’çµ±åˆ
    results = {
        # åŸºæœ¬ãƒ¡ãƒˆãƒªã‚¯ã‚¹
        "coverage_rate": coverage_rate,
        "covered_chunks": covered_count,
        "total_chunks": len(chunks),
        "uncovered_chunks": uncovered_chunks,
        "max_similarities": max_similarities.tolist(),
        "threshold": standard_threshold,

        # ææ¡ˆ1: å¤šæ®µéšã‚«ãƒãƒ¬ãƒ¼ã‚¸
        "multi_threshold": multi_threshold_results,

        # ææ¡ˆ1: ãƒãƒ£ãƒ³ã‚¯ç‰¹æ€§åˆ¥åˆ†æ
        "chunk_analysis": chunk_characteristics,

        # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæƒ…å ±
        "dataset_type": dataset_type,
        "optimal_thresholds": thresholds
    }

    # åˆ†æçµæœã®ã‚µãƒãƒªãƒ¼ã‚’ãƒ­ã‚°å‡ºåŠ›
    logger.info(f"""
    å¤šæ®µéšã‚«ãƒãƒ¬ãƒ¼ã‚¸åˆ†æçµæœ:
    - Strict  (é–¾å€¤{thresholds['strict']:.2f}): {multi_threshold_results['strict']['coverage_rate']:.1%}
    - Standard(é–¾å€¤{thresholds['standard']:.2f}): {multi_threshold_results['standard']['coverage_rate']:.1%}
    - Lenient (é–¾å€¤{thresholds['lenient']:.2f}): {multi_threshold_results['lenient']['coverage_rate']:.1%}

    ãƒãƒ£ãƒ³ã‚¯ç‰¹æ€§åˆ¥ã‚«ãƒãƒ¬ãƒ¼ã‚¸:
    é•·ã•åˆ¥:
    - Short ãƒãƒ£ãƒ³ã‚¯: {chunk_characteristics['by_length'].get('short', {}).get('coverage_rate', 0):.1%}
    - Medium ãƒãƒ£ãƒ³ã‚¯: {chunk_characteristics['by_length'].get('medium', {}).get('coverage_rate', 0):.1%}
    - Long ãƒãƒ£ãƒ³ã‚¯: {chunk_characteristics['by_length'].get('long', {}).get('coverage_rate', 0):.1%}

    ä½ç½®åˆ¥:
    - Beginning (å‰åŠ): {chunk_characteristics['by_position'].get('beginning', {}).get('coverage_rate', 0):.1%}
    - Middle (ä¸­ç›¤): {chunk_characteristics['by_position'].get('middle', {}).get('coverage_rate', 0):.1%}
    - End (å¾ŒåŠ): {chunk_characteristics['by_position'].get('end', {}).get('coverage_rate', 0):.1%}
    """)

    # ã‚¤ãƒ³ã‚µã‚¤ãƒˆãŒã‚ã‚‹å ´åˆã¯è¡¨ç¤º
    if chunk_characteristics['summary']['insights']:
        logger.info("\nğŸ“Š åˆ†æã‚¤ãƒ³ã‚µã‚¤ãƒˆ:")
        for insight in chunk_characteristics['summary']['insights']:
            logger.info(f"  â€¢ {insight}")

    return results


# ==========================================
# çµæœä¿å­˜
# ==========================================

def save_results(
    qa_pairs: List[Dict],
    coverage_results: Dict,
    dataset_type: str,
    output_dir: str = "qa_output"
) -> Dict[str, str]:
    """çµæœã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜

    Args:
        qa_pairs: Q/Aãƒšã‚¢ãƒªã‚¹ãƒˆ
        coverage_results: ã‚«ãƒãƒ¬ãƒ¼ã‚¸åˆ†æçµæœ
        dataset_type: ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚¿ã‚¤ãƒ—
        output_dir: å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª

    Returns:
        ä¿å­˜ã—ãŸãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
    """
    output_path = Path(output_dir)
    output_path.mkdir(exist_ok=True)

    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

    # Q/Aãƒšã‚¢ã‚’ä¿å­˜ï¼ˆJSONï¼‰
    qa_file = output_path / f"qa_pairs_{dataset_type}_{timestamp}.json"
    with open(qa_file, 'w', encoding='utf-8') as f:
        json.dump(qa_pairs, f, ensure_ascii=False, indent=2)

    # Q/Aãƒšã‚¢ã‚’ä¿å­˜ï¼ˆCSVï¼‰
    qa_csv_file = output_path / f"qa_pairs_{dataset_type}_{timestamp}.csv"
    qa_df = pd.DataFrame(qa_pairs)
    qa_df.to_csv(qa_csv_file, index=False, encoding='utf-8')

    # ã‚«ãƒãƒ¬ãƒ¼ã‚¸åˆ†æçµæœã‚’ä¿å­˜
    coverage_file = output_path / f"coverage_{dataset_type}_{timestamp}.json"
    # uncovered_chunksã®ã‚·ãƒªã‚¢ãƒ©ã‚¤ã‚ºå¯¾ç­–
    coverage_save = coverage_results.copy()
    coverage_save['uncovered_chunks'] = [
        {
            'chunk_id': uc['chunk'].get('id', ''),
            'similarity': uc['similarity'],
            'gap': uc['gap'],
            'text_preview': uc['chunk']['text'][:200] + '...'
        }
        for uc in coverage_save.get('uncovered_chunks', [])
    ]

    with open(coverage_file, 'w', encoding='utf-8') as f:
        json.dump(coverage_save, f, ensure_ascii=False, indent=2)

    # ã‚µãƒãƒªãƒ¼æƒ…å ±ã‚’ä¿å­˜
    summary = {
        "dataset_type": dataset_type,
        "dataset_name": DATASET_CONFIGS[dataset_type]["name"],
        "generated_at": timestamp,
        "total_qa_pairs": len(qa_pairs),
        "coverage_rate": coverage_results['coverage_rate'],
        "covered_chunks": coverage_results['covered_chunks'],
        "total_chunks": coverage_results['total_chunks'],
        "files": {
            "qa_json": str(qa_file),
            "qa_csv": str(qa_csv_file),
            "coverage": str(coverage_file)
        }
    }

    summary_file = output_path / f"summary_{dataset_type}_{timestamp}.json"
    with open(summary_file, 'w', encoding='utf-8') as f:
        json.dump(summary, f, ensure_ascii=False, indent=2)

    logger.info(f"çµæœã‚’ä¿å­˜ã—ã¾ã—ãŸ: {output_path}")

    return {
        "qa_json": str(qa_file),
        "qa_csv": str(qa_csv_file),
        "coverage": str(coverage_file),
        "summary": str(summary_file)
    }


# ==========================================
# ãƒ¡ã‚¤ãƒ³å‡¦ç†
# ==========================================

def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    parser = argparse.ArgumentParser(
        description="preprocessedãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰Q/Aãƒšã‚¢ã‚’ç”Ÿæˆ"
    )
    parser.add_argument(
        "--dataset",
        type=str,
        choices=list(DATASET_CONFIGS.keys()),
        default="cc_news",
        help="å‡¦ç†ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ"
    )
    parser.add_argument(
        "--model",
        type=str,
        default="gpt-5-mini",
        help="ä½¿ç”¨ã™ã‚‹OpenAIãƒ¢ãƒ‡ãƒ«"
    )
    parser.add_argument(
        "--output",
        type=str,
        default="qa_output",
        help="å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª"
    )
    parser.add_argument(
        "--max-docs",
        type=int,
        default=None,
        help="å‡¦ç†ã™ã‚‹æœ€å¤§æ–‡æ›¸æ•°ï¼ˆãƒ†ã‚¹ãƒˆç”¨ï¼‰"
    )
    parser.add_argument(
        "--analyze-coverage",
        action="store_true",
        help="ã‚«ãƒãƒ¬ãƒ¼ã‚¸åˆ†æã‚’å®Ÿè¡Œ"
    )
    parser.add_argument(
        "--batch-chunks",
        type=int,
        default=3,
        choices=[1, 2, 3, 4, 5],
        help="1å›ã®APIã§å‡¦ç†ã™ã‚‹ãƒãƒ£ãƒ³ã‚¯æ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 3ï¼‰"
    )
    parser.add_argument(
        "--merge-chunks",
        action="store_true",
        default=True,
        help="å°ã•ã„ãƒãƒ£ãƒ³ã‚¯ã‚’çµ±åˆã™ã‚‹ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: æœ‰åŠ¹ï¼‰"
    )
    parser.add_argument(
        "--no-merge-chunks",
        dest="merge_chunks",
        action="store_false",
        help="ãƒãƒ£ãƒ³ã‚¯çµ±åˆã‚’ç„¡åŠ¹åŒ–"
    )
    parser.add_argument(
        "--min-tokens",
        type=int,
        default=150,
        help="çµ±åˆå¯¾è±¡ã®æœ€å°ãƒˆãƒ¼ã‚¯ãƒ³æ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 150ï¼‰"
    )
    parser.add_argument(
        "--max-tokens",
        type=int,
        default=400,
        help="çµ±åˆå¾Œã®æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³æ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 400ï¼‰"
    )

    args = parser.parse_args()

    # APIã‚­ãƒ¼ç¢ºèª
    api_key = os.getenv("OPENAI_API_KEY")
    if not api_key or api_key == "your-openai-api-key-here":
        logger.error("OPENAI_API_KEYãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
        sys.exit(1)

    logger.info(f"""
    =====================================
    Q/Aãƒšã‚¢ç”Ÿæˆé–‹å§‹
    =====================================
    ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ: {DATASET_CONFIGS[args.dataset]['name']}
    ãƒ¢ãƒ‡ãƒ«: {args.model}
    å‡ºåŠ›å…ˆ: {args.output}
    æœ€å¤§æ–‡æ›¸æ•°: {args.max_docs if args.max_docs else 'åˆ¶é™ãªã—'}
    ã‚«ãƒãƒ¬ãƒ¼ã‚¸åˆ†æ: {'å®Ÿè¡Œ' if args.analyze_coverage else 'ã‚¹ã‚­ãƒƒãƒ—'}
    """)

    try:
        # 1. ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
        logger.info("\n[1/4] ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿...")
        df = load_preprocessed_data(args.dataset)

        # 2. ãƒãƒ£ãƒ³ã‚¯ä½œæˆ
        logger.info("\n[2/4] ãƒãƒ£ãƒ³ã‚¯ä½œæˆ...")
        chunks = create_document_chunks(df, args.dataset, args.max_docs)

        if not chunks:
            logger.error("ãƒãƒ£ãƒ³ã‚¯ãŒä½œæˆã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ")
            sys.exit(1)

        # 3. Q/Aãƒšã‚¢ç”Ÿæˆ
        logger.info("\n[3/4] Q/Aãƒšã‚¢ç”Ÿæˆ...")
        logger.info(f"ã‚ªãƒ—ã‚·ãƒ§ãƒ³: ãƒãƒƒãƒã‚µã‚¤ã‚º={args.batch_chunks}, ãƒãƒ£ãƒ³ã‚¯çµ±åˆ={'æœ‰åŠ¹' if args.merge_chunks else 'ç„¡åŠ¹'}")
        qa_pairs = generate_qa_for_dataset(
            chunks,
            args.dataset,
            args.model,
            chunk_batch_size=args.batch_chunks,
            merge_chunks=args.merge_chunks,
            min_tokens=args.min_tokens,
            max_tokens=args.max_tokens
        )

        if not qa_pairs:
            logger.warning("Q/Aãƒšã‚¢ãŒç”Ÿæˆã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ")

        # 4. ã‚«ãƒãƒ¬ãƒ¼ã‚¸åˆ†æï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
        coverage_results = {}
        if args.analyze_coverage and qa_pairs:
            logger.info("\n[4/4] ã‚«ãƒãƒ¬ãƒ¼ã‚¸åˆ†æ...")
            coverage_results = analyze_coverage(chunks, qa_pairs, args.dataset)

            logger.info(f"""
            ã‚«ãƒãƒ¬ãƒ¼ã‚¸åˆ†æçµæœ:
            - ã‚«ãƒãƒ¬ãƒ¼ã‚¸ç‡: {coverage_results['coverage_rate']:.1%}
            - ã‚«ãƒãƒ¼æ¸ˆã¿ãƒãƒ£ãƒ³ã‚¯: {coverage_results['covered_chunks']}/{coverage_results['total_chunks']}
            - æœªã‚«ãƒãƒ¼ãƒãƒ£ãƒ³ã‚¯: {len(coverage_results['uncovered_chunks'])}
            """)
        else:
            logger.info("\n[4/4] ã‚«ãƒãƒ¬ãƒ¼ã‚¸åˆ†æã‚’ã‚¹ã‚­ãƒƒãƒ—")
            coverage_results = {
                "coverage_rate": 0,
                "covered_chunks": 0,
                "total_chunks": len(chunks),
                "uncovered_chunks": []
            }

        # 5. çµæœä¿å­˜
        logger.info("\nçµæœã‚’ä¿å­˜ä¸­...")
        saved_files = save_results(qa_pairs, coverage_results, args.dataset, args.output)

        # å®Œäº†ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
        logger.info(f"""
        =====================================
        å‡¦ç†å®Œäº†
        =====================================
        ç”ŸæˆQ/Aãƒšã‚¢æ•°: {len(qa_pairs)}
        ä¿å­˜ãƒ•ã‚¡ã‚¤ãƒ«:
        - Q/A (JSON): {saved_files['qa_json']}
        - Q/A (CSV): {saved_files['qa_csv']}
        - ã‚«ãƒãƒ¬ãƒ¼ã‚¸: {saved_files['coverage']}
        - ã‚µãƒãƒªãƒ¼: {saved_files['summary']}
        """)

        # çµ±è¨ˆæƒ…å ±è¡¨ç¤º
        if qa_pairs:
            question_types = {}
            for qa in qa_pairs:
                qt = qa.get('question_type', 'unknown')
                question_types[qt] = question_types.get(qt, 0) + 1

            print("\nè³ªå•ã‚¿ã‚¤ãƒ—åˆ¥çµ±è¨ˆ:")
            for qt, count in sorted(question_types.items()):
                print(f"  {qt}: {count}ä»¶")

    except Exception as e:
        logger.error(f"å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()