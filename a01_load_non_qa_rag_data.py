# a01_load_non_qa_rag_data.py
# !/usr/bin/env python
# -*- coding: utf-8 -*-
"""
a01_load_non_qa_rag_data.py - éQ&Aå‹RAGãƒ‡ãƒ¼ã‚¿å‡¦ç†ãƒ„ãƒ¼ãƒ«
===============================================
èµ·å‹•: streamlit run a01_load_non_qa_rag_data.py --server.port=8502

ã€ä¸»è¦æ©Ÿèƒ½ã€‘
âœ… æ—¥æœ¬èªãƒ»è‹±èªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®å‡¦ç†
   - Wikipediaæ—¥æœ¬èªç‰ˆï¼ˆå‹•ä½œç¢ºèªæ¸ˆã¿ï¼‰
   - CC100æ—¥æœ¬èªï¼ˆå‹•ä½œç¢ºèªæ¸ˆã¿ï¼‰
   - CC-Newsè‹±èªãƒ‹ãƒ¥ãƒ¼ã‚¹ï¼ˆå‹•ä½œç¢ºèªæ¸ˆã¿ï¼‰
âœ… ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼ãƒ»å“è³ªãƒã‚§ãƒƒã‚¯
âœ… RAGç”¨ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºãƒ»å‰å‡¦ç†
âœ… ãƒˆãƒ¼ã‚¯ãƒ³ä½¿ç”¨é‡æ¨å®š
âœ… CSV/TXT/JSONãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå‡ºåŠ›

ã€å¯¾å¿œãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã€‘
1. wikipedia_ja: Wikipediaæ—¥æœ¬èªç‰ˆï¼ˆç™¾ç§‘äº‹å…¸çš„çŸ¥è­˜ï¼‰
2. japanese_text: CC100æ—¥æœ¬èªï¼ˆWebãƒ†ã‚­ã‚¹ãƒˆã‚³ãƒ¼ãƒ‘ã‚¹ï¼‰
3. cc_news: CC-Newsï¼ˆè‹±èªãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹ï¼‰
"""

import streamlit as st
import pandas as pd
import json
import io
from datetime import datetime
from pathlib import Path
import logging
from typing import Dict, List, Optional, Any

# ãƒ­ãƒ¼ã‚«ãƒ«ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
from helper_rag import (
    setup_page_config,
    setup_page_header,
    setup_sidebar_header,
    select_model,
    show_model_info,
    validate_data,
    load_dataset,
    estimate_token_usage,
    create_download_data,
    display_statistics,
    save_files_to_output,
    show_usage_instructions,
    clean_text,
    TokenManager,
    safe_execute
)

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


# ===================================================================
# éQ&Aå‹ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆè¨­å®š
# ===================================================================

class NonQARAGConfig:
    """éQ&Aå‹RAGãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®è¨­å®š"""

    DATASET_CONFIGS = {
        # Wikipediaæ—¥æœ¬èªç‰ˆ - å‹•ä½œç¢ºèªæ¸ˆã¿
        "wikipedia_ja" : {
            "name"            : "Wikipediaæ—¥æœ¬èªç‰ˆ",
            "icon"            : "ğŸ“š",
            "required_columns": ["title", "text"],
            "description"     : "Wikipediaæ—¥æœ¬èªç‰ˆã®è¨˜äº‹ãƒ‡ãƒ¼ã‚¿",
            "hf_dataset"      : "wikimedia/wikipedia",
            "hf_config"       : "20231101.ja",
            "split"           : "train",
            "streaming"       : True,
            "text_field"      : "text",
            "title_field"     : "title",
            "sample_size"     : 1000
        },

        # CC100æ—¥æœ¬èª - å‹•ä½œç¢ºèªæ¸ˆã¿
        "japanese_text": {
            "name"            : "æ—¥æœ¬èªWebãƒ†ã‚­ã‚¹ãƒˆï¼ˆCC100ï¼‰",
            "icon"            : "ğŸ“°",
            "required_columns": ["text"],
            "description"     : "æ—¥æœ¬èªWebãƒ†ã‚­ã‚¹ãƒˆã‚³ãƒ¼ãƒ‘ã‚¹",
            "hf_dataset"      : "range3/cc100-ja",
            "hf_config"       : None,
            "split"           : "train",
            "streaming"       : True,
            "text_field"      : "text",
            "title_field"     : None,
            "sample_size"     : 1000
        },

        # CC-Newsè‹±èªãƒ‹ãƒ¥ãƒ¼ã‚¹ - å‹•ä½œç¢ºèªæ¸ˆã¿
        "cc_news": {
            "name"            : "CC-Newsï¼ˆè‹±èªãƒ‹ãƒ¥ãƒ¼ã‚¹ï¼‰",
            "icon"            : "ğŸŒ",
            "required_columns": ["title", "text"],
            "description"     : "Common Crawlè‹±èªãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹",
            "hf_dataset"      : "cc_news",
            "hf_config"       : None,
            "split"           : "train",
            "streaming"       : True,
            "text_field"      : "text",
            "title_field"     : "title",
            "sample_size"     : 500
        }
    }

    @classmethod
    def get_config(cls, dataset_type: str) -> Dict[str, Any]:
        """ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆè¨­å®šã®å–å¾—"""
        return cls.DATASET_CONFIGS.get(dataset_type, {
            "name"            : "æœªçŸ¥ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ",
            "icon"            : "â“",
            "required_columns": [],
            "description"     : "æœªçŸ¥ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ",
            "hf_dataset"      : None,
            "hf_config"       : None,
            "split"           : "train",
            "port"            : 8500,
            "text_field"      : "text",
            "title_field"     : None,
            "sample_size"     : 1000
        })

    @classmethod
    def get_all_datasets(cls) -> List[str]:
        """å…¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚¿ã‚¤ãƒ—ã®ãƒªã‚¹ãƒˆã‚’å–å¾—"""
        return list(cls.DATASET_CONFIGS.keys())


# ===================================================================
# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆåˆ¥æ¤œè¨¼é–¢æ•°
# ===================================================================

def validate_wikipedia_data_specific(df: pd.DataFrame) -> List[str]:
    """Wikipediaç‰¹æœ‰ã®æ¤œè¨¼"""
    issues = []

    if 'text' in df.columns:
        # ãƒ†ã‚­ã‚¹ãƒˆã®é•·ã•ãƒã‚§ãƒƒã‚¯
        text_lengths = df['text'].str.len()
        avg_length = text_lengths.mean()

        if avg_length < 100:
            issues.append(f"âš ï¸ å¹³å‡ãƒ†ã‚­ã‚¹ãƒˆé•·ãŒçŸ­ã„: {avg_length:.0f}æ–‡å­—")
        else:
            issues.append(f"âœ… é©åˆ‡ãªãƒ†ã‚­ã‚¹ãƒˆé•·: å¹³å‡{avg_length:.0f}æ–‡å­—")

        # Wikipediaç‰¹æœ‰ã®ãƒãƒ¼ã‚¯ã‚¢ãƒƒãƒ—ãƒã‚§ãƒƒã‚¯
        wiki_markup = df['text'].str.contains('==|\\[\\[|\\]\\]', regex=True, na=False).sum()
        if wiki_markup > 0:
            percentage = (wiki_markup / len(df)) * 100
            issues.append(f"ğŸ’¡ Wikiãƒãƒ¼ã‚¯ã‚¢ãƒƒãƒ—å«ã‚€è¨˜äº‹: {wiki_markup}ä»¶ ({percentage:.1f}%)")

    if 'title' in df.columns:
        # ã‚¿ã‚¤ãƒˆãƒ«ã®é‡è¤‡ãƒã‚§ãƒƒã‚¯
        duplicates = df['title'].duplicated().sum()
        if duplicates > 0:
            issues.append(f"âš ï¸ é‡è¤‡ã‚¿ã‚¤ãƒˆãƒ«: {duplicates}ä»¶")

    return issues


def validate_news_data_specific(df: pd.DataFrame, dataset_type: str) -> List[str]:
    """ãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ç‰¹æœ‰ã®æ¤œè¨¼"""
    issues = []

    # ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã®ç‰¹å®šï¼ˆlivedoorã¯contentã€ãã®ä»–ã¯textï¼‰
    if 'content' in df.columns:
        text_field = 'content'
    elif 'body' in df.columns:
        text_field = 'body'
    else:
        text_field = 'text'

    if text_field in df.columns:
        # è¨˜äº‹ã®é•·ã•åˆ†æ
        text_lengths = df[text_field].str.len()
        avg_length = text_lengths.mean()

        issues.append(f"ğŸ“Š å¹³å‡è¨˜äº‹é•·: {avg_length:.0f}æ–‡å­—")

        # çŸ­ã™ãã‚‹è¨˜äº‹ã®æ¤œå‡º
        short_articles = (text_lengths < 100).sum()
        if short_articles > 0:
            percentage = (short_articles / len(df)) * 100
            issues.append(f"âš ï¸ çŸ­ã„è¨˜äº‹ï¼ˆ<100æ–‡å­—ï¼‰: {short_articles}ä»¶ ({percentage:.1f}%)")

    # ã‚«ãƒ†ã‚´ãƒªæƒ…å ±ï¼ˆlivedoorã®å ´åˆï¼‰
    if 'category' in df.columns:
        categories = df['category'].value_counts()
        issues.append(f"ğŸ“‚ ã‚«ãƒ†ã‚´ãƒªæ•°: {len(categories)}ç¨®é¡")
        top_3 = categories.head(3)
        for cat, count in top_3.items():
            issues.append(f"  - {cat}: {count}ä»¶")

    return issues


def validate_scientific_data_specific(df: pd.DataFrame, dataset_type: str) -> List[str]:
    """å­¦è¡“è«–æ–‡ãƒ‡ãƒ¼ã‚¿ç‰¹æœ‰ã®æ¤œè¨¼"""
    issues = []

    if 'abstract' in df.columns:
        # è¦æ—¨ã®é•·ã•åˆ†æ
        abstract_lengths = df['abstract'].str.len()
        avg_length = abstract_lengths.mean()

        issues.append(f"ğŸ“„ å¹³å‡è¦æ—¨é•·: {avg_length:.0f}æ–‡å­—")

        # å­¦è¡“ç”¨èªã®æ¤œå‡º
        academic_keywords = ['research', 'study', 'method', 'result', 'conclusion',
                             'ç ”ç©¶', 'æ–¹æ³•', 'çµæœ', 'è€ƒå¯Ÿ']
        has_keywords = df['abstract'].str.contains('|'.join(academic_keywords),
                                                   case=False, na=False).sum()
        percentage = (has_keywords / len(df)) * 100
        issues.append(f"ğŸ“š å­¦è¡“çš„ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰å«ã‚€: {has_keywords}ä»¶ ({percentage:.1f}%)")

    # PubMedç‰¹æœ‰
    if dataset_type == "pubmed" and 'abstract' in df.columns:
        # åŒ»å­¦ç”¨èªã®æ¤œå‡º
        medical_terms = ['patient', 'treatment', 'disease', 'clinical',
                         'æ‚£è€…', 'æ²»ç™‚', 'ç–¾æ‚£', 'è‡¨åºŠ']
        has_medical = df['abstract'].str.contains('|'.join(medical_terms),
                                                  case=False, na=False).sum()
        percentage = (has_medical / len(df)) * 100
        issues.append(f"ğŸ¥ åŒ»å­¦ç”¨èªå«ã‚€: {has_medical}ä»¶ ({percentage:.1f}%)")

    # arXivç‰¹æœ‰
    if dataset_type == "arxiv" and 'article' in df.columns:
        # æœ¬æ–‡ãŒå­˜åœ¨ã™ã‚‹ã‹
        has_article = df['article'].notna().sum()
        percentage = (has_article / len(df)) * 100
        issues.append(f"ğŸ“– æœ¬æ–‡ã‚ã‚Š: {has_article}ä»¶ ({percentage:.1f}%)")

    return issues


def validate_code_data_specific(df: pd.DataFrame) -> List[str]:
    """ã‚³ãƒ¼ãƒ‰ãƒ‡ãƒ¼ã‚¿ç‰¹æœ‰ã®æ¤œè¨¼"""
    issues = []

    if 'code' in df.columns:
        # ã‚³ãƒ¼ãƒ‰ã®é•·ã•åˆ†æ
        code_lengths = df['code'].str.len()
        avg_length = code_lengths.mean()
        issues.append(f"ğŸ’» å¹³å‡ã‚³ãƒ¼ãƒ‰é•·: {avg_length:.0f}æ–‡å­—")

        # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ–‡å­—åˆ—ã®å­˜åœ¨ç¢ºèª
        if 'func_documentation_string' in df.columns:
            has_docs = df['func_documentation_string'].notna().sum()
            percentage = (has_docs / len(df)) * 100
            issues.append(f"ğŸ“ ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚ã‚Š: {has_docs}ä»¶ ({percentage:.1f}%)")

    # ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°è¨€èªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®æ¤œå‡º
    if 'code' in df.columns:
        code_keywords = ['def ', 'class ', 'import ', 'function', 'return']
        has_keywords = df['code'].str.contains('|'.join(code_keywords),
                                               case=False, na=False).sum()
        percentage = (has_keywords / len(df)) * 100
        issues.append(f"ğŸ”§ ã‚³ãƒ¼ãƒ‰ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰å«ã‚€: {has_keywords}ä»¶ ({percentage:.1f}%)")

    return issues


def validate_stackoverflow_data_specific(df: pd.DataFrame) -> List[str]:
    """Stack Overflowç‰¹æœ‰ã®æ¤œè¨¼"""
    issues = []

    if 'body' in df.columns:
        # è³ªå•ã®é•·ã•åˆ†æ
        body_lengths = df['body'].str.len()
        avg_length = body_lengths.mean()
        issues.append(f"â“ å¹³å‡è³ªå•é•·: {avg_length:.0f}æ–‡å­—")

    # ã‚¿ã‚°æƒ…å ±
    if 'tags' in df.columns:
        has_tags = df['tags'].notna().sum()
        percentage = (has_tags / len(df)) * 100
        issues.append(f"ğŸ·ï¸ ã‚¿ã‚°ä»˜ã: {has_tags}ä»¶ ({percentage:.1f}%)")

        # äººæ°—ã‚¿ã‚°ã®åˆ†æ
        if has_tags > 0:
            all_tags = []
            for tags in df['tags'].dropna():
                if isinstance(tags, str):
                    all_tags.extend(tags.split(','))

            if all_tags:
                from collections import Counter
                top_tags = Counter(all_tags).most_common(5)
                issues.append("ğŸ” äººæ°—ã‚¿ã‚°Top5:")
                for tag, count in top_tags:
                    issues.append(f"  - {tag.strip()}: {count}ä»¶")

    # æŠ€è¡“ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®æ¤œå‡º
    if 'body' in df.columns:
        tech_keywords = ['python', 'javascript', 'java', 'error', 'function', 'code']
        has_tech = df['body'].str.contains('|'.join(tech_keywords),
                                           case=False, na=False).sum()
        percentage = (has_tech / len(df)) * 100
        issues.append(f"ğŸ’¡ æŠ€è¡“ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰å«ã‚€: {has_tech}ä»¶ ({percentage:.1f}%)")

    return issues


# ===================================================================
# ãƒ‡ãƒ¼ã‚¿å‡¦ç†é–¢æ•°
# ===================================================================

@safe_execute
def extract_text_content(df: pd.DataFrame, dataset_type: str) -> pd.DataFrame:
    """ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’æŠ½å‡º"""
    config = NonQARAGConfig.get_config(dataset_type)
    text_field = config["text_field"]
    title_field = config["title_field"]

    df_processed = df.copy()

    # ã‚¿ã‚¤ãƒˆãƒ«ã¨ãƒ†ã‚­ã‚¹ãƒˆã‚’çµåˆ
    if title_field and title_field in df.columns and text_field in df.columns:
        # ã‚¿ã‚¤ãƒˆãƒ«ãŒã‚ã‚‹å ´åˆã¯çµåˆ
        df_processed['Combined_Text'] = df_processed.apply(
            lambda row: f"{clean_text(str(row.get(title_field, '')))} {clean_text(str(row.get(text_field, '')))}".strip(),
            axis=1
        )
    elif text_field in df.columns:
        # ã‚¿ã‚¤ãƒˆãƒ«ãŒãªã„å ´åˆã¯ãƒ†ã‚­ã‚¹ãƒˆã®ã¿
        df_processed['Combined_Text'] = df_processed[text_field].apply(
            lambda x: clean_text(str(x)) if x is not None else ""
        )
    else:
        # ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
        # åˆ©ç”¨å¯èƒ½ãªãƒ†ã‚­ã‚¹ãƒˆç³»ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’æ¢ã™
        text_candidates = ['text', 'content', 'body', 'document', 'abstract', 'description']
        found_field = None
        for field in text_candidates:
            if field in df.columns:
                found_field = field
                break

        if found_field:
            df_processed['Combined_Text'] = df_processed[found_field].apply(
                lambda x: clean_text(str(x)) if x is not None else ""
            )
        else:
            # ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯å…¨ã‚«ãƒ©ãƒ ã‚’çµåˆ
            df_processed['Combined_Text'] = df_processed.apply(
                lambda row: " ".join([str(v) for v in row.values if v is not None]),
                axis=1
            )

    # ç©ºã®ãƒ†ã‚­ã‚¹ãƒˆã‚’é™¤å¤–
    df_processed = df_processed[df_processed['Combined_Text'].str.strip() != '']

    return df_processed


# ===================================================================
# ãƒ¡ã‚¤ãƒ³å‡¦ç†
# ===================================================================

def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†é–¢æ•°"""

    # åˆæœŸè¨­å®šï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚¿ã‚¤ãƒ—ï¼‰
    default_dataset = "japanese_text"

    # ãƒšãƒ¼ã‚¸è¨­å®š
    try:
        st.set_page_config(
            page_title="éQ&Aå‹RAGãƒ‡ãƒ¼ã‚¿å‡¦ç†",
            page_icon="ğŸ“š",
            layout="wide",
            initial_sidebar_state="expanded"
        )
    except st.errors.StreamlitAPIException:
        pass

    # ã‚µã‚¤ãƒ‰ãƒãƒ¼è¨­å®š
    with st.sidebar:
        st.title("ğŸ“š éQ&Aå‹ãƒ‡ãƒ¼ã‚¿å‡¦ç†")
        st.markdown("---")

        # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚¿ã‚¤ãƒ—é¸æŠ
        st.subheader("ğŸ“Š ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚¿ã‚¤ãƒ—é¸æŠ")

        dataset_options = NonQARAGConfig.get_all_datasets()
        dataset_labels = {
            dt: f"{NonQARAGConfig.get_config(dt)['icon']} {NonQARAGConfig.get_config(dt)['name']}"
            for dt in dataset_options
        }

        selected_dataset = st.selectbox(
            "å‡¦ç†ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚¿ã‚¤ãƒ—",
            options=dataset_options,
            format_func=lambda x: dataset_labels[x],
            help="å‡¦ç†ã—ãŸã„ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ã‚¿ã‚¤ãƒ—ã‚’é¸æŠã—ã¦ãã ã•ã„"
        )

        # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆè¨­å®šã‚’å–å¾—
        dataset_config = NonQARAGConfig.get_config(selected_dataset)

        # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæƒ…å ±è¡¨ç¤º
        st.info(f"""
        **é¸æŠä¸­ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ:**
        - ã‚¿ã‚¤ãƒ—: {dataset_config['name']}
        - ä¸»è¦ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰: {dataset_config['text_field']}
        - HuggingFace: {dataset_config['hf_dataset']}
        """)

        # ãƒ¢ãƒ‡ãƒ«é¸æŠ
        st.divider()
        selected_model = select_model()
        show_model_info(selected_model)

        # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå›ºæœ‰ã®ã‚ªãƒ—ã‚·ãƒ§ãƒ³
        st.divider()
        st.subheader("âš™ï¸ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå›ºæœ‰è¨­å®š")

        dataset_specific_options = {}

        if selected_dataset == "wikipedia_ja":
            dataset_specific_options['remove_markup'] = st.checkbox(
                "Wikiãƒãƒ¼ã‚¯ã‚¢ãƒƒãƒ—ã‚’é™¤å»",
                value=True,
                help="[[ãƒªãƒ³ã‚¯]]ã‚„==è¦‹å‡ºã—==ãªã©ã‚’é™¤å»"
            )
            dataset_specific_options['min_text_length'] = st.number_input(
                "æœ€å°ãƒ†ã‚­ã‚¹ãƒˆé•·",
                min_value=50,
                value=200,
                help="ã“ã®é•·ã•æœªæº€ã®è¨˜äº‹ã‚’é™¤å¤–"
            )

        elif selected_dataset == "japanese_text":
            dataset_specific_options['remove_urls'] = st.checkbox(
                "URLã‚’é™¤å»",
                value=True,
                help="ãƒ†ã‚­ã‚¹ãƒˆä¸­ã®URLã‚’é™¤å»"
            )
            dataset_specific_options['min_text_length'] = st.number_input(
                "æœ€å°ãƒ†ã‚­ã‚¹ãƒˆé•·",
                min_value=10,
                value=10,
                help="ã“ã®é•·ã•æœªæº€ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’é™¤å¤–"
            )

        elif selected_dataset == "cc_news":
            dataset_specific_options['remove_urls'] = st.checkbox(
                "URLã‚’é™¤å»",
                value=True,
                help="ãƒ†ã‚­ã‚¹ãƒˆä¸­ã®URLã‚’é™¤å»"
            )
            dataset_specific_options['min_text_length'] = st.number_input(
                "æœ€å°ãƒ†ã‚­ã‚¹ãƒˆé•·",
                min_value=50,
                value=100,
                help="ã“ã®é•·ã•æœªæº€ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’é™¤å¤–"
            )

    # ãƒ¡ã‚¤ãƒ³ã‚³ãƒ³ãƒ†ãƒ³ãƒ„
    st.title(f"{dataset_config['icon']} {dataset_config['name']}å‰å‡¦ç†ã‚¢ãƒ—ãƒª")
    st.caption("RAGï¼ˆRetrieval-Augmented Generationï¼‰ç”¨ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç† - éQ&Aå‹ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå¯¾å¿œ")
    st.markdown("---")

    # ä½¿ã„æ–¹ä¾‹ã‚’Expanderã§è¡¨ç¤º
    with st.expander("ğŸ“– **ä½¿ã„æ–¹ä¾‹**", expanded=False):
        st.markdown(f"""
        ### ğŸ¯ åŸºæœ¬çš„ãªä½¿ã„æ–¹

        1. **å·¦ãƒšã‚¤ãƒ³ã§è¨­å®š**
           - ğŸ“Š **ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚¿ã‚¤ãƒ—ã‚’é¸æŠ** ({dataset_config['name']})
           - ğŸ¤– ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠ
           - âš™ï¸ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå›ºæœ‰ã®è¨­å®šã‚’èª¿æ•´

        2. **å³ãƒšã‚¤ãƒ³ã§å‡¦ç†å®Ÿè¡Œ**

           **ğŸ“ ãƒ‡ãƒ¼ã‚¿ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰**
           - CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã€ã¾ãŸã¯
           - HuggingFaceã‹ã‚‰è‡ªå‹•ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
             - æ¨å¥¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ: `{dataset_config['hf_dataset']}`
             - Split: `{dataset_config['split']}`
             - ã‚µãƒ³ãƒ—ãƒ«æ•°: {dataset_config['sample_size']}ä»¶ç¨‹åº¦

           **ğŸ” ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼**
           - ãƒ‡ãƒ¼ã‚¿ã®å“è³ªã‚’ãƒã‚§ãƒƒã‚¯
           - å¿…é ˆãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã®ç¢ºèª
           - ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå›ºæœ‰ã®æ¤œè¨¼çµæœã‚’ç¢ºèª

           **âš™ï¸ å‰å‡¦ç†å®Ÿè¡Œ**
           - ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºãƒ»çµåˆ
           - ã‚¯ãƒ¬ãƒ³ã‚¸ãƒ³ã‚°å‡¦ç†
           - ã€ŒğŸš€ å‰å‡¦ç†ã‚’å®Ÿè¡Œã€ã‚’ã‚¯ãƒªãƒƒã‚¯

           **ğŸ“Š çµæœãƒ»ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰**
           - å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã‚’CSVã€TXTã€JSONå½¢å¼ã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
           - OUTPUTãƒ•ã‚©ãƒ«ãƒ€ã«ä¿å­˜

        ### ğŸ’¡ ãƒ’ãƒ³ãƒˆ
        - éQ&Aå‹ãƒ‡ãƒ¼ã‚¿ãªã®ã§ã€ã‚¿ã‚¤ãƒˆãƒ«ãƒ»æœ¬æ–‡ãƒ»è¦æ—¨ãªã©ã‚’é©åˆ‡ã«çµåˆã—ã¾ã™
        - ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«ã‚ˆã£ã¦ã¯å¤§é‡ã®ãƒ†ã‚­ã‚¹ãƒˆãŒå«ã¾ã‚Œã‚‹ãŸã‚ã€ã‚µãƒ³ãƒ—ãƒ«æ•°ã‚’èª¿æ•´ã—ã¦ãã ã•ã„
        - HuggingFaceã‹ã‚‰ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ãŸãƒ‡ãƒ¼ã‚¿ã¯`datasets/`ãƒ•ã‚©ãƒ«ãƒ€ã«ä¿å­˜ã•ã‚Œã¾ã™
        """)

    # ã‚¿ãƒ–è¨­å®š
    tab1, tab2, tab3, tab4 = st.tabs([
        "ğŸ“ ãƒ‡ãƒ¼ã‚¿ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰",
        "ğŸ” ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼",
        "âš™ï¸ å‰å‡¦ç†å®Ÿè¡Œ",
        "ğŸ“Š çµæœãƒ»ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰"
    ])

    # Tab 1: ãƒ‡ãƒ¼ã‚¿ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
    with tab1:
        st.header("ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰")

        # ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
        uploaded_file = st.file_uploader(
            f"{dataset_config['name']}ã®CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’é¸æŠ",
            type=['csv'],
            help=f"ä¸»è¦ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰: {dataset_config['text_field']}"
        )

        # HuggingFaceã‹ã‚‰è‡ªå‹•ãƒ­ãƒ¼ãƒ‰
        st.divider()

        if selected_dataset == "custom":
            st.info("ğŸ“ ã‚«ã‚¹ã‚¿ãƒ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãŒé¸æŠã•ã‚Œã¦ã„ã¾ã™ã€‚ä¸Šè¨˜ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„ã€‚")
        else:
            st.subheader("ã¾ãŸã¯ã€HuggingFaceã‹ã‚‰è‡ªå‹•ãƒ­ãƒ¼ãƒ‰")

            # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šã‚’è¡¨ç¤º
            st.info(f"""
            **æ¨å¥¨è¨­å®š:**
            - ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå: `{dataset_config['hf_dataset']}`
            - Config: `{dataset_config['hf_config'] or 'ãªã—'}`
            - Split: `{dataset_config.get('split', 'train')}`
            - ã‚µãƒ³ãƒ—ãƒ«æ•°: {dataset_config['sample_size']}ä»¶
            """)

            dataset_name = st.text_input(
                "HuggingFaceãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå",
                value=dataset_config['hf_dataset'] or "",
                placeholder="ä¾‹: wikimedia/wikipedia ã¾ãŸã¯ range3/cc100-ja"
            )

            col1, col2, col3 = st.columns([1, 1, 1])
            with col1:
                split_name = st.text_input("Splitå",
                                          value=dataset_config.get('split', 'train'),
                                          placeholder="train")
            with col2:
                sample_size = st.number_input("ã‚µãƒ³ãƒ—ãƒ«æ•°", min_value=10,
                                            value=dataset_config['sample_size'])
            with col3:
                config_name = st.text_input("Configå",
                                          value=dataset_config['hf_config'] or "",
                                          placeholder="ä»»æ„")

            if st.button("ğŸ“¥ HuggingFaceã‹ã‚‰ãƒ­ãƒ¼ãƒ‰", type="primary"):
                try:
                    from datasets import load_dataset as hf_load_dataset

                    # å…¥åŠ›å€¤ã®æ¤œè¨¼
                    if not dataset_name:
                        st.error("ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆåã‚’å…¥åŠ›ã—ã¦ãã ã•ã„")
                        st.stop()

                    if not split_name:
                        split_name = "train"

                    with st.spinner(f"HuggingFaceã‹ã‚‰{dataset_name}ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ä¸­..."):
                        # ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒ¢ãƒ¼ãƒ‰ã§ç¢ºå®Ÿã«ãƒ­ãƒ¼ãƒ‰
                        samples = []

                        # å‹•ä½œç¢ºèªæ¸ˆã¿ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ã¿å‡¦ç†
                        if dataset_name == "wikimedia/wikipedia" or dataset_name == "wikipedia":
                            # Wikipediaæ—¥æœ¬èªç‰ˆ
                            actual_dataset = "wikimedia/wikipedia"
                            actual_config = config_name if config_name else "20231101.ja"

                            st.info(f"ğŸ“¥ {actual_dataset}ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­ (config: {actual_config})...")
                            dataset = hf_load_dataset(actual_dataset, actual_config, split=split_name, streaming=True)

                        elif dataset_name == "range3/cc100-ja":
                            # CC100æ—¥æœ¬èª
                            st.info(f"ğŸ“¥ {dataset_name}ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­...")
                            dataset = hf_load_dataset(dataset_name, split=split_name, streaming=True)

                        elif dataset_name == "cc_news":
                            # CC-Newsï¼ˆå‹•ä½œç¢ºèªæ¸ˆã¿ï¼‰
                            st.info(f"ğŸ“¥ {dataset_name}ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­...")
                            if config_name:
                                dataset = hf_load_dataset(dataset_name, config_name, split=split_name, streaming=True)
                            else:
                                dataset = hf_load_dataset(dataset_name, split=split_name, streaming=True)

                        else:
                            # ãã®ä»–ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼ˆéæ¨å¥¨ï¼‰
                            st.warning("âš ï¸ ã“ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯å‹•ä½œä¿è¨¼å¤–ã§ã™")
                            if config_name:
                                dataset = hf_load_dataset(dataset_name, config_name, split=split_name, streaming=True)
                            else:
                                dataset = hf_load_dataset(dataset_name, split=split_name, streaming=True)

                        # ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
                        progress_bar = st.progress(0)
                        for i, item in enumerate(dataset):
                            if i >= sample_size:
                                break
                            samples.append(item)
                            progress_bar.progress((i + 1) / sample_size)

                        df = pd.DataFrame(samples)
                        progress_bar.empty()

                    # datasetsãƒ•ã‚©ãƒ«ãƒ€ã«ä¿å­˜
                    if df is not None and len(df) > 0:
                        datasets_dir = Path("datasets")
                        datasets_dir.mkdir(exist_ok=True)

                        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                        safe_dataset_name = dataset_name.replace("/", "_").replace("-", "_")
                        csv_filename = f"{safe_dataset_name}_{split_name}_{sample_size}_{timestamp}.csv"
                        csv_path = datasets_dir / csv_filename

                        df.to_csv(csv_path, index=False)
                        st.info(f"ğŸ“‚ ãƒ‡ãƒ¼ã‚¿ã‚’datasets/{csv_filename}ã«ä¿å­˜ã—ã¾ã—ãŸ")

                        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä¿å­˜
                        metadata = {
                            'dataset_name' : dataset_name,
                            'dataset_type' : selected_dataset,
                            'config'       : config_name,
                            'split'        : split_name,
                            'sample_size'  : sample_size,
                            'actual_size'  : len(df),
                            'downloaded_at': datetime.now().isoformat(),
                            'columns'      : df.columns.tolist()
                        }

                        metadata_filename = f"{safe_dataset_name}_{split_name}_{sample_size}_{timestamp}_metadata.json"
                        metadata_path = datasets_dir / metadata_filename

                        with open(metadata_path, 'w', encoding='utf-8') as f:
                            json.dump(metadata, f, ensure_ascii=False, indent=2)

                        st.session_state['uploaded_data'] = df
                        st.session_state['uploaded_columns'] = df.columns.tolist()
                        st.success(f"âœ… {len(df)}ä»¶ã®ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ­ãƒ¼ãƒ‰ã—ã€datasets/ãƒ•ã‚©ãƒ«ãƒ€ã«ä¿å­˜ã—ã¾ã—ãŸ")

                except Exception as e:
                    error_msg = str(e)
                    if "Dataset scripts are no longer supported" in error_msg:
                        st.error("âŒ ã“ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯ã‚¹ã‚¯ãƒªãƒ—ãƒˆãƒ™ãƒ¼ã‚¹ã§å»ƒæ­¢ã•ã‚Œã¦ã„ã¾ã™")
                        st.info("""
                        ğŸ’¡ **å‹•ä½œç¢ºèªæ¸ˆã¿ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ã”åˆ©ç”¨ãã ã•ã„ï¼š**
                        - `wikimedia/wikipedia` (Config: 20231101.ja)
                        - `range3/cc100-ja`
                        """)
                    elif "doesn't exist on the Hub" in error_msg:
                        st.error("âŒ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                        st.info("""
                        ğŸ’¡ **ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆåã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚æ¨å¥¨ï¼š**
                        - `wikimedia/wikipedia`
                        - `range3/cc100-ja`
                        """)
                    else:
                        st.error(f"ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ãƒ­ãƒ¼ãƒ‰ã«å¤±æ•—ã—ã¾ã—ãŸ: {error_msg}")
                        st.info("ğŸ’¡ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆåã€configåã€splitåã‚’ç¢ºèªã—ã¦ãã ã•ã„")

        # ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã®å‡¦ç†
        if uploaded_file is not None:
            df = pd.read_csv(uploaded_file)
            st.session_state['uploaded_data'] = df
            st.session_state['uploaded_columns'] = df.columns.tolist()
            st.success(f"âœ… ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¾ã—ãŸ: {uploaded_file.name}")

        # ãƒ‡ãƒ¼ã‚¿ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼
        if 'uploaded_data' in st.session_state:
            df = st.session_state['uploaded_data']
            st.subheader("ğŸ“‹ ãƒ‡ãƒ¼ã‚¿ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼")
            st.info(f"ãƒ‡ãƒ¼ã‚¿ä»¶æ•°: {len(df)}ä»¶ | ã‚«ãƒ©ãƒ æ•°: {len(df.columns)}åˆ—")
            st.dataframe(df.head(10), use_container_width=True)

            # ã‚«ãƒ©ãƒ æƒ…å ±
            with st.expander("ğŸ“Š ã‚«ãƒ©ãƒ è©³ç´°"):
                col_info = pd.DataFrame({
                    'ã‚«ãƒ©ãƒ å'  : df.columns,
                    'ãƒ‡ãƒ¼ã‚¿å‹'  : df.dtypes.astype(str),
                    'éNULLæ•°'  : df.count(),
                    'NULLæ•°'    : df.isnull().sum(),
                    'ãƒ¦ãƒ‹ãƒ¼ã‚¯æ•°': [df[col].nunique() for col in df.columns]
                })
                st.dataframe(col_info, use_container_width=True)

    # Tab 2: ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼
    with tab2:
        st.header("ãƒ‡ãƒ¼ã‚¿å“è³ªãƒã‚§ãƒƒã‚¯")

        if 'uploaded_data' not in st.session_state:
            st.warning("âš ï¸ ã¾ãšãƒ‡ãƒ¼ã‚¿ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„")
        else:
            df = st.session_state['uploaded_data']

            # åŸºæœ¬æ¤œè¨¼
            st.subheader("ğŸ“‹ åŸºæœ¬æ¤œè¨¼")
            issues = validate_data(df, selected_dataset)

            # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå›ºæœ‰ã®æ¤œè¨¼
            st.subheader(f"ğŸ” {dataset_config['name']}å›ºæœ‰ã®æ¤œè¨¼")

            if selected_dataset == "wikipedia_ja":
                specific_issues = validate_wikipedia_data_specific(df)
            elif selected_dataset in ["japanese_text", "cc_news"]:
                specific_issues = validate_news_data_specific(df, selected_dataset)
            else:
                # ãã®ä»–ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®æ¤œè¨¼
                specific_issues = []
                if 'text' in df.columns:
                    specific_issues.append("âœ… ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’æ¤œå‡ºã—ã¾ã—ãŸ")

            issues.extend(specific_issues)

            # æ¤œè¨¼çµæœã®è¡¨ç¤º
            if issues:
                for issue in issues:
                    if "âš ï¸" in issue or "âŒ" in issue:
                        st.warning(issue)
                    elif "âœ…" in issue:
                        st.success(issue)
                    else:
                        st.info(issue)

            # ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã®ã‚µãƒ³ãƒ—ãƒ«è¡¨ç¤º
            st.subheader("ğŸ“ ãƒ†ã‚­ã‚¹ãƒˆã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®ã‚µãƒ³ãƒ—ãƒ«")
            text_field = dataset_config['text_field']
            if text_field in df.columns:
                with st.expander(f"{text_field} ã®ã‚µãƒ³ãƒ—ãƒ«ï¼ˆæœ€åˆã®3ä»¶ï¼‰"):
                    for i, value in enumerate(df[text_field].head(3), 1):
                        st.text(f"[{i}] {str(value)[:500]}...")  # æœ€åˆã®500æ–‡å­—ã®ã¿è¡¨ç¤º

    # Tab 3: å‰å‡¦ç†å®Ÿè¡Œ
    with tab3:
        st.header("RAGç”¨ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†")

        if 'uploaded_data' not in st.session_state:
            st.warning("âš ï¸ ã¾ãšãƒ‡ãƒ¼ã‚¿ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„")
        else:
            df = st.session_state['uploaded_data']

            # å‰å‡¦ç†ã‚ªãƒ—ã‚·ãƒ§ãƒ³
            st.subheader("âš™ï¸ å‰å‡¦ç†è¨­å®š")

            col1, col2 = st.columns(2)
            with col1:
                remove_short_text = st.checkbox(
                    "çŸ­ã„ãƒ†ã‚­ã‚¹ãƒˆã‚’é™¤å¤–",
                    value=True,
                    help="æŒ‡å®šæ–‡å­—æ•°æœªæº€ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’é™¤å¤–"
                )
                if remove_short_text:
                    min_length = st.number_input(
                        "æœ€å°æ–‡å­—æ•°",
                        min_value=10,
                        value=100,
                        help="ã“ã®æ–‡å­—æ•°æœªæº€ã¯é™¤å¤–"
                    )

            with col2:
                remove_duplicates = st.checkbox(
                    "é‡è¤‡ã‚’é™¤å»",
                    value=True,
                    help="å®Œå…¨ã«åŒã˜ãƒ†ã‚­ã‚¹ãƒˆã‚’é™¤å¤–"
                )

            # å‡¦ç†å®Ÿè¡Œãƒœã‚¿ãƒ³
            if st.button("ğŸš€ å‰å‡¦ç†ã‚’å®Ÿè¡Œ", type="primary"):
                with st.spinner("å‡¦ç†ä¸­..."):
                    try:
                        # ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡º
                        df_processed = extract_text_content(df, selected_dataset)

                        # çŸ­ã„ãƒ†ã‚­ã‚¹ãƒˆã®é™¤å¤–
                        if remove_short_text:
                            before_len = len(df_processed)
                            df_processed = df_processed[
                                df_processed['Combined_Text'].str.len() >= min_length
                                ]
                            removed = before_len - len(df_processed)
                            if removed > 0:
                                st.info(f"ğŸ“Š {removed}ä»¶ã®çŸ­ã„ãƒ†ã‚­ã‚¹ãƒˆã‚’é™¤å¤–ã—ã¾ã—ãŸ")

                        # é‡è¤‡é™¤å»
                        if remove_duplicates:
                            before_len = len(df_processed)
                            df_processed = df_processed.drop_duplicates(subset=['Combined_Text'])
                            removed = before_len - len(df_processed)
                            if removed > 0:
                                st.info(f"ğŸ“Š {removed}ä»¶ã®é‡è¤‡ãƒ†ã‚­ã‚¹ãƒˆã‚’é™¤å¤–ã—ã¾ã—ãŸ")

                        # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãƒªã‚»ãƒƒãƒˆ
                        df_processed = df_processed.reset_index(drop=True)

                        # ã‚»ãƒƒã‚·ãƒ§ãƒ³ã«ä¿å­˜
                        st.session_state['processed_data'] = df_processed
                        st.session_state['processing_config'] = {
                            'dataset_type'     : selected_dataset,
                            'options'          : dataset_specific_options,
                            'remove_short_text': remove_short_text,
                            'min_length'       : min_length if remove_short_text else 0,
                            'remove_duplicates': remove_duplicates
                        }

                        st.success(f"âœ… å‰å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸï¼ï¼ˆ{len(df_processed)}ä»¶ï¼‰")

                    except Exception as e:
                        st.error(f"å‰å‡¦ç†ã‚¨ãƒ©ãƒ¼: {str(e)}")
                        logger.error(f"å‰å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")

            # å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã®ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼
            if 'processed_data' in st.session_state:
                df_processed = st.session_state['processed_data']

                st.subheader("ğŸ“‹ å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼")
                st.dataframe(df_processed[['Combined_Text']].head(10), use_container_width=True)

                # ãƒˆãƒ¼ã‚¯ãƒ³ä½¿ç”¨é‡æ¨å®š
                st.subheader("ğŸ’° ãƒˆãƒ¼ã‚¯ãƒ³ä½¿ç”¨é‡æ¨å®š")
                estimate_token_usage(df_processed, selected_model)

                # ãƒ†ã‚­ã‚¹ãƒˆé•·ã®åˆ†å¸ƒ
                st.subheader("ğŸ“Š ãƒ†ã‚­ã‚¹ãƒˆé•·ã®åˆ†å¸ƒ")
                text_lengths = df_processed['Combined_Text'].str.len()

                col1, col2, col3, col4 = st.columns(4)
                with col1:
                    st.metric("å¹³å‡", f"{text_lengths.mean():.0f}æ–‡å­—")
                with col2:
                    st.metric("æœ€å°", f"{text_lengths.min():,}æ–‡å­—")
                with col3:
                    st.metric("æœ€å¤§", f"{text_lengths.max():,}æ–‡å­—")
                with col4:
                    st.metric("ä¸­å¤®å€¤", f"{text_lengths.median():.0f}æ–‡å­—")

    # Tab 4: çµæœãƒ»ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
    with tab4:
        st.header("å‡¦ç†çµæœã¨ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰")

        if 'processed_data' not in st.session_state:
            st.warning("âš ï¸ ã¾ãšå‰å‡¦ç†ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„")
        else:
            df_processed = st.session_state['processed_data']
            config = st.session_state['processing_config']

            # å‡¦ç†ã‚µãƒãƒªãƒ¼
            st.subheader("ğŸ“Š å‡¦ç†ã‚µãƒãƒªãƒ¼")
            col1, col2, col3 = st.columns(3)
            with col1:
                st.metric("å‡¦ç†ä»¶æ•°", f"{len(df_processed):,}ä»¶")
            with col2:
                original_count = len(st.session_state.get('uploaded_data', []))
                removed = original_count - len(df_processed)
                st.metric("é™¤å¤–ä»¶æ•°", f"{removed:,}ä»¶")
            with col3:
                retention_rate = (len(df_processed) / original_count * 100) if original_count > 0 else 0
                st.metric("æ®‹å­˜ç‡", f"{retention_rate:.1f}%")

            # ãƒ•ã‚¡ã‚¤ãƒ«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
            st.subheader("ğŸ“¥ ãƒ•ã‚¡ã‚¤ãƒ«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰")

            # CSVãƒ‡ãƒ¼ã‚¿ä½œæˆ
            csv_buffer = io.StringIO()
            df_processed.to_csv(csv_buffer, index=False)
            csv_data = csv_buffer.getvalue()

            # ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ä½œæˆ
            text_data = '\n'.join(df_processed['Combined_Text'].dropna().astype(str))

            # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä½œæˆ
            metadata = {
                'dataset_type'  : config['dataset_type'],
                'dataset_name'  : dataset_config['name'],
                'processed_at'  : datetime.now().isoformat(),
                'row_count'     : len(df_processed),
                'original_count': original_count,
                'removed_count' : removed,
                'config'        : config
            }

            col1, col2, col3 = st.columns(3)

            with col1:
                st.download_button(
                    label="ğŸ“„ CSVãƒ•ã‚¡ã‚¤ãƒ«",
                    data=csv_data,
                    file_name=f"preprocessed_{config['dataset_type']}.csv",
                    mime="text/csv"
                )

            with col2:
                st.download_button(
                    label="ğŸ“ ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«",
                    data=text_data,
                    file_name=f"{config['dataset_type']}.txt",
                    mime="text/plain"
                )

            with col3:
                st.download_button(
                    label="ğŸ“‹ ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿(JSON)",
                    data=json.dumps(metadata, ensure_ascii=False, indent=2),
                    file_name=f"metadata_{config['dataset_type']}.json",
                    mime="application/json"
                )

            # OUTPUTãƒ•ã‚©ãƒ«ãƒ€ã¸ã®ä¿å­˜
            st.divider()
            if st.button("ğŸ’¾ OUTPUTãƒ•ã‚©ãƒ«ãƒ€ã«ä¿å­˜", type="primary"):
                saved_files = save_files_to_output(
                    df_processed,
                    config['dataset_type'],
                    csv_data,
                    text_data
                )

                if saved_files:
                    st.success("âœ… ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜ã—ã¾ã—ãŸï¼š")
                    for file_type, file_path in saved_files.items():
                        st.write(f"â€¢ {file_path}")
                else:
                    st.error("âŒ ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜ã«å¤±æ•—ã—ã¾ã—ãŸ")

            # ãƒ‡ãƒ¼ã‚¿ã‚µãƒ³ãƒ—ãƒ«è¡¨ç¤º
            st.divider()
            st.subheader("ğŸ“ å‡¦ç†æ¸ˆã¿ãƒ†ã‚­ã‚¹ãƒˆã®ã‚µãƒ³ãƒ—ãƒ«")
            for i, text in enumerate(df_processed['Combined_Text'].head(3), 1):
                with st.expander(f"ã‚µãƒ³ãƒ—ãƒ« {i}"):
                    st.text(str(text)[:1000] + "..." if len(str(text)) > 1000 else str(text))


if __name__ == "__main__":
    main()
