# helper_rag.py
# RAGãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†ã®å…±é€šæ©Ÿèƒ½
# -----------------------------------------

import streamlit as st
import pandas as pd
import re
import io
import logging
import json
import os
from typing import List, Dict, Any, Optional, Tuple
from pathlib import Path
from datetime import datetime
from functools import wraps

# ===================================================================
# åŸºæœ¬ãƒ­ã‚°è¨­å®š
# ===================================================================
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


# ==================================================
# è¨­å®šç®¡ç†ã‚¯ãƒ©ã‚¹ï¼ˆå…±é€šï¼‰
# ==================================================
class AppConfig:
    """ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³è¨­å®šï¼ˆå…¨ã‚¢ãƒ—ãƒªå…±é€šï¼‰"""

    # åˆ©ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«
    AVAILABLE_MODELS = [
        "gpt-5-mini",
        "gpt-5-nano",
        "gpt-5",
        "gpt-4o",
        "gpt-4o-mini",
        "gpt-4o-audio-preview",
        "gpt-4o-mini-audio-preview",
        "gpt-4.1",
        "gpt-4.1-mini",
        "o1",
        "o1-mini",
        "o3",
        "o3-mini",
        "o4",
        "o4-mini"
    ]

    DEFAULT_MODEL = "gpt-5-mini"

    # ãƒ¢ãƒ‡ãƒ«æ–™é‡‘ï¼ˆ1000ãƒˆãƒ¼ã‚¯ãƒ³ã‚ãŸã‚Šã®ãƒ‰ãƒ«ï¼‰
    MODEL_PRICING = {
        "gpt-5"                    : {"input": 0.01, "output": 0.03},
        "gpt-5-mini"               : {"input": 0.0001, "output": 0.0004},
        "gpt-5-nano"               : {"input": 0.00005, "output": 0.0002},
        "gpt-4o"                   : {"input": 0.005, "output": 0.015},
        "gpt-4o-mini"              : {"input": 0.00015, "output": 0.0006},
        "gpt-4o-audio-preview"     : {"input": 0.01, "output": 0.02},
        "gpt-4o-mini-audio-preview": {"input": 0.00025, "output": 0.001},
        "gpt-4.1"                  : {"input": 0.0025, "output": 0.01},
        "gpt-4.1-mini"             : {"input": 0.0001, "output": 0.0004},
        "o1"                       : {"input": 0.015, "output": 0.06},
        "o1-mini"                  : {"input": 0.003, "output": 0.012},
        "o3"                       : {"input": 0.03, "output": 0.12},
        "o3-mini"                  : {"input": 0.006, "output": 0.024},
        "o4"                       : {"input": 0.05, "output": 0.20},
        "o4-mini"                  : {"input": 0.01, "output": 0.04},
    }

    # ãƒ¢ãƒ‡ãƒ«åˆ¶é™
    MODEL_LIMITS = {
        "gpt-5"                    : {"max_tokens": 256000, "max_output": 8192},
        "gpt-5-mini"               : {"max_tokens": 128000, "max_output": 4096},
        "gpt-5-nano"               : {"max_tokens": 64000, "max_output": 2048},
        "gpt-4o"                   : {"max_tokens": 128000, "max_output": 4096},
        "gpt-4o-mini"              : {"max_tokens": 128000, "max_output": 4096},
        "gpt-4o-audio-preview"     : {"max_tokens": 128000, "max_output": 4096},
        "gpt-4o-mini-audio-preview": {"max_tokens": 128000, "max_output": 4096},
        "gpt-4.1"                  : {"max_tokens": 128000, "max_output": 4096},
        "gpt-4.1-mini"             : {"max_tokens": 128000, "max_output": 4096},
        "o1"                       : {"max_tokens": 128000, "max_output": 32768},
        "o1-mini"                  : {"max_tokens": 128000, "max_output": 65536},
        "o3"                       : {"max_tokens": 200000, "max_output": 100000},
        "o3-mini"                  : {"max_tokens": 200000, "max_output": 100000},
        "o4"                       : {"max_tokens": 256000, "max_output": 128000},
        "o4-mini"                  : {"max_tokens": 256000, "max_output": 128000},
    }

    @classmethod
    def get_model_limits(cls, model: str) -> Dict[str, int]:
        """ãƒ¢ãƒ‡ãƒ«ã®åˆ¶é™ã‚’å–å¾—"""
        return cls.MODEL_LIMITS.get(model, {"max_tokens": 128000, "max_output": 4096})

    @classmethod
    def get_model_pricing(cls, model: str) -> Dict[str, float]:
        """ãƒ¢ãƒ‡ãƒ«ã®æ–™é‡‘ã‚’å–å¾—"""
        return cls.MODEL_PRICING.get(model, {"input": 0.00015, "output": 0.0006})


# ==================================================
# RAGè¨­å®šã‚¯ãƒ©ã‚¹ï¼ˆå…¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå¯¾å¿œï¼‰
# ==================================================
class RAGConfig:
    """RAGãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†ã®è¨­å®šï¼ˆå…¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆçµ±åˆï¼‰"""

    DATASET_CONFIGS = {
        # ã‚«ã‚¹ã‚¿ãƒãƒ¼ã‚µãƒãƒ¼ãƒˆFAQ
        "customer_support_faq": {
            "name"            : "ã‚«ã‚¹ã‚¿ãƒãƒ¼ã‚µãƒãƒ¼ãƒˆãƒ»FAQ",
            "icon"            : "ğŸ’¬",
            "required_columns": ["question", "answer"],
            "description"     : "ã‚«ã‚¹ã‚¿ãƒãƒ¼ã‚µãƒãƒ¼ãƒˆFAQãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ",
            "combine_template": "{question} {answer}",
            "port"            : 8501
        },

        # åŒ»ç™‚QA
        "medical_qa"          : {
            "name"            : "åŒ»ç™‚QAãƒ‡ãƒ¼ã‚¿",
            "icon"            : "ğŸ¥",
            "required_columns": ["Question", "Complex_CoT", "Response"],
            "description"     : "åŒ»ç™‚è³ªå•å›ç­”ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ",
            "combine_template": "{question} {complex_cot} {response}",
            "port"            : 8503
        },

        # ç§‘å­¦ãƒ»æŠ€è¡“QA
        "sciq_qa"             : {
            "name"            : "ç§‘å­¦ãƒ»æŠ€è¡“QAï¼ˆSciQï¼‰",
            "icon"            : "ğŸ”¬",
            "required_columns": ["question", "correct_answer"],
            "description"     : "ç§‘å­¦ãƒ»æŠ€è¡“è³ªå•å›ç­”ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ",
            "combine_template": "{question} {correct_answer}",
            "port"            : 8504
        },

        # æ³•å¾‹ãƒ»åˆ¤ä¾‹QA
        "legal_qa"            : {
            "name"            : "æ³•å¾‹ãƒ»åˆ¤ä¾‹QA",
            "icon"            : "âš–ï¸",
            "required_columns": ["question", "answer"],
            "description"     : "æ³•å¾‹ãƒ»åˆ¤ä¾‹è³ªå•å›ç­”ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ",
            "combine_template": "{question} {answer}",
            "port"            : 8505
        },
        
        # TriviaQA
        "trivia_qa"           : {
            "name"            : "é›‘å­¦QAï¼ˆTriviaQAï¼‰",
            "icon"            : "ğŸ¯",
            "required_columns": ["question", "answer"],
            "description"     : "é›‘å­¦è³ªå•å›ç­”ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ",
            "combine_template": "{question} {answer} {entity_pages} {search_results}",
            "port"            : 8506
        }
    }

    @classmethod
    def get_config(cls, dataset_type: str) -> Dict[str, Any]:
        """ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆè¨­å®šã®å–å¾—"""
        return cls.DATASET_CONFIGS.get(dataset_type, {
            "name"            : "æœªçŸ¥ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ",
            "icon"            : "â“",
            "required_columns": [],
            "description"     : "æœªçŸ¥ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ",
            "combine_template": "{}",
            "port"            : 8500
        })

    @classmethod
    def get_all_datasets(cls) -> List[str]:
        """å…¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚¿ã‚¤ãƒ—ã®ãƒªã‚¹ãƒˆã‚’å–å¾—"""
        return list(cls.DATASET_CONFIGS.keys())

    @classmethod
    def get_dataset_by_port(cls, port: int) -> Optional[str]:
        """ãƒãƒ¼ãƒˆç•ªå·ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚¿ã‚¤ãƒ—ã‚’å–å¾—"""
        for dataset_type, config in cls.DATASET_CONFIGS.items():
            if config.get("port") == port:
                return dataset_type
        return None


# ==================================================
# ãƒˆãƒ¼ã‚¯ãƒ³ç®¡ç†ã‚¯ãƒ©ã‚¹ï¼ˆå…±é€šï¼‰
# ==================================================
class TokenManager:
    """ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã®ç®¡ç†ï¼ˆç°¡æ˜“ç‰ˆï¼‰"""

    @staticmethod
    def count_tokens(text: str, model: str = None) -> int:
        """ãƒ†ã‚­ã‚¹ãƒˆã®ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆï¼ˆç°¡æ˜“æ¨å®šï¼‰"""
        if not text:
            return 0

        # ç°¡æ˜“æ¨å®š: æ—¥æœ¬èªæ–‡å­—ã¯0.5ãƒˆãƒ¼ã‚¯ãƒ³ã€è‹±æ•°å­—ã¯0.25ãƒˆãƒ¼ã‚¯ãƒ³
        japanese_chars = len([c for c in text if ord(c) > 127])
        english_chars = len(text) - japanese_chars
        estimated_tokens = int(japanese_chars * 0.5 + english_chars * 0.25)

        # æœ€ä½1ãƒˆãƒ¼ã‚¯ãƒ³ã¯å¿…è¦
        return max(1, estimated_tokens)

    @staticmethod
    def estimate_cost(input_tokens: int, output_tokens: int, model: str) -> float:
        """APIä½¿ç”¨ã‚³ã‚¹ãƒˆã®æ¨å®š"""
        pricing = AppConfig.get_model_pricing(model)
        input_cost = (input_tokens / 1000) * pricing["input"]
        output_cost = (output_tokens / 1000) * pricing["output"]
        return input_cost + output_cost


# ==================================================
# ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ãƒ‡ã‚³ãƒ¬ãƒ¼ã‚¿ï¼ˆå…±é€šï¼‰
# ==================================================
def safe_execute(func):
    """å®‰å…¨å®Ÿè¡Œãƒ‡ã‚³ãƒ¬ãƒ¼ã‚¿"""

    @wraps(func)
    def wrapper(*args, **kwargs):
        try:
            return func(*args, **kwargs)
        except Exception as e:
            logger.error(f"Error in {func.__name__}: {str(e)}")
            st.error(f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
            return None

    return wrapper


# ==================================================
# UIé–¢æ•°ç¾¤ï¼ˆå…±é€šï¼‰
# ==================================================
def select_model(key: str = "model_selection") -> str:
    """ãƒ¢ãƒ‡ãƒ«é¸æŠUI"""
    models = AppConfig.AVAILABLE_MODELS
    default_model = AppConfig.DEFAULT_MODEL

    try:
        default_index = models.index(default_model)
    except ValueError:
        default_index = 0

    selected = st.sidebar.selectbox(
        "ğŸ¤– ãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠ",
        models,
        index=default_index,
        key=key,
        help="åˆ©ç”¨ã™ã‚‹OpenAIãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠã—ã¦ãã ã•ã„"
    )

    return selected


def show_model_info(selected_model: str) -> None:
    """é¸æŠã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã®æƒ…å ±ã‚’è¡¨ç¤º"""
    try:
        limits = AppConfig.get_model_limits(selected_model)
        pricing = AppConfig.get_model_pricing(selected_model)

        with st.sidebar.expander("ğŸ“Š é¸æŠãƒ¢ãƒ‡ãƒ«æƒ…å ±", expanded=False):
            # åŸºæœ¬æƒ…å ±
            col1, col2 = st.columns(2)
            with col1:
                st.write("**æœ€å¤§å…¥åŠ›**")
                st.write(f"{limits['max_tokens']:,}")
            with col2:
                st.write("**æœ€å¤§å‡ºåŠ›**")
                st.write(f"{limits['max_output']:,}")

            # æ–™é‡‘æƒ…å ±
            st.write("**æ–™é‡‘ï¼ˆ1000ãƒˆãƒ¼ã‚¯ãƒ³ï¼‰**")
            st.write(f"- å…¥åŠ›: ${pricing['input']:.5f}")
            st.write(f"- å‡ºåŠ›: ${pricing['output']:.5f}")

            # ãƒ¢ãƒ‡ãƒ«ç‰¹æ€§
            if selected_model.startswith("o"):
                st.info("ğŸ§  æ¨è«–ç‰¹åŒ–ãƒ¢ãƒ‡ãƒ«")
                st.caption("é«˜åº¦ãªæ¨è«–ã‚¿ã‚¹ã‚¯ã«æœ€é©åŒ–")
            elif "audio" in selected_model:
                st.info("ğŸµ éŸ³å£°å¯¾å¿œãƒ¢ãƒ‡ãƒ«")
                st.caption("éŸ³å£°å…¥åŠ›ãƒ»å‡ºåŠ›ã«å¯¾å¿œ")
            elif "gpt-4o" in selected_model:
                st.info("ğŸ‘ï¸ ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ãƒ¢ãƒ‡ãƒ«")
                st.caption("ãƒ†ã‚­ã‚¹ãƒˆãƒ»ç”»åƒã®ç†è§£ãŒå¯èƒ½")
            else:
                st.info("ğŸ’¬ æ¨™æº–å¯¾è©±ãƒ¢ãƒ‡ãƒ«")
                st.caption("ä¸€èˆ¬çš„ãªå¯¾è©±ãƒ»ãƒ†ã‚­ã‚¹ãƒˆå‡¦ç†")

            # RAGç”¨é€”ã§ã®æ¨å¥¨åº¦
            st.write("**RAGç”¨é€”æ¨å¥¨åº¦**")
            if selected_model in ["gpt-4o-mini", "gpt-4.1-mini"]:
                st.success("âœ… æœ€é©ï¼ˆã‚³ã‚¹ãƒˆåŠ¹ç‡è‰¯å¥½ï¼‰")
            elif selected_model in ["gpt-4o", "gpt-4.1"]:
                st.info("ğŸ’¡ é«˜å“è³ªï¼ˆã‚³ã‚¹ãƒˆé«˜ï¼‰")
            elif selected_model.startswith("o"):
                st.warning("âš ï¸ æ¨è«–ç‰¹åŒ–ï¼ˆRAGç”¨é€”ã«ã¯éå‰°ï¼‰")
            else:
                st.info("ğŸ’¬ æ¨™æº–çš„ãªæ€§èƒ½")

    except Exception as e:
        logger.error(f"ãƒ¢ãƒ‡ãƒ«æƒ…å ±è¡¨ç¤ºã‚¨ãƒ©ãƒ¼: {e}")
        st.sidebar.error("ãƒ¢ãƒ‡ãƒ«æƒ…å ±ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ")


def estimate_token_usage(df_processed: pd.DataFrame, selected_model: str) -> None:
    """å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã®ãƒˆãƒ¼ã‚¯ãƒ³ä½¿ç”¨é‡æ¨å®š"""
    try:
        if 'Combined_Text' in df_processed.columns:
            # ã‚µãƒ³ãƒ—ãƒ«ãƒ†ã‚­ã‚¹ãƒˆã§ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’æ¨å®š
            sample_size = min(10, len(df_processed))
            sample_texts = df_processed['Combined_Text'].head(sample_size).tolist()
            total_chars = df_processed['Combined_Text'].str.len().sum()

            if sample_texts:
                sample_text = " ".join(sample_texts)
                sample_tokens = TokenManager.count_tokens(sample_text, selected_model)
                sample_chars = len(sample_text)

                if sample_chars > 0:
                    # å…¨ä½“ã®ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’æ¨å®š
                    estimated_total_tokens = int((total_chars / sample_chars) * sample_tokens)

                    with st.expander("ğŸ”¢ ãƒˆãƒ¼ã‚¯ãƒ³ä½¿ç”¨é‡æ¨å®š", expanded=False):
                        col1, col2, col3 = st.columns(3)
                        with col1:
                            st.metric("æ¨å®šç·ãƒˆãƒ¼ã‚¯ãƒ³æ•°", f"{estimated_total_tokens:,}")
                        with col2:
                            avg_tokens_per_record = estimated_total_tokens / len(df_processed)
                            st.metric("å¹³å‡ãƒˆãƒ¼ã‚¯ãƒ³/ãƒ¬ã‚³ãƒ¼ãƒ‰", f"{avg_tokens_per_record:.0f}")
                        with col3:
                            # embeddingç”¨ã®ã‚³ã‚¹ãƒˆæ¨å®šï¼ˆå‚è€ƒå€¤ï¼‰
                            embedding_cost = (estimated_total_tokens / 1000) * 0.0001
                            st.metric("æ¨å®šembeddingè²»ç”¨", f"${embedding_cost:.4f}")

                        st.info(f"ğŸ’¡ é¸æŠãƒ¢ãƒ‡ãƒ«ã€Œ{selected_model}ã€ã§ã®æ¨å®šå€¤")
                        st.caption("â€» å®Ÿéš›ã®ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã¨ã¯ç•°ãªã‚‹å ´åˆãŒã‚ã‚Šã¾ã™")

    except Exception as e:
        logger.error(f"ãƒˆãƒ¼ã‚¯ãƒ³ä½¿ç”¨é‡æ¨å®šã‚¨ãƒ©ãƒ¼: {e}")
        st.error("ãƒˆãƒ¼ã‚¯ãƒ³ä½¿ç”¨é‡ã®æ¨å®šã«å¤±æ•—ã—ã¾ã—ãŸ")


# ==================================================
# ãƒ‡ãƒ¼ã‚¿å‡¦ç†é–¢æ•°ç¾¤ï¼ˆå…±é€šï¼‰
# ==================================================
def clean_text(text: str) -> str:
    """ãƒ†ã‚­ã‚¹ãƒˆã®ã‚¯ãƒ¬ãƒ³ã‚¸ãƒ³ã‚°å‡¦ç†"""
    if pd.isna(text) or text == "":
        return ""

    # æ–‡å­—åˆ—ã«å¤‰æ›
    text = str(text)

    # æ”¹è¡Œã‚’ç©ºç™½ã«ç½®æ›
    text = text.replace('\n', ' ').replace('\r', ' ')

    # é€£ç¶šã—ãŸç©ºç™½ã‚’1ã¤ã®ç©ºç™½ã«ã¾ã¨ã‚ã‚‹
    text = re.sub(r'\s+', ' ', text)

    # å…ˆé ­ãƒ»æœ«å°¾ã®ç©ºç™½ã‚’é™¤å»
    text = text.strip()

    # å¼•ç”¨ç¬¦ã®æ­£è¦åŒ–
    text = text.replace('"', '"').replace('"', '"')
    text = text.replace(''', "'").replace(''', "'")

    return text


def combine_columns(row: pd.Series, dataset_type: str) -> str:
    """è¤‡æ•°åˆ—ã‚’çµåˆã—ã¦1ã¤ã®ãƒ†ã‚­ã‚¹ãƒˆã«ã™ã‚‹ï¼ˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå¯¾å¿œï¼‰"""
    config_data = RAGConfig.get_config(dataset_type)
    required_columns = config_data["required_columns"]

    # å„åˆ—ã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡ºãƒ»ã‚¯ãƒ¬ãƒ³ã‚¸ãƒ³ã‚°
    cleaned_values = []
    for col in required_columns:
        if col in row.index:
            value = row.get(col, '')
            cleaned_text = clean_text(str(value))
            if cleaned_text:  # ç©ºã§ãªã„å ´åˆã®ã¿è¿½åŠ 
                cleaned_values.append(cleaned_text)

    # åŒ»ç™‚QAã®ç‰¹åˆ¥å‡¦ç†ï¼ˆQuestion, Complex_CoT, Responseï¼‰
    if dataset_type == "medical_qa":
        # å¤§æ–‡å­—å°æ–‡å­—ã‚’è€ƒæ…®ã—ãŸåˆ—åãƒãƒƒãƒ”ãƒ³ã‚°
        medical_cols = {}
        for col in row.index:
            col_lower = col.lower()
            if 'question' in col_lower:
                medical_cols['question'] = clean_text(str(row.get(col, '')))
            elif 'complex_cot' in col_lower or 'cot' in col_lower:
                medical_cols['complex_cot'] = clean_text(str(row.get(col, '')))
            elif 'response' in col_lower:
                medical_cols['response'] = clean_text(str(row.get(col, '')))

        # åŒ»ç™‚QAç”¨ã®çµåˆ
        medical_values = [v for v in medical_cols.values() if v]
        if medical_values:
            return " ".join(medical_values).strip()

    # çµåˆ
    combined = " ".join(cleaned_values)
    return combined.strip()


def validate_data(df: pd.DataFrame, dataset_type: str = None) -> List[str]:
    """ãƒ‡ãƒ¼ã‚¿ã®æ¤œè¨¼"""
    issues = []

    # åŸºæœ¬çµ±è¨ˆ
    issues.append(f"ç·è¡Œæ•°: {len(df):,}")
    issues.append(f"ç·åˆ—æ•°: {len(df.columns)}")

    # å¿…é ˆåˆ—ã®ç¢ºèª
    if dataset_type:
        config_data = RAGConfig.get_config(dataset_type)
        required_columns = config_data["required_columns"]

        # å¤§æ–‡å­—å°æ–‡å­—ã‚’è€ƒæ…®ã—ãŸåˆ—åãƒã‚§ãƒƒã‚¯
        available_columns = [col.lower() for col in df.columns]
        missing_columns = []
        found_columns = []

        for req_col in required_columns:
            req_col_lower = req_col.lower()
            # éƒ¨åˆ†ä¸€è‡´ã‚‚è¨±å¯ï¼ˆä¾‹ï¼šQuestion -> question, Complex_CoT -> complex_cotï¼‰
            found = False
            for available_col in df.columns:
                if req_col_lower in available_col.lower() or available_col.lower() in req_col_lower:
                    found_columns.append(available_col)
                    found = True
                    break
            if not found:
                missing_columns.append(req_col)

        if missing_columns:
            issues.append(f"âš ï¸ å¿…é ˆåˆ—ãŒä¸è¶³: {missing_columns}")
        else:
            issues.append(f"âœ… å¿…é ˆåˆ—ç¢ºèªæ¸ˆã¿: {found_columns}")

    # å„åˆ—ã®ç©ºå€¤ç¢ºèª
    for col in df.columns:
        empty_count = df[col].isna().sum() + (df[col] == '').sum()
        if empty_count > 0:
            percentage = (empty_count / len(df)) * 100
            issues.append(f"{col}åˆ—: ç©ºå€¤ {empty_count:,}å€‹ ({percentage:.1f}%)")

    # é‡è¤‡è¡Œã®ç¢ºèª
    duplicate_count = df.duplicated().sum()
    if duplicate_count > 0:
        issues.append(f"âš ï¸ é‡è¤‡è¡Œ: {duplicate_count:,}å€‹")
    else:
        issues.append("âœ… é‡è¤‡è¡Œãªã—")

    return issues


@safe_execute
def load_dataset(uploaded_file, dataset_type: str = None) -> Tuple[pd.DataFrame, List[str]]:
    """ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®èª­ã¿è¾¼ã¿ã¨åŸºæœ¬æ¤œè¨¼"""
    # CSVãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿
    df = pd.read_csv(uploaded_file)

    # åŸºæœ¬æ¤œè¨¼
    validation_results = validate_data(df, dataset_type)

    logger.info(f"ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆèª­ã¿è¾¼ã¿å®Œäº†: {len(df):,}è¡Œ, {len(df.columns)}åˆ—")
    return df, validation_results


@safe_execute
def process_rag_data(df: pd.DataFrame, dataset_type: str, combine_columns_option: bool = True) -> pd.DataFrame:
    """RAGãƒ‡ãƒ¼ã‚¿ã®å‰å‡¦ç†ã‚’å®Ÿè¡Œ"""
    # åŸºæœ¬çš„ãªå‰å‡¦ç†
    df_processed = df.copy()

    # é‡è¤‡è¡Œã®é™¤å»
    initial_rows = len(df_processed)
    df_processed = df_processed.drop_duplicates()
    duplicates_removed = initial_rows - len(df_processed)

    # ç©ºè¡Œã®é™¤å»ï¼ˆå…¨åˆ—ãŒNAã®è¡Œï¼‰
    df_processed = df_processed.dropna(how='all')
    empty_rows_removed = initial_rows - duplicates_removed - len(df_processed)

    # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®ãƒªã‚»ãƒƒãƒˆ
    df_processed = df_processed.reset_index(drop=True)

    logger.info(f"å‰å‡¦ç†å®Œäº†: é‡è¤‡é™¤å»={duplicates_removed:,}è¡Œ, ç©ºè¡Œé™¤å»={empty_rows_removed:,}è¡Œ")

    # å„åˆ—ã®ã‚¯ãƒ¬ãƒ³ã‚¸ãƒ³ã‚°ï¼ˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå¯¾å¿œï¼‰
    config_data = RAGConfig.get_config(dataset_type)
    required_columns = config_data["required_columns"]

    # å¤§æ–‡å­—å°æ–‡å­—ã‚’è€ƒæ…®ã—ãŸåˆ—åå‡¦ç†
    for col in df_processed.columns:
        for req_col in required_columns:
            if req_col.lower() in col.lower() or col.lower() in req_col.lower():
                df_processed[col] = df_processed[col].apply(clean_text)

    # åˆ—ã®çµåˆï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
    if combine_columns_option:
        df_processed['Combined_Text'] = df_processed.apply(
            lambda row: combine_columns(row, dataset_type),
            axis=1
        )

        # ç©ºã®çµåˆãƒ†ã‚­ã‚¹ãƒˆã‚’é™¤å»
        before_filter = len(df_processed)
        df_processed = df_processed[df_processed['Combined_Text'].str.strip() != '']
        after_filter = len(df_processed)
        empty_combined_removed = before_filter - after_filter

        if empty_combined_removed > 0:
            logger.info(f"ç©ºã®çµåˆãƒ†ã‚­ã‚¹ãƒˆã‚’é™¤å»: {empty_combined_removed:,}è¡Œ")

    return df_processed


@safe_execute
def create_download_data(df: pd.DataFrame, include_combined: bool = True, dataset_type: str = None) -> Tuple[
    str, Optional[str]]:
    """ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ç”¨ãƒ‡ãƒ¼ã‚¿ã®ä½œæˆ"""
    # CSVãƒ‡ãƒ¼ã‚¿ã®ä½œæˆ
    csv_buffer = io.StringIO()
    df.to_csv(csv_buffer, index=False, encoding='utf-8')
    csv_data = csv_buffer.getvalue()

    # çµåˆãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®ä½œæˆ
    text_data = None
    if include_combined and 'Combined_Text' in df.columns:
        # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãªã—ã§çµåˆãƒ†ã‚­ã‚¹ãƒˆã®ã¿ã‚’å‡ºåŠ›
        text_lines = []
        for text in df['Combined_Text']:
            if text and str(text).strip():
                text_lines.append(str(text).strip())
        text_data = '\n'.join(text_lines)

    return csv_data, text_data


def display_statistics(df_original: pd.DataFrame, df_processed: pd.DataFrame, dataset_type: str = None) -> None:
    """å‡¦ç†å‰å¾Œã®çµ±è¨ˆæƒ…å ±ã‚’è¡¨ç¤º"""
    st.subheader("ğŸ“Š çµ±è¨ˆæƒ…å ±")

    col1, col2, col3 = st.columns(3)

    with col1:
        st.metric("å…ƒã®è¡Œæ•°", f"{len(df_original):,}")
    with col2:
        st.metric("å‡¦ç†å¾Œã®è¡Œæ•°", f"{len(df_processed):,}")
    with col3:
        removed_rows = len(df_original) - len(df_processed)
        st.metric("é™¤å»ã•ã‚ŒãŸè¡Œæ•°", f"{removed_rows:,}")

    # çµåˆãƒ†ã‚­ã‚¹ãƒˆã®åˆ†æ
    if 'Combined_Text' in df_processed.columns:
        st.subheader("ğŸ“ çµåˆå¾Œãƒ†ã‚­ã‚¹ãƒˆåˆ†æ")
        text_lengths = df_processed['Combined_Text'].str.len()

        col1, col2, col3 = st.columns(3)
        with col1:
            st.metric("å¹³å‡æ–‡å­—æ•°", f"{text_lengths.mean():.0f}")
        with col2:
            st.metric("æœ€å¤§æ–‡å­—æ•°", f"{text_lengths.max():,}")
        with col3:
            st.metric("æœ€å°æ–‡å­—æ•°", f"{text_lengths.min():,}")

        # ãƒ‘ãƒ¼ã‚»ãƒ³ã‚¿ã‚¤ãƒ«è¡¨ç¤º
        percentiles = text_lengths.quantile([0.25, 0.5, 0.75])
        st.write("**æ–‡å­—æ•°åˆ†å¸ƒ:**")
        col1, col2, col3 = st.columns(3)
        with col1:
            st.write(f"25%ç‚¹: {percentiles[0.25]:.0f}æ–‡å­—")
        with col2:
            st.write(f"ä¸­å¤®å€¤: {percentiles[0.5]:.0f}æ–‡å­—")
        with col3:
            st.write(f"75%ç‚¹: {percentiles[0.75]:.0f}æ–‡å­—")


# ==================================================
# ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜é–¢æ•°ç¾¤ï¼ˆå…±é€šï¼‰
# ==================================================
def create_output_directory() -> Path:
    """OUTPUTãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ä½œæˆ"""
    try:
        output_dir = Path("OUTPUT")
        output_dir.mkdir(exist_ok=True)

        # æ›¸ãè¾¼ã¿æ¨©é™ã®ãƒ†ã‚¹ãƒˆ
        test_file = output_dir / ".test_write"
        try:
            test_file.write_text("test", encoding='utf-8')
            if test_file.exists():
                test_file.unlink()
                logger.info("æ›¸ãè¾¼ã¿æ¨©é™ãƒ†ã‚¹ãƒˆ: æˆåŠŸ")
        except Exception as e:
            raise PermissionError(f"æ›¸ãè¾¼ã¿æ¨©é™ãƒ†ã‚¹ãƒˆã«å¤±æ•—: {e}")

        logger.info(f"OUTPUTãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæº–å‚™å®Œäº†: {output_dir.absolute()}")
        return output_dir

    except Exception as e:
        logger.error(f"ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆã‚¨ãƒ©ãƒ¼: {e}")
        raise


@safe_execute
def save_files_to_output(df_processed, dataset_type: str, csv_data: str, text_data: str = None) -> Dict[str, str]:
    """å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã‚’OUTPUTãƒ•ã‚©ãƒ«ãƒ€ã«ä¿å­˜"""
    output_dir = create_output_directory()
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    saved_files = {}

    # CSVãƒ•ã‚¡ã‚¤ãƒ«ã®ä¿å­˜
    csv_filename = f"preprocessed_{dataset_type}.csv"
    csv_path = output_dir / csv_filename

    with open(csv_path, 'w', encoding='utf-8') as f:
        f.write(csv_data)

    if csv_path.exists():
        saved_files['csv'] = str(csv_path)
        logger.info(f"CSVãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜å®Œäº†: {csv_path}")

    # ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã®ä¿å­˜
    if text_data and len(text_data.strip()) > 0:
        txt_filename = f"{dataset_type}.txt"
        txt_path = output_dir / txt_filename

        with open(txt_path, 'w', encoding='utf-8') as f:
            f.write(text_data)

        if txt_path.exists():
            saved_files['txt'] = str(txt_path)
            logger.info(f"ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜å®Œäº†: {txt_path}")

    # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ã®ä¿å­˜
    metadata = {
        "dataset_type"        : dataset_type,
        "processed_rows"      : len(df_processed),
        "processing_timestamp": timestamp,
        "created_at"          : datetime.now().isoformat(),
        "files_created"       : list(saved_files.keys()),
        "processing_info"     : {
            "original_rows": st.session_state.get('original_rows', 0),
            "removed_rows" : st.session_state.get('original_rows', 0) - len(df_processed)
        }
    }

    metadata_filename = f"metadata_{dataset_type}.json"
    metadata_path = output_dir / metadata_filename

    with open(metadata_path, 'w', encoding='utf-8') as f:
        json.dump(metadata, f, indent=2, ensure_ascii=False)

    if metadata_path.exists():
        saved_files['metadata'] = str(metadata_path)
        logger.info(f"ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜å®Œäº†: {metadata_path}")

    return saved_files


# ==================================================
# ä½¿ç”¨æ–¹æ³•èª¬æ˜é–¢æ•°ï¼ˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå¯¾å¿œï¼‰
# ==================================================
def show_usage_instructions(dataset_type: str) -> None:
    """ä½¿ç”¨æ–¹æ³•ã®èª¬æ˜ã‚’è¡¨ç¤ºï¼ˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆåˆ¥å¯¾å¿œï¼‰"""
    config_data = RAGConfig.get_config(dataset_type)
    required_columns_str = ", ".join(config_data["required_columns"])

    st.markdown("---")
    st.subheader("ğŸ“– ä½¿ç”¨æ–¹æ³•")

    # åŸºæœ¬çš„ãªä½¿ç”¨æ–¹æ³•
    basic_usage = f"""
    ### ğŸ“‹ å‰å‡¦ç†æ‰‹é †
    1. **ãƒ¢ãƒ‡ãƒ«é¸æŠ**: ã‚µã‚¤ãƒ‰ãƒãƒ¼ã§RAGç”¨é€”ã«é©ã—ãŸãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠ
    2. **CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰**: {required_columns_str} åˆ—ã‚’å«ã‚€CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’é¸æŠ
    3. **å‰å‡¦ç†å®Ÿè¡Œ**: ä»¥ä¸‹ã®å‡¦ç†ãŒè‡ªå‹•ã§å®Ÿè¡Œã•ã‚Œã¾ã™ï¼š
       - æ”¹è¡Œãƒ»ç©ºç™½ã®æ­£è¦åŒ–
       - é‡è¤‡è¡Œã®é™¤å»
       - ç©ºè¡Œã®é™¤å»
       - å¼•ç”¨ç¬¦ã®æ­£è¦åŒ–
    4. **åˆ—çµåˆ**: Vector Store/RAGç”¨ã«æœ€é©åŒ–ã•ã‚ŒãŸè‡ªç„¶ãªæ–‡ç« ã¨ã—ã¦çµåˆ
    5. **ãƒˆãƒ¼ã‚¯ãƒ³ä½¿ç”¨é‡ç¢ºèª**: é¸æŠãƒ¢ãƒ‡ãƒ«ã§ã®ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã¨ã‚³ã‚¹ãƒˆã‚’æ¨å®š
    6. **ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰**: å‰å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã‚’å„ç¨®å½¢å¼ã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰

    ### ğŸ¯ RAGæœ€é©åŒ–ã®ç‰¹å¾´
    - **è‡ªç„¶ãªæ–‡ç« çµåˆ**: ãƒ©ãƒ™ãƒ«ãªã—ã§èª­ã¿ã‚„ã™ã„æ–‡ç« ã¨ã—ã¦çµåˆ
    - **OpenAI embeddingå¯¾å¿œ**: text-embedding-ada-002ç­‰ã«æœ€é©åŒ–
    - **æ¤œç´¢æ€§èƒ½å‘ä¸Š**: æ„å‘³çš„æ¤œç´¢ã®ç²¾åº¦å‘ä¸Š

    ### ğŸ’¡ æ¨å¥¨ãƒ¢ãƒ‡ãƒ«
    - **ã‚³ã‚¹ãƒˆé‡è¦–**: gpt-4o-mini, gpt-4.1-mini
    - **å“è³ªé‡è¦–**: gpt-4o, gpt-4.1
    - **æ¨è«–ã‚¿ã‚¹ã‚¯**: o1-mini, o3-miniï¼ˆRAGç”¨é€”ã«ã¯éå‰°ï¼‰
    """

    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆç‰¹æœ‰ã®èª¬æ˜
    dataset_specific = ""
    if dataset_type == "customer_support_faq":
        dataset_specific = """
    ### ğŸ’¬ ã‚«ã‚¹ã‚¿ãƒãƒ¼ã‚µãƒãƒ¼ãƒˆFAQã®ç‰¹å¾´
    - **FAQå½¢å¼**: è³ªå•ã¨å›ç­”ã®ãƒšã‚¢ã«ã‚ˆã‚‹æ§‹é€ 
    - **å®Ÿç”¨çš„ãªå†…å®¹**: å®Ÿéš›ã®é¡§å®¢ã‹ã‚‰ã®è³ªå•ã«åŸºã¥ã
    - **ç°¡æ½”ãªå›ç­”**: åˆ†ã‹ã‚Šã‚„ã™ãå®Ÿç”¨çš„ãªå›ç­”
        """
    elif dataset_type == "medical_qa":
        dataset_specific = """
    ### ğŸ¥ åŒ»ç™‚QAãƒ‡ãƒ¼ã‚¿ã®ç‰¹å¾´
    - **è¤‡é›‘ãªæ¨è«–**: Complex_CoTåˆ—ã«ã‚ˆã‚‹æ®µéšçš„æ¨è«–éç¨‹
    - **å°‚é–€ç”¨èª**: åŒ»ç™‚å°‚é–€ç”¨èªã®é©åˆ‡ãªå‡¦ç†
    - **è©³ç´°ãªå›ç­”**: åŒ»ç™‚æƒ…å ±ã«ç‰¹åŒ–ã—ãŸåŒ…æ‹¬çš„ãªå›ç­”
        """
    elif dataset_type == "sciq_qa":
        dataset_specific = """
    ### ğŸ”¬ SciQãƒ‡ãƒ¼ã‚¿ã®ç‰¹å¾´
    - **ç§‘å­¦ãƒ»æŠ€è¡“å•é¡Œ**: åŒ–å­¦ã€ç‰©ç†ã€ç”Ÿç‰©å­¦ã€æ•°å­¦ãªã©ã®åˆ†é‡ã‚’ã‚«ãƒãƒ¼
    - **å¤šè‚¢é¸æŠå½¢å¼**: distractoråˆ—ã«ã‚ˆã‚‹é¸æŠè‚¢å•é¡Œ
    - **è£œè¶³èª¬æ˜**: supportåˆ—ã«ã‚ˆã‚‹è©³ç´°ãªèƒŒæ™¯æƒ…å ±
    - **å¹…åºƒã„é›£æ˜“åº¦**: åŸºç¤ã‹ã‚‰å¿œç”¨ã¾ã§æ§˜ã€…ãªãƒ¬ãƒ™ãƒ«ã®ç§‘å­¦å•é¡Œ
        """
    elif dataset_type == "legal_qa":
        dataset_specific = """
    ### âš–ï¸ æ³•å¾‹ãƒ»åˆ¤ä¾‹QAãƒ‡ãƒ¼ã‚¿ã®ç‰¹å¾´
    - **æ³•å¾‹å°‚é–€ç”¨èª**: æ¡æ–‡ã€åˆ¤ä¾‹ã€æ³•çš„æ¦‚å¿µã®é©åˆ‡ãªå‡¦ç†
    - **è©³ç´°ãªå›ç­”**: æ³•çš„æ ¹æ‹ ã‚’å«ã‚€åŒ…æ‹¬çš„ãªèª¬æ˜
    - **æ­£ç¢ºæ€§é‡è¦–**: æ³•çš„æƒ…å ±ã®æ­£ç¢ºæ€§ã‚’ä¿æŒã—ãŸå‰å‡¦ç†
    - **å¼•ç”¨ãƒ»å‚ç…§**: æ¡æ–‡ç•ªå·ã‚„åˆ¤ä¾‹ç•ªå·ãªã©ã®æ³•çš„æ ¹æ‹ ã®ä¿è­·
        """

    st.markdown(basic_usage + dataset_specific)


# ==================================================
# ãƒšãƒ¼ã‚¸è¨­å®šé–¢æ•°ï¼ˆå…±é€šï¼‰
# ==================================================
def setup_page_config(dataset_type: str) -> None:
    """ãƒšãƒ¼ã‚¸è¨­å®šã®åˆæœŸåŒ–"""
    config_data = RAGConfig.get_config(dataset_type)

    try:
        st.set_page_config(
            page_title=f"{config_data['name']}å‰å‡¦ç†ï¼ˆå®Œå…¨ç‹¬ç«‹ç‰ˆï¼‰",
            page_icon=config_data['icon'],
            layout="wide",
            initial_sidebar_state="expanded"
        )
    except st.errors.StreamlitAPIException:
        pass


def setup_page_header(dataset_type: str) -> None:
    """ãƒšãƒ¼ã‚¸ãƒ˜ãƒƒãƒ€ãƒ¼ã®è¨­å®š"""
    config_data = RAGConfig.get_config(dataset_type)

    st.title(f"{config_data['icon']} {config_data['name']}å‰å‡¦ç†ã‚¢ãƒ—ãƒª")
    st.caption("RAGï¼ˆRetrieval-Augmented Generationï¼‰ç”¨ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç† - å®Œå…¨ç‹¬ç«‹ç‰ˆ")
    st.markdown("---")


def setup_sidebar_header(dataset_type: str) -> None:
    """ã‚µã‚¤ãƒ‰ãƒãƒ¼ãƒ˜ãƒƒãƒ€ãƒ¼ã®è¨­å®š"""
    config_data = RAGConfig.get_config(dataset_type)

    st.sidebar.title(f"{config_data['icon']} {config_data['name']}")
    st.sidebar.markdown("---")


# ==================================================
# ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆï¼ˆå…±é€šé–¢æ•°ä¸€è¦§ï¼‰
# ==================================================
__all__ = [
    # è¨­å®šã‚¯ãƒ©ã‚¹
    'AppConfig',
    'RAGConfig',
    'TokenManager',

    # ãƒ‡ã‚³ãƒ¬ãƒ¼ã‚¿
    'safe_execute',

    # UIé–¢æ•°
    'select_model',
    'show_model_info',
    'estimate_token_usage',

    # ãƒ‡ãƒ¼ã‚¿å‡¦ç†é–¢æ•°
    'clean_text',
    'combine_columns',
    'validate_data',
    'load_dataset',
    'process_rag_data',
    'create_download_data',
    'display_statistics',

    # ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜é–¢æ•°
    'create_output_directory',
    'save_files_to_output',

    # ä½¿ç”¨æ–¹æ³•ãƒ»ãƒšãƒ¼ã‚¸è¨­å®šé–¢æ•°
    'show_usage_instructions',
    'setup_page_config',
    'setup_page_header',
    'setup_sidebar_header',
]
