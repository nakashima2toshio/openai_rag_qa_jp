#!/usr/bin/env python3
"""
ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯ã‚«ãƒãƒ¬ãƒƒã‚¸åˆ†æã¨Q/Aç”Ÿæˆã‚·ã‚¹ãƒ†ãƒ ï¼ˆæ”¹è‰¯ç‰ˆï¼‰
=====================================================
2025-11-08 01:03:57,956 - INFO -     ãƒãƒƒãƒ 3/3 å®Œäº†: 182å€‹
2025-11-08 01:03:57,956 - INFO - Q/AåŸ‹ã‚è¾¼ã¿ç”Ÿæˆå®Œäº†: åˆè¨ˆ4278å€‹

ğŸ“Š ã‚«ãƒãƒ¬ãƒƒã‚¸åˆ†æçµæœ:
  ã‚«ãƒãƒ¬ãƒƒã‚¸ç‡: 90.3%
  ã‚«ãƒãƒ¼æ¸ˆã¿ãƒãƒ£ãƒ³ã‚¯: 1526/1689
  é–¾å€¤: 0.6
  å¹³å‡æœ€å¤§é¡ä¼¼åº¦: 0.745

  ã‚«ãƒãƒ¬ãƒƒã‚¸åˆ†å¸ƒ:
    é«˜ã‚«ãƒãƒ¬ãƒƒã‚¸ (â‰¥0.7): 1173ãƒãƒ£ãƒ³ã‚¯
    ä¸­ã‚«ãƒãƒ¬ãƒƒã‚¸ (0.5-0.7): 484ãƒãƒ£ãƒ³ã‚¯
    ä½ã‚«ãƒãƒ¬ãƒƒã‚¸ (<0.5): 32ãƒãƒ£ãƒ³ã‚¯
2025-11-08 01:06:58,836 - INFO - çµæœã‚’ä¿å­˜ã—ã¾ã—ãŸ: qa_output/a03

================================================================================
å‡¦ç†å®Œäº†
================================================================================

âœ… ç”Ÿæˆã•ã‚ŒãŸQ/Aãƒšã‚¢æ•°: 4278
âœ… ä¿å­˜ãƒ•ã‚¡ã‚¤ãƒ«:
  - qa_json: qa_output/a03/qa_pairs_cc_news_20251108_010658.json
  - qa_csv: qa_output/a03/qa_pairs_cc_news_20251108_010658.csv
  - coverage: qa_output/a03/coverage_cc_news_20251108_010658.json
  - summary: qa_output/a03/summary_cc_news_20251108_010658.json

ğŸ“Š Q/Aãƒšã‚¢çµ±è¨ˆ:
  - comprehensive: 1ä»¶
  - contextual: 2753ä»¶
  - factual_detailed: 2ä»¶
  - keyword_based: 1520ä»¶
  - thematic: 2ä»¶


[å®Ÿè¡Œã‚³ãƒãƒ³ãƒ‰]
python a03_rag_qa_coverage_improved.py \
--input OUTPUT/preprocessed_cc_news.csv \
--dataset cc_news \
--analyze-coverage \
--coverage-threshold 0.60 \
--qa-per-chunk 10 \
--max-chunks 2000 \
--output qa_output


| è¨­å®š  | æ–‡æ›¸æ•°   | ãƒãƒ£ãƒ³ã‚¯æ•°  | Q/Aæ•°    | å®Ÿè¡Œæ™‚é–“   | ã‚«ãƒãƒ¬ãƒ¼ã‚¸äºˆæƒ³      | ã‚³ã‚¹ãƒˆ    |
|-----|-------|--------|---------|--------|--------------|--------|
| ç¾çŠ¶  | 150   | 609    | 7,308   | 2åˆ†     | 99.7% (0.52) | $0.001 |
| æ¨å¥¨  | è‡ªå‹•    | 2,000  | 20,000  | 8-10åˆ†  | 95%+ (0.60)  | $0.005 |
| ä¸­è¦æ¨¡ | 1,000 | 2,400  | 24,000  | 10-12åˆ† | 95%+ (0.60)  | $0.006 |
| å…¨æ–‡æ›¸ | 7,499 | 18,000 | 144,000 | 60-90åˆ† | 95%+ (0.60)  | $0.025 |

"""

from helper_rag_qa import (
    SemanticCoverage,
    TemplateBasedQAGenerator,
)
import os
import sys
import json
import argparse
import pandas as pd
import numpy as np
from pathlib import Path
from typing import List, Dict, Optional, Tuple
from datetime import datetime
import logging
import re
from collections import Counter

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


# ============================================================================
# MeCabå¯¾å¿œã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡ºã‚¯ãƒ©ã‚¹ï¼ˆregex_mecab.pyã‹ã‚‰ç§»æ¤ï¼‰
# ============================================================================

class KeywordExtractor:
    """
    MeCabã¨æ­£è¦è¡¨ç¾ã‚’çµ±åˆã—ãŸã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡ºã‚¯ãƒ©ã‚¹

    MeCabãŒåˆ©ç”¨å¯èƒ½ãªå ´åˆã¯è¤‡åˆåè©æŠ½å‡ºã‚’å„ªå…ˆã—ã€
    åˆ©ç”¨ä¸å¯ã®å ´åˆã¯æ­£è¦è¡¨ç¾ç‰ˆã«è‡ªå‹•ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
    """

    def __init__(self, prefer_mecab: bool = True):
        """
        Args:
            prefer_mecab: MeCabã‚’å„ªå…ˆçš„ã«ä½¿ç”¨ã™ã‚‹ã‹ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: Trueï¼‰
        """
        self.prefer_mecab = prefer_mecab
        self.mecab_available = self._check_mecab_availability()

        # ã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰å®šç¾©
        self.stopwords = {
            'ã“ã¨', 'ã‚‚ã®', 'ã“ã‚Œ', 'ãã‚Œ', 'ãŸã‚', 'ã‚ˆã†', 'ã•ã‚“',
            'ã¾ã™', 'ã§ã™', 'ã‚ã‚‹', 'ã„ã‚‹', 'ã™ã‚‹', 'ãªã‚‹', 'ã§ãã‚‹',
            'ã„ã†', 'çš„', 'ãª', 'ã«', 'ã‚’', 'ã¯', 'ãŒ', 'ã§', 'ã¨',
            'ã®', 'ã‹ã‚‰', 'ã¾ã§', 'ç­‰', 'ãªã©', 'ã‚ˆã‚‹', 'ãŠã', 'ãã‚‹'
        }

        if self.mecab_available:
            logger.info("âœ… MeCabãŒåˆ©ç”¨å¯èƒ½ã§ã™ï¼ˆè¤‡åˆåè©æŠ½å‡ºãƒ¢ãƒ¼ãƒ‰ï¼‰")
        else:
            logger.info("âš ï¸ MeCabãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ï¼ˆæ­£è¦è¡¨ç¾ãƒ¢ãƒ¼ãƒ‰ï¼‰")

    def _check_mecab_availability(self) -> bool:
        """MeCabã®åˆ©ç”¨å¯èƒ½æ€§ã‚’ãƒã‚§ãƒƒã‚¯"""
        try:
            import MeCab
            # å®Ÿéš›ã«ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹åŒ–ã—ã¦å‹•ä½œç¢ºèª
            tagger = MeCab.Tagger()
            tagger.parse("ãƒ†ã‚¹ãƒˆ")
            return True
        except (ImportError, RuntimeError) as e:
            return False

    def extract(self, text: str, top_n: int = 5) -> List[str]:
        """
        ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡ºï¼ˆè‡ªå‹•ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å¯¾å¿œï¼‰

        Args:
            text: åˆ†æå¯¾è±¡ãƒ†ã‚­ã‚¹ãƒˆ
            top_n: æŠ½å‡ºã™ã‚‹ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ•°

        Returns:
            ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒªã‚¹ãƒˆ
        """
        if self.mecab_available and self.prefer_mecab:
            try:
                keywords = self._extract_with_mecab(text, top_n)
                if keywords:  # ç©ºã§ãªã‘ã‚Œã°æˆåŠŸ
                    return keywords
            except Exception as e:
                logger.warning(f"âš ï¸ MeCabæŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")

        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: æ­£è¦è¡¨ç¾ç‰ˆ
        return self._extract_with_regex(text, top_n)

    def _extract_with_mecab(self, text: str, top_n: int) -> List[str]:
        """MeCabã‚’ä½¿ç”¨ã—ãŸè¤‡åˆåè©æŠ½å‡º"""
        import MeCab

        tagger = MeCab.Tagger()
        node = tagger.parseToNode(text)

        # è¤‡åˆåè©ã®æŠ½å‡º
        compound_buffer = []
        compound_nouns = []

        while node:
            features = node.feature.split(',')
            pos = features[0]  # å“è©

            if pos == 'åè©':
                compound_buffer.append(node.surface)
            else:
                # åè©ä»¥å¤–ãŒæ¥ãŸã‚‰ãƒãƒƒãƒ•ã‚¡ã‚’ãƒ•ãƒ©ãƒƒã‚·ãƒ¥
                if compound_buffer:
                    compound_noun = ''.join(compound_buffer)
                    if len(compound_noun) > 0:
                        compound_nouns.append(compound_noun)
                    compound_buffer = []

            node = node.next

        # æœ€å¾Œã®ãƒãƒƒãƒ•ã‚¡ã‚’ãƒ•ãƒ©ãƒƒã‚·ãƒ¥
        if compound_buffer:
            compound_noun = ''.join(compound_buffer)
            if len(compound_noun) > 0:
                compound_nouns.append(compound_noun)

        # ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã¨é »åº¦ã‚«ã‚¦ãƒ³ãƒˆ
        return self._filter_and_count(compound_nouns, top_n)

    def _extract_with_regex(self, text: str, top_n: int) -> List[str]:
        """æ­£è¦è¡¨ç¾ã‚’ä½¿ç”¨ã—ãŸã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡º"""
        # ã‚«ã‚¿ã‚«ãƒŠèªã€æ¼¢å­—è¤‡åˆèªã€è‹±æ•°å­—ã‚’æŠ½å‡º
        pattern = r'[ã‚¡-ãƒ´ãƒ¼]{2,}|[ä¸€-é¾¥]{2,}|[A-Za-z]{2,}[A-Za-z0-9]*'
        words = re.findall(pattern, text)

        # ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã¨é »åº¦ã‚«ã‚¦ãƒ³ãƒˆ
        return self._filter_and_count(words, top_n)

    def _filter_and_count(self, words: List[str], top_n: int) -> List[str]:
        """é »åº¦ãƒ™ãƒ¼ã‚¹ã®ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°"""
        # ã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰é™¤å¤–
        filtered = [w for w in words if w not in self.stopwords and len(w) > 1]

        # é »åº¦ã‚«ã‚¦ãƒ³ãƒˆ
        word_freq = Counter(filtered)

        # ä¸Šä½Nä»¶ã‚’è¿”ã™
        return [word for word, freq in word_freq.most_common(top_n)]


# ã‚°ãƒ­ãƒ¼ãƒãƒ«ãªKeywordExtractorã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ï¼ˆä¸€åº¦ã ã‘åˆæœŸåŒ–ï¼‰
_keyword_extractor = None

def get_keyword_extractor() -> KeywordExtractor:
    """KeywordExtractorã®ã‚·ãƒ³ã‚°ãƒ«ãƒˆãƒ³ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’å–å¾—"""
    global _keyword_extractor
    if _keyword_extractor is None:
        _keyword_extractor = KeywordExtractor()
    return _keyword_extractor

# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆè¨­å®š
DATASET_CONFIGS = {
    "cc_news": {
        "name": "CC-Newsè‹±èªãƒ‹ãƒ¥ãƒ¼ã‚¹",
        "text_column": "Combined_Text",
        "title_column": "title",
        "lang": "en"
    },
    "japanese_text": {
        "name": "æ—¥æœ¬èªWebãƒ†ã‚­ã‚¹ãƒˆ",
        "text_column": "Combined_Text",
        "title_column": None,
        "lang": "ja"
    },
    "wikipedia_ja": {
        "name": "Wikipediaæ—¥æœ¬èªç‰ˆ",
        "text_column": "Combined_Text",
        "title_column": "title",
        "lang": "ja"
    }
}


def load_input_data(input_file: str, dataset_type: Optional[str] = None, max_docs: Optional[int] = None) -> str:
    """å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿"""
    file_path = Path(input_file)
    if not file_path.exists():
        raise FileNotFoundError(f"å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {input_file}")

    logger.info(f"ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ä¸­: {input_file}")

    if file_path.suffix.lower() == '.csv':
        df = pd.read_csv(file_path)

        if dataset_type and dataset_type in DATASET_CONFIGS:
            config = DATASET_CONFIGS[dataset_type]
            text_col = config["text_column"]

            if text_col not in df.columns:
                if "text" in df.columns:
                    text_col = "text"
                else:
                    raise ValueError(f"ãƒ†ã‚­ã‚¹ãƒˆã‚«ãƒ©ãƒ  '{text_col}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")

            if max_docs:
                df = df.head(max_docs)

            texts = df[text_col].dropna().tolist()
            combined_text = "\n\n".join([str(t) for t in texts])

            logger.info(f"èª­ã¿è¾¼ã¿å®Œäº†: {len(texts)}ä»¶ã®æ–‡æ›¸")
        else:
            text_cols = [col for col in df.columns if 'text' in col.lower() or 'content' in col.lower()]
            if text_cols:
                text_col = text_cols[0]
            else:
                text_col = df.columns[0]

            if max_docs:
                df = df.head(max_docs)

            texts = df[text_col].dropna().tolist()
            combined_text = "\n\n".join([str(t) for t in texts])

    else:
        with open(file_path, 'r', encoding='utf-8') as f:
            combined_text = f.read()

        if max_docs:
            paragraphs = combined_text.split('\n\n')
            paragraphs = paragraphs[:max_docs]
            combined_text = '\n\n'.join(paragraphs)

    return combined_text


def generate_comprehensive_qa_for_chunk(chunk_text: str, chunk_idx: int, qa_per_chunk: int = 5, lang: str = "auto") -> List[Dict]:
    """
    å˜ä¸€ãƒãƒ£ãƒ³ã‚¯ã«å¯¾ã—ã¦åŒ…æ‹¬çš„ãªQ/Aã‚’ç”Ÿæˆï¼ˆæ”¹è‰¯ç‰ˆï¼‰

    Args:
        chunk_text: ãƒãƒ£ãƒ³ã‚¯ã®ãƒ†ã‚­ã‚¹ãƒˆ
        chunk_idx: ãƒãƒ£ãƒ³ã‚¯ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
        qa_per_chunk: ãƒãƒ£ãƒ³ã‚¯ã‚ãŸã‚Šã®Q/Aæ•°
        lang: è¨€èªã‚³ãƒ¼ãƒ‰ ("en", "ja", "auto")

    Returns:
        ç”Ÿæˆã•ã‚ŒãŸQ/Aãƒšã‚¢ã®ãƒªã‚¹ãƒˆ
    """
    qas = []

    # è‹±èª/æ—¥æœ¬èªã®åˆ¤å®šï¼ˆlangãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒæŒ‡å®šã•ã‚Œã¦ã„ã‚Œã°å„ªå…ˆï¼‰
    if lang == "auto":
        # è‡ªå‹•åˆ¤å®š: è‹±èªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®å‡ºç¾é »åº¦ã§åˆ¤å®š
        english_indicators = ['the ', 'The ', ' is ', ' are ', ' was ', ' were ', ' have ', ' has ', 'and ', 'for ']
        japanese_indicators = ['ã€‚', 'ã¯', 'ãŒ', 'ã‚’', 'ã«', 'ã§', 'ã¨', 'ã®']

        english_count = sum(1 for word in english_indicators if word in chunk_text[:200])
        japanese_count = sum(1 for char in japanese_indicators if char in chunk_text[:200])

        is_english = english_count > japanese_count
    else:
        is_english = (lang == "en")

    # ãƒãƒ£ãƒ³ã‚¯ã‹ã‚‰é‡è¦ãªæƒ…å ±ã‚’æŠ½å‡º
    sentences = chunk_text.split('. ' if is_english else 'ã€‚')
    sentences = [s.strip() for s in sentences if len(s.strip()) > 20]

    if not sentences:
        return []

    # æˆ¦ç•¥1: ãƒãƒ£ãƒ³ã‚¯å…¨ä½“ã«é–¢ã™ã‚‹åŒ…æ‹¬çš„Q/A
    if len(chunk_text) > 50:
        # ãƒãƒ£ãƒ³ã‚¯å…¨ä½“ã®è¦ç´„çš„ãªè³ªå•
        qa = {
            'question': f"What information is discussed in this section?" if is_english else f"ã“ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã«ã¯ã©ã®ã‚ˆã†ãªæƒ…å ±ãŒå«ã¾ã‚Œã¦ã„ã¾ã™ã‹ï¼Ÿ",
            'answer': chunk_text[:500],  # ã‚ˆã‚Šé•·ã„å›ç­”
            'type': 'comprehensive',
            'chunk_idx': chunk_idx,
            'coverage_strategy': 'full_chunk'
        }
        qas.append(qa)

    # æˆ¦ç•¥2: æ–‡ã”ã¨ã®è©³ç´°ãªQ/Aç”Ÿæˆ
    for i, sent in enumerate(sentences[:qa_per_chunk - 1]):
        if len(sent) < 20:
            continue

        if is_english:
            # è‹±èªç”¨ã®è©³ç´°ãªQ/A
            # 1. Whatå‹ã®è³ªå•ï¼ˆäº‹å®Ÿç¢ºèªï¼‰
            # å›ºæœ‰åè©ã‚„ä¸»è¦æ¦‚å¿µã‚’æŠ½å‡º
            main_concepts = re.findall(r'\b[A-Z][a-z]+(?:\s+[A-Z][a-z]+)*\b', sent[:50])
            if main_concepts:
                concept = main_concepts[0]
                qa = {
                    'question': f"What specific information is provided about {concept}?",
                    'answer': sent + (" " + sentences[i + 1] if i + 1 < len(sentences) else ""),
                    'type': 'factual_detailed',
                    'chunk_idx': chunk_idx
                }
            else:
                qa = {
                    'question': f"What information is provided in the following context: {sent[:50]}?",
                    'answer': sent + (" " + sentences[i + 1] if i + 1 < len(sentences) else ""),
                    'type': 'factual_detailed',
                    'chunk_idx': chunk_idx
                }
            qas.append(qa)

            # 2. æ–‡è„ˆã‚’å«ã‚€è³ªå•
            if i > 0:
                # å‰ã®æ–‡ã¨ç¾åœ¨ã®æ–‡ã®ä¸»è¦æ¦‚å¿µã‚’æŠ½å‡º
                prev_concepts = re.findall(r'\b[A-Z][a-z]+(?:\s+[A-Z][a-z]+)*\b', sentences[i-1][:30])
                curr_concepts = re.findall(r'\b[A-Z][a-z]+(?:\s+[A-Z][a-z]+)*\b', sent[:30])

                if prev_concepts and curr_concepts:
                    qa = {
                        'question': f"How does {curr_concepts[0]} relate to {prev_concepts[0]}?",
                        'answer': sentences[i - 1] + " " + sent,
                        'type': 'contextual',
                        'chunk_idx': chunk_idx
                    }
                else:
                    qa = {
                        'question': f"How does the information '{sent[:30]}...' connect to the previous context?",
                        'answer': sentences[i - 1] + " " + sent,
                        'type': 'contextual',
                        'chunk_idx': chunk_idx
                    }
                qas.append(qa)

            # 3. ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡ºå‹
            # å›ºæœ‰åè©ã‚„é‡è¦ãªèªå¥ã‚’æ¢ã™
            important_words = re.findall(r'\b[A-Z][a-z]+(?:\s+[A-Z][a-z]+)*\b', sent)
            if important_words:
                keyword = important_words[0]
                qa = {
                    'question': f"What is mentioned about {keyword}?",
                    'answer': sent,
                    'type': 'keyword_based',
                    'chunk_idx': chunk_idx
                }
                qas.append(qa)

        else:
            # æ—¥æœ¬èªç”¨ã®è©³ç´°ãªQ/A
            qa = {
                'question': f"ã€Œ{sent[:30]}ã€ã«ã¤ã„ã¦è©³ã—ãèª¬æ˜ã—ã¦ãã ã•ã„ã€‚",
                'answer': sent + ("ã€‚" + sentences[i + 1] if i + 1 < len(sentences) else ""),
                'type': 'factual_detailed',
                'chunk_idx': chunk_idx
            }
            qas.append(qa)

            # æ—¥æœ¬èªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡ºå‹Q/Aï¼ˆMeCabä½¿ç”¨ï¼‰
            extractor = get_keyword_extractor()
            keywords = extractor.extract(sent, top_n=2)
            for keyword in keywords:
                if len(keyword) > 1:  # 1æ–‡å­—ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã¯é™¤å¤–
                    qa = {
                        'question': f"ã€Œ{keyword}ã€ã«ã¤ã„ã¦ä½•ãŒè¿°ã¹ã‚‰ã‚Œã¦ã„ã¾ã™ã‹ï¼Ÿ",
                        'answer': sent,
                        'type': 'keyword_based',
                        'chunk_idx': chunk_idx,
                        'keyword': keyword
                    }
                    qas.append(qa)

    # æˆ¦ç•¥3: ãƒãƒ£ãƒ³ã‚¯ã®ä¸»è¦ãƒ†ãƒ¼ãƒã«é–¢ã™ã‚‹Q/A
    if len(chunk_text) > 100:
        # ãƒãƒ£ãƒ³ã‚¯ã®æœ€åˆã¨æœ€å¾Œã®æ–‡ã‚’çµ„ã¿åˆã‚ã›ãŸè³ªå•
        first_sent = sentences[0] if sentences else chunk_text[:100]
        last_sent = sentences[-1] if sentences else chunk_text[-100:]

        if is_english:
            # è‹±èª: ä¸»è¦æ¦‚å¿µã‚’æŠ½å‡ºã—ã¦ãƒ†ãƒ¼ãƒè³ªå•ã‚’ä½œæˆ
            theme_concepts = re.findall(r'\b[A-Z][a-z]+(?:\s+[A-Z][a-z]+)*\b', first_sent[:50])
            if theme_concepts:
                qa = {
                    'question': f"What is the main theme related to {theme_concepts[0]}?",
                    'answer': chunk_text[:400],
                    'type': 'thematic',
                    'chunk_idx': chunk_idx
                }
            else:
                qa = {
                    'question': f"What is the main theme discussed in this content?",
                    'answer': chunk_text[:400],
                    'type': 'thematic',
                    'chunk_idx': chunk_idx
                }
        else:
            # æ—¥æœ¬èª: ä¸»è¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’ä½¿ã£ãŸãƒ†ãƒ¼ãƒè³ªå•
            extractor = get_keyword_extractor()
            theme_keywords = extractor.extract(chunk_text[:200], top_n=1)
            if theme_keywords:
                qa = {
                    'question': f"ã€Œ{theme_keywords[0]}ã€ã«é–¢ã™ã‚‹ä¸»è¦ãƒ†ãƒ¼ãƒã¯ä½•ã§ã™ã‹ï¼Ÿ",
                    'answer': chunk_text[:400],
                    'type': 'thematic',
                    'chunk_idx': chunk_idx
                }
            else:
                qa = {
                    'question': f"ã“ã®å†…å®¹ã®ä¸»è¦ãƒ†ãƒ¼ãƒã¯ä½•ã§ã™ã‹ï¼Ÿ",
                    'answer': chunk_text[:400],
                    'type': 'thematic',
                    'chunk_idx': chunk_idx
                }
        qas.append(qa)

    return qas[:qa_per_chunk]


def calculate_improved_coverage(
    chunks: List[Dict],
    qa_pairs: List[Dict],
    analyzer: SemanticCoverage,
    threshold: float = 0.65
) -> Tuple[Dict, List[float]]:
    """
    æ”¹å–„ã•ã‚ŒãŸã‚«ãƒãƒ¬ãƒƒã‚¸è¨ˆç®—ï¼ˆãƒãƒƒãƒå‡¦ç†ç‰ˆï¼‰

    Args:
        chunks: ãƒãƒ£ãƒ³ã‚¯ãƒªã‚¹ãƒˆ
        qa_pairs: Q/Aãƒšã‚¢ãƒªã‚¹ãƒˆ
        analyzer: SemanticCoverageã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
        threshold: ã‚«ãƒãƒ¬ãƒƒã‚¸åˆ¤å®šé–¾å€¤ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ0.65ã«ä¸‹ã’ã‚‹ï¼‰

    Returns:
        ã‚«ãƒãƒ¬ãƒƒã‚¸çµæœã¨é¡ä¼¼åº¦è¡Œåˆ—
    """
    if not qa_pairs or not chunks:
        return {"coverage_rate": 0, "covered_chunks": 0, "total_chunks": len(chunks)}, []

    logger.info(f"ã‚«ãƒãƒ¬ãƒƒã‚¸è¨ˆç®—é–‹å§‹: {len(chunks)}ãƒãƒ£ãƒ³ã‚¯, {len(qa_pairs)}Q/A")

    # ãƒãƒ£ãƒ³ã‚¯ã®åŸ‹ã‚è¾¼ã¿ã‚’ç”Ÿæˆï¼ˆæ—¢ã«ãƒãƒƒãƒå‡¦ç†ï¼‰
    doc_embeddings = analyzer.generate_embeddings(chunks)

    # Q/Aãƒšã‚¢ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’æº–å‚™ï¼ˆãƒãƒƒãƒå‡¦ç†ç”¨ï¼‰
    qa_texts = []
    for qa in qa_pairs:
        # è³ªå•ã¨å›ç­”ã‚’é‡ã¿ä»˜ã‘ã—ã¦çµåˆï¼ˆå›ç­”ã«ã‚ˆã‚Šé‡ã¿ã‚’ç½®ãï¼‰
        question = qa.get('question', '')
        answer = qa.get('answer', '')

        # ã‚ˆã‚Šé•·ã„æ–‡è„ˆã‚’ä½œã‚‹ï¼ˆè³ªå•1å› + å›ç­”2å›ã§å›ç­”ã‚’å¼·èª¿ï¼‰
        combined_text = f"{question} {answer} {answer}"
        qa_texts.append(combined_text)

    # ãƒãƒƒãƒå‡¦ç†ã§Q/AåŸ‹ã‚è¾¼ã¿ã‚’ç”Ÿæˆ
    logger.info(f"Q/AåŸ‹ã‚è¾¼ã¿ã‚’ãƒãƒƒãƒç”Ÿæˆä¸­... ({len(qa_texts)}å€‹)")

    # OpenAI APIã®ãƒãƒƒãƒã‚µã‚¤ã‚ºåˆ¶é™ã‚’è€ƒæ…®ï¼ˆæœ€å¤§2048ï¼‰
    MAX_BATCH_SIZE = 2048
    qa_embeddings = []

    if len(qa_texts) <= MAX_BATCH_SIZE:
        # ä¸€åº¦ã«ã™ã¹ã¦å‡¦ç†å¯èƒ½
        # generate_embeddingsã¯è¾æ›¸ã®ãƒªã‚¹ãƒˆã‚’æƒ³å®šã—ã¦ã„ã‚‹ãŸã‚ã€å¤‰æ›
        qa_chunks = [{"text": text} for text in qa_texts]
        qa_embeddings = analyzer.generate_embeddings(qa_chunks)
        logger.info(f"  ãƒãƒƒãƒå‡¦ç†å®Œäº†: 1å›ã®APIå‘¼ã³å‡ºã—ã§{len(qa_texts)}å€‹ã®åŸ‹ã‚è¾¼ã¿ã‚’ç”Ÿæˆ")
    else:
        # ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’è¶…ãˆã‚‹å ´åˆã¯åˆ†å‰²å‡¦ç†
        num_batches = (len(qa_texts) + MAX_BATCH_SIZE - 1) // MAX_BATCH_SIZE
        logger.info(f"  å¤§é‡ãƒ‡ãƒ¼ã‚¿ã®ãŸã‚{num_batches}å›ã«åˆ†å‰²ã—ã¦ãƒãƒƒãƒå‡¦ç†")

        for i in range(0, len(qa_texts), MAX_BATCH_SIZE):
            batch = qa_texts[i:i+MAX_BATCH_SIZE]
            # generate_embeddingsã¯è¾æ›¸ã®ãƒªã‚¹ãƒˆã‚’æƒ³å®šã—ã¦ã„ã‚‹ãŸã‚ã€å¤‰æ›
            batch_chunks = [{"text": text} for text in batch]
            batch_embeddings = analyzer.generate_embeddings(batch_chunks)
            qa_embeddings.extend(batch_embeddings)
            logger.info(f"    ãƒãƒƒãƒ {i//MAX_BATCH_SIZE + 1}/{num_batches} å®Œäº†: {len(batch)}å€‹")

    logger.info(f"Q/AåŸ‹ã‚è¾¼ã¿ç”Ÿæˆå®Œäº†: åˆè¨ˆ{len(qa_embeddings)}å€‹")

    # ã‚«ãƒãƒ¬ãƒƒã‚¸è¡Œåˆ—ã®è¨ˆç®—
    coverage_matrix = np.zeros((len(chunks), len(qa_pairs)))
    covered_chunks = set()

    # å„ãƒãƒ£ãƒ³ã‚¯ã«å¯¾ã™ã‚‹æœ€å¤§é¡ä¼¼åº¦ã‚’è¿½è·¡
    max_similarities = np.zeros(len(chunks))

    for i, doc_emb in enumerate(doc_embeddings):
        for j, qa_emb in enumerate(qa_embeddings):
            similarity = analyzer.cosine_similarity(doc_emb, qa_emb)
            coverage_matrix[i, j] = similarity

            # ã“ã®ãƒãƒ£ãƒ³ã‚¯ã®æœ€å¤§é¡ä¼¼åº¦ã‚’æ›´æ–°
            if similarity > max_similarities[i]:
                max_similarities[i] = similarity

            # é–¾å€¤ã‚’è¶…ãˆãŸã‚‰ã‚«ãƒãƒ¼ã•ã‚ŒãŸã¨ãƒãƒ¼ã‚¯
            if similarity >= threshold:
                covered_chunks.add(i)

    # çµ±è¨ˆæƒ…å ±ã®è¨ˆç®—
    coverage_rate = len(covered_chunks) / len(chunks) if chunks else 0
    avg_max_similarity = np.mean(max_similarities)

    # ã‚«ãƒãƒ¬ãƒƒã‚¸çµæœ
    coverage_results = {
        "coverage_rate": coverage_rate,
        "covered_chunks": len(covered_chunks),
        "total_chunks": len(chunks),
        "threshold": threshold,
        "avg_max_similarity": float(avg_max_similarity),
        "min_max_similarity": float(np.min(max_similarities)),
        "max_max_similarity": float(np.max(max_similarities)),
        "uncovered_chunks": list(set(range(len(chunks))) - covered_chunks),
        "coverage_distribution": {
            "high_coverage": int(np.sum(max_similarities >= 0.7)),
            "medium_coverage": int(np.sum((max_similarities >= 0.5) & (max_similarities < 0.7))),
            "low_coverage": int(np.sum(max_similarities < 0.5))
        }
    }

    return coverage_results, max_similarities.tolist()


def process_with_improved_methods(
    document_text: str,
    methods: List[str],
    model: str = "gpt-4o-mini",
    qa_per_chunk: int = 4,
    max_chunks: int = 300,
    lang: str = "auto"
) -> Tuple[List[Dict], SemanticCoverage, List[Dict]]:
    """
    æ”¹è‰¯ç‰ˆï¼š80%ã‚«ãƒãƒ¬ãƒƒã‚¸ã‚’é”æˆã™ã‚‹ãŸã‚ã®Q/Aç”Ÿæˆ

    Args:
        document_text: å‡¦ç†å¯¾è±¡ãƒ†ã‚­ã‚¹ãƒˆ
        methods: ä½¿ç”¨ã™ã‚‹æ‰‹æ³•ã®ãƒªã‚¹ãƒˆ
        model: ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«
        qa_per_chunk: ãƒãƒ£ãƒ³ã‚¯ã‚ãŸã‚Šã®Q/Aæ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 4ï¼‰
        max_chunks: å‡¦ç†ã™ã‚‹æœ€å¤§ãƒãƒ£ãƒ³ã‚¯æ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 300ï¼‰
        lang: è¨€èªã‚³ãƒ¼ãƒ‰ ("en", "ja", "auto")
    """
    all_qas = []

    # SemanticCoverageåˆæœŸåŒ–
    analyzer = SemanticCoverage(embedding_model="text-embedding-3-small")
    chunks = analyzer.create_semantic_chunks(document_text, verbose=False)
    logger.info(f"ãƒãƒ£ãƒ³ã‚¯ä½œæˆå®Œäº†: {len(chunks)}å€‹")

    # ã‚«ãƒãƒ¬ãƒƒã‚¸æˆ¦ç•¥ã®è¨­å®š
    total_chunks = len(chunks)
    target_coverage = 0.8  # 80%ç›®æ¨™

    # ãƒãƒ£ãƒ³ã‚¯å‡¦ç†ã®æœ€å¤§æ•°ã‚’è¨­å®š
    max_chunks_to_process = min(total_chunks, max_chunks)

    if total_chunks > max_chunks_to_process:
        # å‡ç­‰ã«ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
        step = total_chunks // max_chunks_to_process
        selected_chunks = [chunks[i] for i in range(0, total_chunks, step)][:max_chunks_to_process]
        logger.info(f"å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿: {total_chunks}ãƒãƒ£ãƒ³ã‚¯ã‹ã‚‰{len(selected_chunks)}ãƒãƒ£ãƒ³ã‚¯ã‚’ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°")
    else:
        selected_chunks = chunks

    logger.info(f"å‡¦ç†ãƒãƒ£ãƒ³ã‚¯æ•°: {len(selected_chunks)}, ãƒãƒ£ãƒ³ã‚¯ã‚ãŸã‚ŠQ/Aæ•°: {qa_per_chunk}")
    logger.info(f"äºˆæƒ³Q/Aç·æ•°: {len(selected_chunks) * qa_per_chunk}")

    # å„æ‰‹æ³•ã§Q/Aç”Ÿæˆ
    if "rule" in methods or "template" in methods:
        logger.info("åŒ…æ‹¬çš„Q/Aç”Ÿæˆã‚’é–‹å§‹...")

        for i, chunk in enumerate(selected_chunks):
            # å„ãƒãƒ£ãƒ³ã‚¯ã‹ã‚‰åŒ…æ‹¬çš„ãªQ/Aã‚’ç”Ÿæˆ
            chunk_qas = generate_comprehensive_qa_for_chunk(
                chunk['text'],
                i,
                qa_per_chunk=qa_per_chunk,
                lang=lang
            )
            all_qas.extend(chunk_qas)

            # é€²æ—è¡¨ç¤º
            if (i + 1) % 50 == 0:
                logger.info(f"  é€²æ—: {i + 1}/{len(selected_chunks)}ãƒãƒ£ãƒ³ã‚¯å‡¦ç†æ¸ˆã¿, ç”ŸæˆQ/Aæ•°: {len(all_qas)}")

    # LLMãƒ™ãƒ¼ã‚¹ã®æ‰‹æ³•ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
    if "llm" in methods:
        logger.info("LLMãƒ™ãƒ¼ã‚¹Q/Aç”Ÿæˆ...")
        # ã‚³ã‚¹ãƒˆåˆ¶ç´„ã®ãŸã‚ä¸€éƒ¨ã®ãƒãƒ£ãƒ³ã‚¯ã®ã¿
        for chunk in selected_chunks[:10]:
            # LLMã§é«˜å“è³ªãªQ/Aã‚’ç”Ÿæˆï¼ˆã“ã“ã¯å®Ÿéš›ã®LLMå‘¼ã³å‡ºã—ãŒå¿…è¦ï¼‰
            pass

    # é‡è¤‡é™¤å»ï¼ˆè³ªå•ã®é¡ä¼¼åº¦ãƒ™ãƒ¼ã‚¹ï¼‰
    unique_questions = {}
    for qa in all_qas:
        q = qa.get('question', '')
        # ç°¡æ˜“çš„ãªé‡è¤‡ãƒã‚§ãƒƒã‚¯ï¼ˆæœ€åˆã®30æ–‡å­—ï¼‰
        q_key = q[:30]
        if q_key not in unique_questions:
            unique_questions[q_key] = qa

    unique_qas = list(unique_questions.values())

    logger.info(f"Q/Aç”Ÿæˆå®Œäº†: {len(unique_qas)}å€‹ï¼ˆé‡è¤‡é™¤å»å¾Œï¼‰")

    # ã‚«ãƒãƒ¬ãƒƒã‚¸å‘ä¸Šã®ãŸã‚ã®è¿½åŠ ç”Ÿæˆ
    if len(unique_qas) < total_chunks * 2:
        logger.info(f"è¿½åŠ Q/Aç”Ÿæˆä¸­... (ç›®æ¨™: {total_chunks * 2}å€‹)")

        # ã¾ã å‡¦ç†ã—ã¦ã„ãªã„ãƒãƒ£ãƒ³ã‚¯ã‹ã‚‰è¿½åŠ ç”Ÿæˆ
        for i in range(len(selected_chunks), min(len(chunks), len(selected_chunks) + 50)):
            if i < len(chunks):
                chunk_qas = generate_comprehensive_qa_for_chunk(
                    chunks[i]['text'],
                    i,
                    qa_per_chunk=2,  # è¿½åŠ ã¯2å€‹ãšã¤
                    lang=lang
                )
                unique_qas.extend(chunk_qas)

    logger.info(f"æœ€çµ‚Q/Aæ•°: {len(unique_qas)}å€‹")

    return unique_qas, analyzer, chunks


def save_results(
    qa_pairs: List[Dict],
    coverage_results: Optional[Dict] = None,
    dataset_type: str = "custom",
    output_dir: str = "qa_output"
) -> Dict[str, str]:
    """çµæœã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜"""
    # qa_output/a03 ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ä¿å­˜
    output_path = Path(output_dir) / "a03"
    output_path.mkdir(parents=True, exist_ok=True)

    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

    # Q/Aãƒšã‚¢ã‚’ä¿å­˜ï¼ˆJSONï¼‰
    qa_file = output_path / f"qa_pairs_{dataset_type}_{timestamp}.json"
    with open(qa_file, 'w', encoding='utf-8') as f:
        json.dump(qa_pairs, f, ensure_ascii=False, indent=2)

    # Q/Aãƒšã‚¢ã‚’ä¿å­˜ï¼ˆCSVï¼‰
    qa_csv_file = output_path / f"qa_pairs_{dataset_type}_{timestamp}.csv"
    qa_df = pd.DataFrame(qa_pairs)
    qa_df.to_csv(qa_csv_file, index=False, encoding='utf-8')

    saved_files = {
        "qa_json": str(qa_file),
        "qa_csv": str(qa_csv_file)
    }

    # ã‚«ãƒãƒ¬ãƒƒã‚¸åˆ†æçµæœãŒã‚ã‚‹å ´åˆã¯ä¿å­˜
    if coverage_results:
        coverage_file = output_path / f"coverage_{dataset_type}_{timestamp}.json"
        with open(coverage_file, 'w', encoding='utf-8') as f:
            json.dump(coverage_results, f, ensure_ascii=False, indent=2)
        saved_files["coverage"] = str(coverage_file)

    # ã‚µãƒãƒªãƒ¼æƒ…å ±ã‚’ä¿å­˜
    summary = {
        "dataset_type": dataset_type,
        "generated_at": timestamp,
        "total_qa_pairs": len(qa_pairs),
        "files": saved_files
    }

    if coverage_results:
        summary["coverage_rate"] = coverage_results.get('coverage_rate', 0)
        summary["coverage_details"] = coverage_results.get('coverage_distribution', {})

    summary_file = output_path / f"summary_{dataset_type}_{timestamp}.json"
    with open(summary_file, 'w', encoding='utf-8') as f:
        json.dump(summary, f, ensure_ascii=False, indent=2)

    saved_files["summary"] = str(summary_file)

    logger.info(f"çµæœã‚’ä¿å­˜ã—ã¾ã—ãŸ: {output_path}")

    return saved_files


def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    parser = argparse.ArgumentParser(
        description="ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯ã‚«ãƒãƒ¬ãƒƒã‚¸åˆ†æã¨Q/Aç”Ÿæˆã‚·ã‚¹ãƒ†ãƒ ï¼ˆ80%ã‚«ãƒãƒ¬ãƒƒã‚¸é”æˆç‰ˆï¼‰"
    )

    parser.add_argument("--input", type=str, help="å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹")
    parser.add_argument("--dataset", type=str, choices=list(DATASET_CONFIGS.keys()), help="ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚¿ã‚¤ãƒ—")
    parser.add_argument("--max-docs", type=int, default=None, help="å‡¦ç†ã™ã‚‹æœ€å¤§æ–‡æ›¸æ•°")
    parser.add_argument("--methods", type=str, nargs='+', default=['rule', 'template'], help="ä½¿ç”¨ã™ã‚‹æ‰‹æ³•")
    parser.add_argument("--model", type=str, default="gpt-4o-mini", help="ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«")
    parser.add_argument("--output", type=str, default="qa_output", help="å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª")
    parser.add_argument("--analyze-coverage", action="store_true", help="ã‚«ãƒãƒ¬ãƒƒã‚¸åˆ†æã‚’å®Ÿè¡Œ")
    parser.add_argument("--coverage-threshold", type=float, default=0.65, help="ã‚«ãƒãƒ¬ãƒƒã‚¸åˆ¤å®šé–¾å€¤")
    parser.add_argument("--qa-per-chunk", type=int, default=4, help="ãƒãƒ£ãƒ³ã‚¯ã‚ãŸã‚Šã®Q/Aç”Ÿæˆæ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 4ï¼‰")
    parser.add_argument("--max-chunks", type=int, default=300, help="å‡¦ç†ã™ã‚‹æœ€å¤§ãƒãƒ£ãƒ³ã‚¯æ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 300ï¼‰")
    parser.add_argument("--demo", action="store_true", help="ãƒ‡ãƒ¢ãƒ¢ãƒ¼ãƒ‰")

    args = parser.parse_args()

    print("\n" + "=" * 80)
    print("ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯ã‚«ãƒãƒ¬ãƒƒã‚¸åˆ†æã¨Q/Aç”Ÿæˆã‚·ã‚¹ãƒ†ãƒ ï¼ˆæ”¹è‰¯ç‰ˆï¼‰")
    print("ç›®æ¨™ã‚«ãƒãƒ¬ãƒƒã‚¸: 80%")
    print("=" * 80)

    # APIã‚­ãƒ¼ãƒã‚§ãƒƒã‚¯
    api_key = os.getenv('OPENAI_API_KEY')
    print(f"\nğŸ“‹ ç’°å¢ƒãƒã‚§ãƒƒã‚¯:")
    print(f"  OpenAI APIã‚­ãƒ¼: {'âœ… è¨­å®šæ¸ˆã¿' if api_key else 'âŒ æœªè¨­å®š'}")

    # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    if args.demo or not args.input:
        print("\nğŸ“ ãƒ‡ãƒ¢ãƒ¢ãƒ¼ãƒ‰")
        document_text = "This is a demo text for testing purposes. " * 100
        dataset_type = "demo"
    else:
        print(f"\nğŸ“ å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«: {args.input}")
        print(f"  ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ: {args.dataset if args.dataset else 'è‡ªå‹•æ¤œå‡º'}")

        try:
            document_text = load_input_data(args.input, args.dataset, args.max_docs)
            dataset_type = args.dataset if args.dataset else "custom"
        except Exception as e:
            logger.error(f"ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            sys.exit(1)

    print(f"\nğŸ› ï¸  ä½¿ç”¨ã™ã‚‹æ‰‹æ³•: {', '.join(args.methods)}")
    print(f"  ã‚«ãƒãƒ¬ãƒƒã‚¸é–¾å€¤: {args.coverage_threshold}")
    print(f"  ãƒãƒ£ãƒ³ã‚¯ã‚ãŸã‚ŠQ/Aæ•°: {args.qa_per_chunk}")
    print(f"  æœ€å¤§å‡¦ç†ãƒãƒ£ãƒ³ã‚¯æ•°: {args.max_chunks}")
    print(f"  å‡ºåŠ›å…ˆ: {args.output}")

    print("\n" + "=" * 80)
    print("å‡¦ç†é–‹å§‹")
    print("=" * 80)

    try:
        # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‹ã‚‰è¨€èªæƒ…å ±ã‚’å–å¾—
        lang = "auto"
        if dataset_type in DATASET_CONFIGS:
            lang = DATASET_CONFIGS[dataset_type].get("lang", "auto")
            logger.info(f"ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆè¨€èª: {lang}")

        # Q/Aç”Ÿæˆå‡¦ç†ï¼ˆæ”¹è‰¯ç‰ˆï¼‰
        qa_pairs, analyzer, chunks = process_with_improved_methods(
            document_text,
            args.methods,
            args.model,
            qa_per_chunk=args.qa_per_chunk,
            max_chunks=args.max_chunks,
            lang=lang
        )

        # ã‚«ãƒãƒ¬ãƒƒã‚¸åˆ†æï¼ˆæ”¹è‰¯ç‰ˆï¼‰
        coverage_results = None
        if args.analyze_coverage and qa_pairs and api_key:
            print("\n" + "=" * 80)
            print("ã‚«ãƒãƒ¬ãƒƒã‚¸åˆ†æï¼ˆæ”¹è‰¯ç‰ˆï¼‰")
            print("=" * 80)

            try:
                # æ”¹è‰¯ç‰ˆã‚«ãƒãƒ¬ãƒƒã‚¸è¨ˆç®—
                coverage_results, max_similarities = calculate_improved_coverage(
                    chunks,
                    qa_pairs,
                    analyzer,
                    threshold=args.coverage_threshold
                )

                print(f"\nğŸ“Š ã‚«ãƒãƒ¬ãƒƒã‚¸åˆ†æçµæœ:")
                print(f"  ã‚«ãƒãƒ¬ãƒƒã‚¸ç‡: {coverage_results['coverage_rate']:.1%}")
                print(f"  ã‚«ãƒãƒ¼æ¸ˆã¿ãƒãƒ£ãƒ³ã‚¯: {coverage_results['covered_chunks']}/{coverage_results['total_chunks']}")
                print(f"  é–¾å€¤: {coverage_results['threshold']}")
                print(f"  å¹³å‡æœ€å¤§é¡ä¼¼åº¦: {coverage_results['avg_max_similarity']:.3f}")
                print(f"\n  ã‚«ãƒãƒ¬ãƒƒã‚¸åˆ†å¸ƒ:")
                print(f"    é«˜ã‚«ãƒãƒ¬ãƒƒã‚¸ (â‰¥0.7): {coverage_results['coverage_distribution']['high_coverage']}ãƒãƒ£ãƒ³ã‚¯")
                print(f"    ä¸­ã‚«ãƒãƒ¬ãƒƒã‚¸ (0.5-0.7): {coverage_results['coverage_distribution']['medium_coverage']}ãƒãƒ£ãƒ³ã‚¯")
                print(f"    ä½ã‚«ãƒãƒ¬ãƒƒã‚¸ (<0.5): {coverage_results['coverage_distribution']['low_coverage']}ãƒãƒ£ãƒ³ã‚¯")

                # ã‚«ãƒãƒ¬ãƒƒã‚¸ãŒä½ã„å ´åˆã®è­¦å‘Š
                if coverage_results['coverage_rate'] < 0.7:
                    print(f"\nâš ï¸ ã‚«ãƒãƒ¬ãƒƒã‚¸ãŒç›®æ¨™ã®80%ã«é”ã—ã¦ã„ã¾ã›ã‚“ã€‚")
                    print(f"  æ¨å¥¨äº‹é …:")
                    print(f"  1. é–¾å€¤ã‚’{args.coverage_threshold - 0.05:.2f}ã«ä¸‹ã’ã‚‹")
                    print(f"  2. ã‚ˆã‚Šå¤šãã®Q/Aã‚’ç”Ÿæˆã™ã‚‹ï¼ˆç¾åœ¨: {len(qa_pairs)}å€‹ï¼‰")
                    print(f"  3. LLMãƒ™ãƒ¼ã‚¹ã®æ‰‹æ³•ã‚’è¿½åŠ ã™ã‚‹ï¼ˆ--methods rule template llmï¼‰")

            except Exception as e:
                logger.warning(f"ã‚«ãƒãƒ¬ãƒƒã‚¸åˆ†æä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
                import traceback
                traceback.print_exc()

        # çµæœä¿å­˜
        saved_files = save_results(qa_pairs, coverage_results, dataset_type, args.output)

        # å®Œäº†ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
        print("\n" + "=" * 80)
        print("å‡¦ç†å®Œäº†")
        print("=" * 80)
        print(f"\nâœ… ç”Ÿæˆã•ã‚ŒãŸQ/Aãƒšã‚¢æ•°: {len(qa_pairs)}")
        print(f"âœ… ä¿å­˜ãƒ•ã‚¡ã‚¤ãƒ«:")
        for file_type, file_path in saved_files.items():
            print(f"  - {file_type}: {file_path}")

        # Q/Aã‚¿ã‚¤ãƒ—çµ±è¨ˆ
        if qa_pairs:
            print(f"\nğŸ“Š Q/Aãƒšã‚¢çµ±è¨ˆ:")
            type_counts = {}
            for qa in qa_pairs:
                qa_type = qa.get('type', 'unknown')
                type_counts[qa_type] = type_counts.get(qa_type, 0) + 1

            for qa_type, count in sorted(type_counts.items()):
                print(f"  - {qa_type}: {count}ä»¶")

    except Exception as e:
        logger.error(f"å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()