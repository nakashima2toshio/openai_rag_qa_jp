# streamlit run a34_rag_search_cloud_vs.py --server.port=8503
# a34_rag_search_cloud_vs.py - æœ€æ–°OpenAI Responses APIå®Œå…¨å¯¾å¿œç‰ˆï¼ˆå‹•çš„Vector Storeå¯¾å¿œãƒ»é‡è¤‡å•é¡Œä¿®æ­£ç‰ˆï¼‰
# OpenAI Responses API + file_search ãƒ„ãƒ¼ãƒ« + ç’°å¢ƒå¤‰æ•°APIã‚­ãƒ¼å¯¾å¿œ + å‹•çš„Vector Store IDç®¡ç†
"""
ğŸ” æœ€æ–°RAGæ¤œç´¢ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆå‹•çš„Vector Storeå¯¾å¿œãƒ»é‡è¤‡å•é¡Œä¿®æ­£ç‰ˆï¼‰

ã€å‰ææ¡ä»¶ã€‘
1. OpenAI APIã‚­ãƒ¼ã®ç’°å¢ƒå¤‰æ•°è¨­å®šï¼ˆå¿…é ˆï¼‰:
   export OPENAI_API_KEY='your-api-key-here'

2. å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªï¼ˆå¿…é ˆï¼‰:
   pip install streamlit openai
   pip install openai-agents

ã€å®Ÿè¡Œæ–¹æ³•ã€‘
streamlit run a34_rag_search_cloud_vs.py --server.port=8501

ã€ä¸»è¦æ©Ÿèƒ½ã€‘
âœ… æœ€æ–°Responses APIä½¿ç”¨
âœ… file_search ãƒ„ãƒ¼ãƒ«ã§Vector Storeæ¤œç´¢
âœ… å‹•çš„Vector Store IDç®¡ç†ï¼ˆvector_stores.jsonï¼‰
âœ… é‡è¤‡Vector Storeå¯¾å¿œï¼ˆæœ€æ–°å„ªå…ˆé¸æŠï¼‰
âœ… ãƒ•ã‚¡ã‚¤ãƒ«å¼•ç”¨è¡¨ç¤º
âœ… å‹å®‰å…¨å®Ÿè£…ï¼ˆå‹ã‚¨ãƒ©ãƒ¼å®Œå…¨ä¿®æ­£ï¼‰
âœ… ç’°å¢ƒå¤‰æ•°ã§APIã‚­ãƒ¼ç®¡ç†
âœ… è‹±èª/æ—¥æœ¬èªè³ªå•å¯¾å¿œ
âœ… ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚ºå¯èƒ½ãªæ¤œç´¢ã‚ªãƒ—ã‚·ãƒ§ãƒ³
âœ… æœ€æ–°Vector Storeè‡ªå‹•å–å¾—ãƒ»æ›´æ–°æ©Ÿèƒ½

ã€Vector Storeé€£æºã€‘
- a30_020_make_vsid.py ã§ä½œæˆã•ã‚ŒãŸVector Storeã‚’è‡ªå‹•èªè­˜
- vector_stores.json ãƒ•ã‚¡ã‚¤ãƒ«ã§å‹•çš„ç®¡ç†
- åŒåVector Storeé‡è¤‡æ™‚ã¯æœ€æ–°ä½œæˆæ—¥æ™‚ã‚’å„ªå…ˆ
- OpenAI APIã‹ã‚‰æœ€æ–°çŠ¶æ…‹ã‚’å–å¾—ãƒ»æ›´æ–°
"""
import streamlit as st
import time
import logging
import json
from typing import List, Dict, Any, Optional, Tuple
from datetime import datetime
from pathlib import Path
import traceback

# OpenAI SDK ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
try:
    from openai import OpenAI

    OPENAI_AVAILABLE = True
    logger = logging.getLogger(__name__)
    logger.info("âœ… OpenAI SDK ãŒãƒ­ãƒ¼ãƒ‰ã•ã‚Œã¾ã—ãŸ")
except ImportError as e:
    OPENAI_AVAILABLE = False
    st.error(f"OpenAI SDK ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {e}")
    st.stop()

# Agent SDK ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
try:
    from agents import Agent, Runner, SQLiteSession

    AGENT_SDK_AVAILABLE = True
    logger.info("âœ… OpenAI Agent SDK ã‚‚ãƒ­ãƒ¼ãƒ‰ã•ã‚Œã¾ã—ãŸ")
except ImportError as e:
    AGENT_SDK_AVAILABLE = False
    logger.info(f"Agent SDK ã¯åˆ©ç”¨ã§ãã¾ã›ã‚“ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³æ©Ÿèƒ½ã®ãŸã‚å•é¡Œãªã—ï¼‰: {e}")

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


# ===================================================================
# Vector Storeè¨­å®šç®¡ç†ã‚¯ãƒ©ã‚¹ï¼ˆé‡è¤‡å•é¡Œä¿®æ­£ç‰ˆï¼‰
# ===================================================================
class VectorStoreManager:
    """Vector Storeè¨­å®šã®å‹•çš„ç®¡ç†ï¼ˆé‡è¤‡å•é¡Œä¿®æ­£ç‰ˆï¼‰"""

    CONFIG_FILE_PATH = Path("vector_stores.json")

    # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®Vector Storeè¨­å®šï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç”¨ï¼‰
    # æ³¨: CC Newsé–¢é€£ã‚’ä¸Šä½ã«é…ç½®ï¼ˆPythonã®è¾æ›¸ã¯3.7+ã§æŒ¿å…¥é †åºã‚’ä¿æŒï¼‰
    DEFAULT_VECTOR_STORES = {
        "CC News Q&A (LLM)"       : "vs_cc_news_basic_placeholder",  # CC News LLMç”Ÿæˆæ–¹å¼
        "CC News Q&A (Coverage)"  : "vs_cc_news_coverage_placeholder",  # CC Newsã‚«ãƒãƒ¬ãƒƒã‚¸æ”¹è‰¯æ–¹å¼
        "CC News Q&A (Hybrid)"    : "vs_cc_news_hybrid_placeholder",  # CC Newsãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ç”Ÿæˆæ–¹å¼
        "Customer Support FAQ"    : "vs_68c94da49c80819189dd42d6e941c4b5",
        "Science & Technology Q&A": "vs_68c94db932fc8191b6e17f86e6601bc1",
        "Medical Q&A"             : "vs_68c94daffc708191b3c561f4dd6b2af8",
        "Legal Q&A"               : "vs_68c94dc1cc008191a197bdbc3947a67b",
        "Trivia Q&A"              : "vs_68c94dc9e6b08191946d7cafcd9880a3",
        "Unified Knowledge Base"  : "vs_unified_placeholder",  # çµ±åˆãƒŠãƒ¬ãƒƒã‚¸ãƒ™ãƒ¼ã‚¹ï¼ˆãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼ï¼‰
    }

    # a30_020_make_vsid.py ã®VectorStoreConfigã¨å¯¾å¿œã™ã‚‹ãƒãƒƒãƒ”ãƒ³ã‚°
    STORE_NAME_MAPPING = {
        "customer_support_faq": "Customer Support FAQ Knowledge Base",
        "medical_qa"          : "Medical Q&A Knowledge Base",
        "sciq_qa"             : "Science & Technology Q&A Knowledge Base",
        "legal_qa"            : "Legal Q&A Knowledge Base",
        "trivia_qa"           : "Trivia Q&A Knowledge Base",
        "unified_all"         : "Unified Knowledge Base - All Domains"
    }

    # è¡¨ç¤ºåã¸ã®é€†ãƒãƒƒãƒ”ãƒ³ã‚°ï¼ˆUIè¡¨ç¤ºç”¨ï¼‰
    DISPLAY_NAME_MAPPING = {
        "Customer Support FAQ Knowledge Base"              : "Customer Support FAQ",
        "Medical Q&A Knowledge Base"                       : "Medical Q&A",
        "Science & Technology Q&A Knowledge Base"          : "Science & Technology Q&A",
        "Legal Q&A Knowledge Base"                         : "Legal Q&A",
        "Trivia Q&A Knowledge Base"                        : "Trivia Q&A",
        "Unified Knowledge Base - All Domains"             : "Unified Knowledge Base",
        # CC News Q&Aé–¢é€£ã®ãƒãƒƒãƒ”ãƒ³ã‚°
        "CC News Q&A - Basic Generation (a02_make_qa)"     : "CC News Q&A (LLM)",
        "CC News Q&A - Coverage Improved (a03_coverage)"   : "CC News Q&A (Coverage)",
        "CC News Q&A - Hybrid Method (a10_hybrid)"         : "CC News Q&A (Hybrid)"
    }

    def __init__(self, openai_client: OpenAI = None):
        self.openai_client = openai_client
        self._cache = {}
        self._last_update = None

    def load_vector_stores(self) -> Dict[str, str]:
        """Vector Storeè¨­å®šã‚’èª­ã¿è¾¼ã¿"""
        try:
            if self.CONFIG_FILE_PATH.exists():
                with open(self.CONFIG_FILE_PATH, 'r', encoding='utf-8') as f:
                    data = json.load(f)

                # è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®å½¢å¼ç¢ºèª
                if 'vector_stores' in data and isinstance(data['vector_stores'], dict):
                    stores = data['vector_stores']
                    logger.info(f"âœ… Vector Storeè¨­å®šã‚’èª­ã¿è¾¼ã¿: {len(stores)}ä»¶")
                    return stores
                else:
                    logger.warning("âš ï¸ è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼ãŒä¸æ­£ã§ã™")
                    return self.DEFAULT_VECTOR_STORES.copy()
            else:
                logger.info("â„¹ï¸ è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ã¾ã›ã‚“ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’ä½¿ç”¨")
                return self.DEFAULT_VECTOR_STORES.copy()

        except Exception as e:
            logger.error(f"âŒ Vector Storeè¨­å®šèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            st.warning(f"è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            return self.DEFAULT_VECTOR_STORES.copy()

    def save_vector_stores(self, stores: Dict[str, str]) -> bool:
        """Vector Storeè¨­å®šã‚’ä¿å­˜"""
        try:
            config_data = {
                "vector_stores": stores,
                "last_updated" : datetime.now().isoformat(),
                "source"       : "a34_rag_search_cloud_vs.py",
                "version"      : "1.1"
            }

            with open(self.CONFIG_FILE_PATH, 'w', encoding='utf-8') as f:
                json.dump(config_data, f, indent=2, ensure_ascii=False)

            logger.info(f"âœ… Vector Storeè¨­å®šã‚’ä¿å­˜: {self.CONFIG_FILE_PATH}")
            return True

        except Exception as e:
            logger.error(f"âŒ Vector Storeè¨­å®šä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
            st.error(f"è¨­å®šä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
            return False

    def fetch_latest_vector_stores(self) -> Dict[str, str]:
        """OpenAI APIã‹ã‚‰æœ€æ–°ã®Vector Storeä¸€è¦§ã‚’å–å¾—ã—ã€æ—¢çŸ¥ã®åå‰ã¨ãƒãƒƒãƒãƒ³ã‚°ï¼ˆé‡è¤‡å•é¡Œä¿®æ­£ç‰ˆï¼‰"""
        if not self.openai_client:
            logger.warning("âš ï¸ OpenAI ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆãŒæœªè¨­å®šã§ã™")
            return self.load_vector_stores()

        try:
            # OpenAI APIã‹ã‚‰Vector Storeä¸€è¦§ã‚’å–å¾—
            stores_response = self.openai_client.vector_stores.list()

            # Vector Storeã‚’ä½œæˆæ—¥æ™‚ã§ã‚½ãƒ¼ãƒˆï¼ˆæ–°ã—ã„é †ï¼‰
            sorted_stores = sorted(
                stores_response.data,
                key=lambda x: x.created_at if hasattr(x, 'created_at') else 0,
                reverse=True
            )

            api_stores = {}
            store_candidates = {}  # åŒåStoreå€™è£œã‚’ç®¡ç†

            logger.info(f"ğŸ“Š å–å¾—ã—ãŸVector Storeæ•°: {len(sorted_stores)}")

            for store in sorted_stores:
                store_name = store.name
                store_id = store.id
                created_at = getattr(store, 'created_at', 0)

                logger.info(f"ğŸ” å‡¦ç†ä¸­: '{store_name}' ({store_id}) - ä½œæˆæ—¥æ™‚: {created_at}")

                # æ—¢çŸ¥ã®store_nameãƒ‘ã‚¿ãƒ¼ãƒ³ã¨ã®ãƒãƒƒãƒãƒ³ã‚°
                matched_display_name = None

                # å®Œå…¨ä¸€è‡´ç¢ºèª
                if store_name in self.DISPLAY_NAME_MAPPING:
                    matched_display_name = self.DISPLAY_NAME_MAPPING[store_name]
                else:
                    # éƒ¨åˆ†ä¸€è‡´ç¢ºèªï¼ˆæŸ”è»Ÿãªãƒãƒƒãƒãƒ³ã‚°ï¼‰
                    for full_name, display_name in self.DISPLAY_NAME_MAPPING.items():
                        if (full_name.lower() in store_name.lower() or
                                any(keyword in store_name.lower() for keyword in full_name.lower().split())):
                            matched_display_name = display_name
                            break

                if matched_display_name:
                    # åŒåã®å ´åˆã¯æœ€æ–°ã®ã‚‚ã®ï¼ˆä½œæˆæ—¥æ™‚ãŒæ–°ã—ã„ï¼‰ã‚’å„ªå…ˆ
                    if matched_display_name not in store_candidates:
                        store_candidates[matched_display_name] = {
                            'id'        : store_id,
                            'name'      : store_name,
                            'created_at': created_at
                        }
                        logger.info(f"âœ… æ–°è¦å€™è£œ: '{matched_display_name}' -> '{store_name}' ({store_id})")
                    else:
                        # æ—¢å­˜å€™è£œã¨æ¯”è¼ƒ
                        existing = store_candidates[matched_display_name]
                        if created_at > existing['created_at']:
                            logger.info(
                                f"ğŸ”„ æ›´æ–°: '{matched_display_name}' -> '{store_name}' ({store_id}) [æ–°: {created_at} > æ—§: {existing['created_at']}]")
                            store_candidates[matched_display_name] = {
                                'id'        : store_id,
                                'name'      : store_name,
                                'created_at': created_at
                            }
                        else:
                            logger.info(
                                f"â­ï¸ ã‚¹ã‚­ãƒƒãƒ—: '{matched_display_name}' -> '{store_name}' ({store_id}) [æ–°: {created_at} <= æ—¢å­˜: {existing['created_at']}]")
                else:
                    # æ—¢çŸ¥ãƒ‘ã‚¿ãƒ¼ãƒ³ã«ãƒãƒƒãƒã—ãªã„å ´åˆ
                    if store_name not in store_candidates:
                        store_candidates[store_name] = {
                            'id'        : store_id,
                            'name'      : store_name,
                            'created_at': created_at
                        }
                        logger.info(f"â„¹ï¸ æ–°è¦åº—èˆ—: '{store_name}' ({store_id})")

            # æœ€çµ‚çš„ãªapi_storesã‚’æ§‹ç¯‰ï¼ˆDEFAULT_VECTOR_STORESã®é †åºã‚’ç¶­æŒï¼‰
            # ã¾ãšDEFAULT_VECTOR_STORESã®é †åºã§ã‚½ãƒ¼ãƒˆ
            default_order = list(self.DEFAULT_VECTOR_STORES.keys())

            # DEFAULT_VECTOR_STORESã«å«ã¾ã‚Œã‚‹ã‚‚ã®ã‚’å…ˆã«è¿½åŠ 
            for display_name in default_order:
                if display_name in store_candidates:
                    candidate = store_candidates[display_name]
                    api_stores[display_name] = candidate['id']
                    logger.info(f"ğŸ¯ æœ€çµ‚é¸æŠ: '{display_name}' -> {candidate['id']} (ä½œæˆæ—¥æ™‚: {candidate['created_at']})")

            # DEFAULT_VECTOR_STORESã«å«ã¾ã‚Œãªã„æ–°è¦Storeã‚’å¾Œã«è¿½åŠ 
            for display_name, candidate in store_candidates.items():
                if display_name not in api_stores:
                    api_stores[display_name] = candidate['id']
                    logger.info(f"ğŸ¯ æœ€çµ‚é¸æŠï¼ˆæ–°è¦ï¼‰: '{display_name}' -> {candidate['id']} (ä½œæˆæ—¥æ™‚: {candidate['created_at']})")

            if api_stores:
                logger.info(f"âœ… OpenAI APIã‹ã‚‰{len(api_stores)}å€‹ã®Vector Storeã‚’å–å¾—å®Œäº†")
                return api_stores
            else:
                logger.warning("âš ï¸ APIã‹ã‚‰æœ‰åŠ¹ãªVector StoreãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
                return self.load_vector_stores()

        except Exception as e:
            logger.error(f"âŒ OpenAI APIå–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
            logger.error(traceback.format_exc())
            st.warning(f"æœ€æ–°æƒ…å ±ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
            return self.load_vector_stores()

    def get_vector_stores(self, force_refresh: bool = False) -> Dict[str, str]:
        """Vector Storeä¸€è¦§ã‚’å–å¾—ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥æ©Ÿèƒ½ä»˜ãï¼‰"""
        now = datetime.now()

        # å¼·åˆ¶ãƒªãƒ•ãƒ¬ãƒƒã‚·ãƒ¥ã¾ãŸã¯åˆå›å–å¾—ã®å ´åˆ
        if force_refresh or not self._cache:
            logger.info("ğŸ”„ Vector Storeæƒ…å ±ã‚’æ›´æ–°ä¸­...")

            # APIã‹ã‚‰ã®æœ€æ–°æƒ…å ±ã‚’å–å¾—
            if self.openai_client and st.session_state.get('auto_refresh_stores', True):
                try:
                    api_stores = self.fetch_latest_vector_stores()
                    self._cache = api_stores
                    self._last_update = now
                    return api_stores
                except Exception as e:
                    logger.warning(f"âš ï¸ APIå–å¾—ã«å¤±æ•—ã€è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰èª­ã¿è¾¼ã¿: {e}")

            # è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰èª­ã¿è¾¼ã¿ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
            stores = self.load_vector_stores()
            self._cache = stores
            self._last_update = now
            return stores

        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒã‚§ãƒƒã‚¯ï¼ˆ5åˆ†é–“æœ‰åŠ¹ï¼‰
        if self._last_update and (now - self._last_update).seconds >= 300:
            logger.info("â° ã‚­ãƒ£ãƒƒã‚·ãƒ¥æœ‰åŠ¹æœŸé™åˆ‡ã‚Œã€æ›´æ–°ä¸­...")
            return self.get_vector_stores(force_refresh=True)

        logger.info("ğŸ’¾ ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰å–å¾—")
        return self._cache

    def refresh_and_save(self) -> Dict[str, str]:
        """æœ€æ–°ã®Vector Storeæƒ…å ±ã‚’å–å¾—ã—ã¦ä¿å­˜"""
        if not self.openai_client:
            st.error("OpenAI ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
            return self.load_vector_stores()

        try:
            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒªã‚¢
            self._cache = {}
            self._last_update = None

            # æœ€æ–°æƒ…å ±ã‚’å¼·åˆ¶å–å¾—
            latest_stores = self.get_vector_stores(force_refresh=True)

            # è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
            if self.save_vector_stores(latest_stores):
                st.success(f"âœ… Vector Storeè¨­å®šã‚’æ›´æ–°ã—ã¾ã—ãŸï¼ˆ{len(latest_stores)}ä»¶ï¼‰")

                # è©³ç´°æƒ…å ±ã‚’è¡¨ç¤º
                with st.expander("ğŸ“Š æ›´æ–°ã•ã‚ŒãŸVector Storeä¸€è¦§", expanded=True):
                    for name, store_id in latest_stores.items():
                        st.write(f"**{name}**: `{store_id}`")

                return latest_stores
            else:
                st.error("âŒ è¨­å®šã®ä¿å­˜ã«å¤±æ•—ã—ã¾ã—ãŸ")
                return self.load_vector_stores()

        except Exception as e:
            st.error(f"âŒ æ›´æ–°å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
            logger.error(f"æ›´æ–°å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
            logger.error(traceback.format_exc())
            return self.load_vector_stores()

    def debug_vector_stores(self) -> Dict[str, Any]:
        """ãƒ‡ãƒãƒƒã‚°ç”¨ï¼šVector Storeæƒ…å ±ã®è©³ç´°å–å¾—"""
        debug_info = {
            "config_file_exists": self.CONFIG_FILE_PATH.exists(),
            "cached_stores"     : self._cache,
            "last_update"       : self._last_update.isoformat() if self._last_update else None,
            "api_stores"        : {}
        }

        if self.openai_client:
            try:
                stores_response = self.openai_client.vector_stores.list()
                for store in stores_response.data:
                    debug_info["api_stores"][store.name] = {
                        "id"         : store.id,
                        "created_at" : store.created_at,
                        "file_counts": getattr(store, 'file_counts', None),
                        "usage_bytes": getattr(store, 'usage_bytes', None)
                    }
            except Exception as e:
                debug_info["api_error"] = str(e)

        return debug_info


# ã‚°ãƒ­ãƒ¼ãƒãƒ« Vector Store Manager ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
@st.cache_resource
def get_vector_store_manager():
    """Vector Store Manager ã®ã‚·ãƒ³ã‚°ãƒ«ãƒˆãƒ³å–å¾—"""
    try:
        openai_client = OpenAI()
        return VectorStoreManager(openai_client)
    except Exception as e:
        logger.warning(f"OpenAI ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåˆæœŸåŒ–å¤±æ•—: {e}")
        return VectorStoreManager()


# ===================================================================
# å‹•çš„Vector Storeå–å¾—
# ===================================================================
def get_current_vector_stores(force_refresh: bool = False) -> Tuple[Dict[str, str], List[str]]:
    """ç¾åœ¨ã®Vector Storeè¨­å®šã‚’å–å¾—"""
    manager = get_vector_store_manager()
    stores = manager.get_vector_stores(force_refresh=force_refresh)
    store_list = list(stores.keys())
    return stores, store_list


# helper_ragã‹ã‚‰ãƒ¢ãƒ‡ãƒ«é–¢é€£ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
try:
    from helper_rag import AppConfig, select_model, show_model_info
    HELPER_AVAILABLE = True
except ImportError as e:
    HELPER_AVAILABLE = False
    logger.warning(f"ãƒ˜ãƒ«ãƒ‘ãƒ¼ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã«å¤±æ•—: {e}")

# ãƒ†ã‚¹ãƒˆç”¨è³ªå•ï¼ˆè‹±èªç‰ˆ - RAGãƒ‡ãƒ¼ã‚¿ã«æœ€é©åŒ–ï¼‰
test_questions_en = [
    "How do I create a new account?",
    "What payment methods are available?",
    "Can I return a product?",
    "I forgot my password",
    "How can I contact the support team?"
]

test_questions_2_en = [
    "What are the latest trends in artificial intelligence?",
    "What is the principle of quantum computing?",
    "What are the types and characteristics of renewable energy?",
    "What are the current status and challenges of gene editing technology?",
    "What are the latest technologies in space exploration?"
]

test_questions_3_en = [
    "How to prevent high blood pressure?",
    "What are the symptoms and treatment of diabetes?",
    "What are the risk factors for heart disease?",
    "What are the guidelines for healthy eating?",
    "What is the relationship between exercise and health?"
]

test_questions_4_en = [
    "What are the important clauses in contracts?",
    "How to protect intellectual property rights?",
    "What are the basic principles of labor law?",
    "What is an overview of personal data protection law?",
    "What is the scope of application of consumer protection law?"
]

# çµ±åˆãƒŠãƒ¬ãƒƒã‚¸ãƒ™ãƒ¼ã‚¹ç”¨ã®ãƒ†ã‚¹ãƒˆè³ªå•ï¼ˆå…¨ãƒ‰ãƒ¡ã‚¤ãƒ³ã‹ã‚‰ï¼‰
test_questions_unified_en = [
    "How do I reset my password?",  # Customer Support
    "What is machine learning?",  # Science & Technology
    "What are the symptoms of flu?",  # Medical
    "What is a non-disclosure agreement?",  # Legal
    "Who invented the telephone?",  # Trivia
]

# CC News Q&Aç”¨ã®ãƒ†ã‚¹ãƒˆè³ªå•ï¼ˆè‹±èªï¼‰
test_questions_cc_news_en = [
    "What are the latest developments in artificial intelligence?",
    "What happened in the recent tech industry news?",
    "What are the current trends in global markets?",
    "What are the major political events this week?",
    "What innovations are emerging in technology?"
]

# æ—¥æœ¬èªãƒ†ã‚¹ãƒˆè³ªå•ã¯å‰Šé™¤ï¼ˆè‹±èªã®ã¿ä½¿ç”¨ï¼‰

# OpenAI APIã‚­ãƒ¼ã®è¨­å®šï¼ˆç’°å¢ƒå¤‰æ•°ã‹ã‚‰è‡ªå‹•å–å¾—ï¼‰
try:
    # ç’°å¢ƒå¤‰æ•° OPENAI_API_KEY ã‹ã‚‰è‡ªå‹•çš„ã«èª­ã¿å–ã‚Š
    openai_client = OpenAI()
    logger.info("âœ… OpenAI APIã‚­ãƒ¼ãŒç’°å¢ƒå¤‰æ•°ã‹ã‚‰æ­£å¸¸ã«èª­ã¿è¾¼ã¾ã‚Œã¾ã—ãŸ")
except Exception as e:
    st.error(f"OpenAI API ã‚­ãƒ¼ã®è¨­å®šã«å•é¡ŒãŒã‚ã‚Šã¾ã™: {e}")
    st.error("ç’°å¢ƒå¤‰æ•° OPENAI_API_KEY ãŒè¨­å®šã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèªã—ã¦ãã ã•ã„")
    st.code("export OPENAI_API_KEY='your-api-key-here'")
    st.stop()


class ModernRAGManager:
    """æœ€æ–°Responses API + file_search ã‚’ä½¿ç”¨ã—ãŸRAGãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼"""

    def __init__(self):
        self.agent_sessions = {}  # Agent SDKç”¨ã‚»ãƒƒã‚·ãƒ§ãƒ³ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰

    def search_with_responses_api(self, query: str, store_name: str, store_id: str, **kwargs) -> Tuple[
        str, Dict[str, Any]]:
        """æœ€æ–°Responses API + file_search ãƒ„ãƒ¼ãƒ«ã‚’ä½¿ç”¨ã—ãŸæ¤œç´¢"""
        try:
            # file_search ãƒ„ãƒ¼ãƒ«ã®è¨­å®šï¼ˆæ­£ã—ã„å‹ã§å®šç¾©ï¼‰
            file_search_tool_dict: Dict[str, Any] = {
                "type"            : "file_search",
                "vector_store_ids": [store_id]
            }

            # ã‚ªãƒ—ã‚·ãƒ§ãƒ³è¨­å®šï¼ˆå‹å®‰å…¨ãªæ–¹æ³•ï¼‰
            max_results = kwargs.get('max_results', 20)
            include_results = kwargs.get('include_results', True)
            filters = kwargs.get('filters', None)
            selected_model = kwargs.get('selected_model', 'gpt-4o-mini')  # ãƒ¢ãƒ‡ãƒ«é¸æŠã‚’å—ã‘å–ã‚‹

            # å‹å®‰å…¨ãªè¾æ›¸æ›´æ–°
            if max_results and isinstance(max_results, int):
                file_search_tool_dict["max_num_results"] = max_results
            if filters is not None:
                file_search_tool_dict["filters"] = filters

            # include ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®è¨­å®š
            include_params = []
            if include_results:
                include_params.append("file_search_call.results")

            # Responses APIå‘¼ã³å‡ºã—ï¼ˆå‹å®‰å…¨ãªæ–¹æ³•ï¼‰
            # é¸æŠã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨
            response = openai_client.responses.create(
                model=selected_model,
                input=query,
                tools=[file_search_tool_dict],  # type: ignore[arg-type]
                include=include_params if include_params else None
            )

            # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ãƒ†ã‚­ã‚¹ãƒˆã®æŠ½å‡º
            response_text = self._extract_response_text(response)

            # ãƒ•ã‚¡ã‚¤ãƒ«å¼•ç”¨ã®æŠ½å‡º
            citations = self._extract_citations(response)

            # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®æ§‹ç¯‰ï¼ˆå‹ã‚’æ˜ç¤ºçš„ã«æŒ‡å®šï¼‰
            metadata: Dict[str, Any] = {
                "store_name": store_name,
                "store_id"  : store_id,
                "query"     : query,
                "timestamp" : datetime.now().isoformat(),
                "model"     : selected_model,  # é¸æŠã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã‚’è¨˜éŒ²
                "method"    : "responses_api_file_search",
                "citations" : citations,
                "tool_calls": self._extract_tool_calls(response)
            }

            # ä½¿ç”¨çµ±è¨ˆãŒã‚ã‚Œã°è¿½åŠ ï¼ˆå‹å®‰å…¨ãªæ–¹æ³•ï¼‰
            if hasattr(response, 'usage') and response.usage is not None:
                try:
                    # ResponseUsageã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’è¾æ›¸ã«å¤‰æ›
                    if hasattr(response.usage, 'model_dump'):
                        metadata["usage"] = response.usage.model_dump()
                    elif hasattr(response.usage, 'dict'):
                        metadata["usage"] = response.usage.dict()
                    else:
                        # æ‰‹å‹•ã§å±æ€§ã‚’æŠ½å‡º
                        usage_dict = {}
                        for attr in ['prompt_tokens', 'completion_tokens', 'total_tokens']:
                            if hasattr(response.usage, attr):
                                usage_dict[attr] = getattr(response.usage, attr)
                        metadata["usage"] = usage_dict
                except Exception as e:
                    logger.warning(f"ä½¿ç”¨çµ±è¨ˆã®å¤‰æ›ã«å¤±æ•—: {e}")
                    metadata["usage"] = str(response.usage)

            return response_text, metadata

        except Exception as e:
            error_msg = f"Responses APIæ¤œç´¢ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}"
            logger.error(error_msg)
            logger.error(traceback.format_exc())

            # ã‚¨ãƒ©ãƒ¼æ™‚ã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ï¼ˆå‹å®‰å…¨ï¼‰
            error_metadata: Dict[str, Any] = {
                "error"     : str(e),
                "method"    : "responses_api_error",
                "store_name": store_name,
                "store_id"  : store_id,
                "query"     : query,
                "timestamp" : datetime.now().isoformat()
            }
            return error_msg, error_metadata

    def search_with_agent_sdk(self, query: str, store_name: str, store_id: str) -> Tuple[str, Dict[str, Any]]:
        """Agent SDKã‚’ä½¿ç”¨ã—ãŸæ¤œç´¢ï¼ˆç°¡æ˜“ç‰ˆ - file_searchã¯Responses APIã§å®Ÿè¡Œï¼‰"""
        try:
            if not AGENT_SDK_AVAILABLE:
                logger.info("Agent SDKåˆ©ç”¨ä¸å¯ã€Responses APIã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯")
                return self.search_with_responses_api(query, store_name, store_id)

            # æ³¨æ„: Agent SDKã§ã®file_searchãƒ„ãƒ¼ãƒ«çµ±åˆã¯è¤‡é›‘ãªãŸã‚ã€
            # ç¾åœ¨ã¯ç°¡æ˜“ç‰ˆã¨ã—ã¦é€šå¸¸ã®Agentå®Ÿè¡Œã®ã¿è¡Œã„ã€
            # å®Ÿéš›ã®RAGæ©Ÿèƒ½ã¯Responses APIã«å§”è­²

            # Agent SDKã‚»ãƒƒã‚·ãƒ§ãƒ³ã®å–å¾—/ä½œæˆ
            session_key = f"{store_name}_agent"
            if session_key not in self.agent_sessions:
                self.agent_sessions[session_key] = SQLiteSession(session_key)

            session = self.agent_sessions[session_key]

            # ç°¡æ˜“Agentä½œæˆï¼ˆfile_searchãªã—ï¼‰
            agent = Agent(
                name=f"RAG_Agent_{store_name.replace(' ', '_')}",
                instructions=f"""
                You are a helpful assistant specializing in {store_name}.
                Provide informative and accurate responses based on your knowledge.
                Be professional and helpful in your responses.
                """,
                model="gpt-4o-mini"
            )

            # Runnerå®Ÿè¡Œï¼ˆã‚»ãƒƒã‚·ãƒ§ãƒ³ç®¡ç†ã®ã¿ã®åˆ©ç‚¹ï¼‰
            result = Runner.run_sync(
                agent,
                query,
                session=session
            )

            response_text = result.final_output if hasattr(result, 'final_output') else str(result)

            # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®æ§‹ç¯‰
            metadata: Dict[str, Any] = {
                "store_name": store_name,
                "store_id"  : store_id,
                "query"     : query,
                "timestamp" : datetime.now().isoformat(),
                "model"     : "gpt-4o-mini",
                "method"    : "agent_sdk_simple_session",
                "note"      : "Agent SDKã‚»ãƒƒã‚·ãƒ§ãƒ³ç®¡ç†ã®ã¿ã€RAGæ©Ÿèƒ½ãªã—"
            }

            logger.info("Agent SDKæ¤œç´¢å®Œäº†ï¼ˆç°¡æ˜“ç‰ˆï¼‰")
            return response_text, metadata

        except Exception as e:
            error_msg = f"Agent SDKæ¤œç´¢ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ: {str(e)}"
            logger.error(error_msg)
            logger.warning("Agent SDKã‚¨ãƒ©ãƒ¼ã«ã‚ˆã‚ŠResponses APIã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯")
            # Agent SDKãŒå¤±æ•—ã—ãŸå ´åˆã¯Responses APIã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            return self.search_with_responses_api(query, store_name, store_id)

    def search(self, query: str, store_name: str, store_id: str, use_agent_sdk: bool = True, **kwargs) -> Tuple[
        str, Dict[str, Any]]:
        """çµ±åˆæ¤œç´¢ãƒ¡ã‚½ãƒƒãƒ‰"""
        if use_agent_sdk and AGENT_SDK_AVAILABLE:
            return self.search_with_agent_sdk(query, store_name, store_id)
        else:
            return self.search_with_responses_api(query, store_name, store_id, **kwargs)

    def _extract_response_text(self, response) -> str:
        """ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡º"""
        try:
            # output_textå±æ€§ãŒã‚ã‚‹å ´åˆ
            if hasattr(response, 'output_text'):
                return response.output_text

            # outputé…åˆ—ã‹ã‚‰æŠ½å‡º
            if hasattr(response, 'output') and response.output:
                for item in response.output:
                    if hasattr(item, 'type') and item.type == "message":
                        if hasattr(item, 'content') and item.content:
                            for content in item.content:
                                if hasattr(content, 'type') and content.type == "output_text":
                                    return content.text

            return "ãƒ¬ã‚¹ãƒãƒ³ã‚¹ãƒ†ã‚­ã‚¹ãƒˆã®æŠ½å‡ºã«å¤±æ•—ã—ã¾ã—ãŸ"

        except Exception as e:
            logger.error(f"ãƒ¬ã‚¹ãƒãƒ³ã‚¹ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
            return f"ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}"

    def _extract_citations(self, response) -> List[Dict[str, Any]]:
        """ãƒ•ã‚¡ã‚¤ãƒ«å¼•ç”¨æƒ…å ±ã‚’æŠ½å‡º"""
        citations: List[Dict[str, Any]] = []
        try:
            if hasattr(response, 'output') and response.output:
                for item in response.output:
                    if hasattr(item, 'type') and item.type == "message":
                        if hasattr(item, 'content') and item.content:
                            for content in item.content:
                                if hasattr(content, 'annotations'):
                                    for annotation in content.annotations:
                                        if hasattr(annotation, 'type') and annotation.type == "file_citation":
                                            citations.append({
                                                "file_id" : getattr(annotation, 'file_id', ''),
                                                "filename": getattr(annotation, 'filename', ''),
                                                "index"   : getattr(annotation, 'index', 0)
                                            })
        except Exception as e:
            logger.error(f"å¼•ç”¨æƒ…å ±æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")

        return citations

    def _extract_tool_calls(self, response) -> List[Dict[str, Any]]:
        """ãƒ„ãƒ¼ãƒ«å‘¼ã³å‡ºã—æƒ…å ±ã‚’æŠ½å‡º"""
        tool_calls: List[Dict[str, Any]] = []
        try:
            if hasattr(response, 'output') and response.output:
                for item in response.output:
                    if hasattr(item, 'type') and item.type == "file_search_call":
                        tool_calls.append({
                            "id"     : getattr(item, 'id', ''),
                            "type"   : "file_search",
                            "status" : getattr(item, 'status', ''),
                            "queries": getattr(item, 'queries', [])
                        })
        except Exception as e:
            logger.error(f"ãƒ„ãƒ¼ãƒ«å‘¼ã³å‡ºã—æƒ…å ±æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")

        return tool_calls


# ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
@st.cache_resource
def get_rag_manager():
    """RAGãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ã®ã‚·ãƒ³ã‚°ãƒ«ãƒˆãƒ³å–å¾—"""
    return ModernRAGManager()


def initialize_session_state():
    """ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã®åˆæœŸåŒ–"""
    if 'search_history' not in st.session_state:
        st.session_state.search_history = []
    if 'current_query' not in st.session_state:
        st.session_state.current_query = ""
    if 'selected_store' not in st.session_state:
        # å‹•çš„ã«æœ€åˆã®Vector Storeã‚’é¸æŠ
        _, store_list = get_current_vector_stores()
        st.session_state.selected_store = store_list[0] if store_list else "Customer Support FAQ"
    if 'selected_model' not in st.session_state:
        st.session_state.selected_model = AppConfig.DEFAULT_MODEL if HELPER_AVAILABLE else "gpt-4o-mini"  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ¢ãƒ‡ãƒ«
    if 'use_agent_sdk' not in st.session_state:
        st.session_state.use_agent_sdk = False  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯Responses APIç›´æ¥ä½¿ç”¨
    if 'search_options' not in st.session_state:
        st.session_state.search_options = {
            'max_results'    : 20,
            'include_results': True,
            'show_citations' : True
        }
    if 'auto_refresh_stores' not in st.session_state:
        st.session_state.auto_refresh_stores = True


def display_search_history():
    """æ¤œç´¢å±¥æ­´ã®è¡¨ç¤º"""
    st.header("ğŸ•’ æ¤œç´¢å±¥æ­´")

    if not st.session_state.search_history:
        st.info("æ¤œç´¢å±¥æ­´ãŒã‚ã‚Šã¾ã›ã‚“")
        return

    # å±¥æ­´ã‚’ã‚¨ã‚¯ã‚¹ãƒ‘ãƒ³ãƒ€ãƒ¼ã§è¡¨ç¤º
    for i, item in enumerate(st.session_state.search_history[:10]):  # æœ€æ–°10ä»¶
        with st.expander(f"å±¥æ­´ {i + 1}: {item['query'][:50]}..."):
            st.markdown(f"**è³ªå•:** {item['query']}")
            st.markdown(f"**Vector Store:** {item['store_name']}")
            st.markdown(f"**Store ID:** `{item.get('store_id', 'N/A')}`")
            st.markdown(f"**å®Ÿè¡Œæ™‚é–“:** {item['timestamp']}")
            st.markdown(f"**æ¤œç´¢æ–¹æ³•:** {item.get('method', 'unknown')}")

            # å¼•ç”¨æƒ…å ±è¡¨ç¤º
            if 'citations' in item and item['citations']:
                st.markdown("**å¼•ç”¨ãƒ•ã‚¡ã‚¤ãƒ«:**")
                for citation in item['citations']:
                    st.markdown(f"- {citation.get('filename', 'Unknown file')}")

            col1, col2 = st.columns(2)
            with col1:
                if st.button("å†å®Ÿè¡Œ", key=f"rerun_{i}"):
                    st.session_state.current_query = item['query']
                    st.session_state.selected_store = item['store_name']
                    st.rerun()
            with col2:
                if st.button("è©³ç´°è¡¨ç¤º", key=f"detail_{i}"):
                    st.json(item)


def get_selected_store_index(selected_store: str, store_list: List[str]) -> int:
    """é¸æŠã•ã‚ŒãŸVector Storeã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’å–å¾—"""
    try:
        return store_list.index(selected_store)
    except ValueError:
        return 0  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯æœ€åˆã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹


def get_test_questions_by_store(store_name: str) -> List[str]:
    """Vector Storeã«å¿œã˜ãŸãƒ†ã‚¹ãƒˆè³ªå•ã‚’å–å¾—ï¼ˆè‹±èªã®ã¿ï¼‰"""
    # å‹•çš„ãªVector Storeã«å¯¾å¿œã™ã‚‹ãŸã‚ã®ãƒãƒƒãƒ”ãƒ³ã‚°ï¼ˆè‹±èªã®ã¿ï¼‰
    store_question_mapping = {
        "Customer Support FAQ"    : test_questions_en,
        "Science & Technology Q&A": test_questions_2_en,
        "Medical Q&A"             : test_questions_3_en,
        "Legal Q&A"               : test_questions_4_en,
        "Unified Knowledge Base"  : test_questions_unified_en,  # çµ±åˆãƒŠãƒ¬ãƒƒã‚¸ãƒ™ãƒ¼ã‚¹
        "CC News Q&A (LLM)"       : test_questions_cc_news_en,  # CC News LLMç”Ÿæˆ
        "CC News Q&A (Coverage)"  : test_questions_cc_news_en,  # CC Newsã‚«ãƒãƒ¬ãƒƒã‚¸æ”¹è‰¯
        "CC News Q&A (Hybrid)"    : test_questions_cc_news_en,  # CC Newsãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰
    }

    # å®Œå…¨ä¸€è‡´ç¢ºèª
    if store_name in store_question_mapping:
        return store_question_mapping[store_name]

    # éƒ¨åˆ†ä¸€è‡´ç¢ºèªï¼ˆæŸ”è»Ÿå¯¾å¿œï¼‰
    for mapped_store, questions in store_question_mapping.items():
        if (mapped_store.lower() in store_name.lower() or
                any(word in store_name.lower() for word in mapped_store.lower().split())):
            return questions

    # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼ˆCustomer Support FAQï¼‰
    return test_questions_en


def display_test_questions():
    """ãƒ†ã‚¹ãƒˆç”¨è³ªå•ã®è¡¨ç¤ºï¼ˆå‹•çš„Vector Storeå¯¾å¿œï¼‰"""
    # ç¾åœ¨é¸æŠã•ã‚Œã¦ã„ã‚‹Vector Storeã‚’å–å¾—
    selected_store = st.session_state.get('selected_store', 'Customer Support FAQ')

    # å¯¾å¿œã™ã‚‹è³ªå•ã‚’å–å¾—
    questions = get_test_questions_by_store(selected_store)

    # ãƒ˜ãƒƒãƒ€ãƒ¼ã®å‹•çš„ç”Ÿæˆ
    header = f"Test Questions ({selected_store})"
    st.header(f"ğŸ’¡ {header}")

    # RAGãƒ‡ãƒ¼ã‚¿ãŒè‹±èªã®å ´åˆã®æ³¨æ„æ›¸ã
    st.success("âœ… è‹±èªè³ªå•ï¼ˆRAGãƒ‡ãƒ¼ã‚¿ã«æœ€é©åŒ–ï¼‰")

    if not questions:
        st.info("No test questions available for this Vector Store")
        return

    # è³ªå•ãƒœã‚¿ãƒ³ã®è¡¨ç¤º
    for i, question in enumerate(questions):
        button_key = f"test_q_{selected_store}_{i}_{hash(question)}"
        if st.button(f"Q{i + 1}: {question}", key=button_key):
            st.session_state.current_query = question
            st.session_state.selected_store = selected_store
            st.rerun()


def display_vector_store_management():
    """Vector Storeç®¡ç†UIï¼ˆé‡è¤‡å•é¡Œä¿®æ­£ç‰ˆï¼‰"""
    st.header("ğŸ—„ï¸ Vector Storeç®¡ç†ï¼ˆæœ€æ–°IDå„ªå…ˆï¼‰")

    manager = get_vector_store_manager()

    col1, col2 = st.columns(2)
    with col1:
        st.write("**ç¾åœ¨ã®è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«**")
        if manager.CONFIG_FILE_PATH.exists():
            file_stat = manager.CONFIG_FILE_PATH.stat()
            st.success(f"âœ… å­˜åœ¨ ({file_stat.st_size} bytes)")
            modified_time = datetime.fromtimestamp(file_stat.st_mtime)
            st.write(f"æœ€çµ‚æ›´æ–°: {modified_time.strftime('%Y-%m-%d %H:%M:%S')}")
        else:
            st.warning("âš ï¸ è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«æœªä½œæˆ")

    with col2:
        st.write("**æ“ä½œ**")
        if st.button("ğŸ”„ æœ€æ–°æƒ…å ±ã«æ›´æ–°", type="primary"):
            with st.spinner("æœ€æ–°ã®Vector Storeæƒ…å ±ã‚’å–å¾—ä¸­..."):
                updated_stores = manager.refresh_and_save()
                st.session_state['vector_stores_updated'] = datetime.now().isoformat()
                # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒªã‚¢
                st.cache_resource.clear()
                st.rerun()

        if st.button("ğŸ“Š ãƒ‡ãƒãƒƒã‚°æƒ…å ±è¡¨ç¤º"):
            debug_info = manager.debug_vector_stores()
            with st.expander("ğŸ” ãƒ‡ãƒãƒƒã‚°æƒ…å ±", expanded=True):
                st.json(debug_info)

        if st.button("ğŸ“ è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«è¡¨ç¤º"):
            if manager.CONFIG_FILE_PATH.exists():
                try:
                    with open(manager.CONFIG_FILE_PATH, 'r', encoding='utf-8') as f:
                        config_content = f.read()
                    st.code(config_content, language='json')
                except Exception as e:
                    st.error(f"ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            else:
                st.warning("è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ã¾ã›ã‚“")


def display_system_info():
    """ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±ã®è¡¨ç¤º"""
    with st.expander("ğŸ”§ ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±", expanded=False):
        st.write("**åˆ©ç”¨å¯èƒ½ãªæ©Ÿèƒ½:**")
        st.write(f"- OpenAI SDK: {'âœ…' if OPENAI_AVAILABLE else 'âŒ'}")
        st.write(f"- Responses API: âœ…")
        st.write(f"- file_search ãƒ„ãƒ¼ãƒ«: âœ…")
        st.write(f"- Agent SDK: {'âœ…ï¼ˆç°¡æ˜“ç‰ˆï¼‰' if AGENT_SDK_AVAILABLE else 'âŒ'}")
        st.write(f"- Vector Store RAG: âœ…")
        st.write(f"- ãƒ•ã‚¡ã‚¤ãƒ«å¼•ç”¨: âœ…")
        st.write(f"- æ¤œç´¢çµæœè©³ç´°: âœ…")
        st.write(f"- å‹å®‰å…¨å®Ÿè£…: âœ…")
        st.write(f"- ç’°å¢ƒå¤‰æ•°APIã‚­ãƒ¼: âœ…")
        st.write(f"- å‹•çš„Vector Storeç®¡ç†: âœ…")
        st.write(f"- é‡è¤‡IDè§£æ±º: âœ…ï¼ˆæœ€æ–°å„ªå…ˆï¼‰")

        st.write("**APIã‚­ãƒ¼è¨­å®š:**")
        st.write("- ç’°å¢ƒå¤‰æ•° `OPENAI_API_KEY` ã‹ã‚‰è‡ªå‹•å–å¾—")
        st.write("- Streamlit secrets.toml ä¸è¦")
        st.code("export OPENAI_API_KEY='your-api-key-here'")

        # å‹•çš„Vector Storeæƒ…å ±
        st.write("**Vector Storesï¼ˆå‹•çš„ãƒ»æœ€æ–°å„ªå…ˆï¼‰:**")
        stores, _ = get_current_vector_stores()
        for i, (name, store_id) in enumerate(stores.items(), 1):
            st.write(f"{i}. {name}: `{store_id}`")

        if st.session_state.search_history:
            st.write(f"**æ¤œç´¢å±¥æ­´:** {len(st.session_state.search_history)} ä»¶")

        # Vector Storeé€£å‹•æƒ…å ±
        st.write("**è¨­å®šæƒ…å ±:**")
        selected_store = st.session_state.get('selected_store', 'Customer Support FAQ')
        selected_model = st.session_state.get('selected_model', 'gpt-4o-mini')

        st.write(f"- é¸æŠVector Store: {selected_store}")
        st.write(f"- é¸æŠãƒ¢ãƒ‡ãƒ«: {selected_model}")
        st.write(f"- Agent SDKä½¿ç”¨: {'æœ‰åŠ¹' if st.session_state.get('use_agent_sdk', False) else 'ç„¡åŠ¹'}")
        st.write(f"- è‡ªå‹•æ›´æ–°: {'æœ‰åŠ¹' if st.session_state.get('auto_refresh_stores', True) else 'ç„¡åŠ¹'}")


def display_search_options():
    """æ¤œç´¢ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã®è¡¨ç¤º"""
    with st.expander("âš™ï¸ æ¤œç´¢ã‚ªãƒ—ã‚·ãƒ§ãƒ³", expanded=False):
        # æœ€å¤§çµæœæ•°
        max_results = st.slider(
            "æœ€å¤§æ¤œç´¢çµæœæ•°",
            min_value=1,
            max_value=50,
            value=st.session_state.search_options['max_results'],
            help="Vector Storeã‹ã‚‰å–å¾—ã™ã‚‹æœ€å¤§çµæœæ•°"
        )
        st.session_state.search_options['max_results'] = max_results

        # æ¤œç´¢çµæœè©³ç´°ã‚’å«ã‚ã‚‹
        include_results = st.checkbox(
            "æ¤œç´¢çµæœè©³ç´°ã‚’å«ã‚ã‚‹",
            value=st.session_state.search_options['include_results'],
            help="file_search_call.resultsã‚’å«ã‚ã‚‹"
        )
        st.session_state.search_options['include_results'] = include_results

        # å¼•ç”¨è¡¨ç¤º
        show_citations = st.checkbox(
            "ãƒ•ã‚¡ã‚¤ãƒ«å¼•ç”¨ã‚’è¡¨ç¤º",
            value=st.session_state.search_options['show_citations'],
            help="ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã«ãƒ•ã‚¡ã‚¤ãƒ«å¼•ç”¨æƒ…å ±ã‚’è¡¨ç¤º"
        )
        st.session_state.search_options['show_citations'] = show_citations

        # Agent SDKä½¿ç”¨è¨­å®š
        if AGENT_SDK_AVAILABLE:
            use_agent_sdk = st.checkbox(
                "Agent SDKã‚’ä½¿ç”¨",
                value=st.session_state.use_agent_sdk,
                help="Agent SDKã‚’ä½¿ç”¨ã—ã¦ã‚»ãƒƒã‚·ãƒ§ãƒ³ç®¡ç†ã‚’æœ‰åŠ¹åŒ–"
            )
            st.session_state.use_agent_sdk = use_agent_sdk

        # Vector Storeè‡ªå‹•æ›´æ–°è¨­å®š
        auto_refresh = st.checkbox(
            "Vector Storeè‡ªå‹•æ›´æ–°",
            value=st.session_state.auto_refresh_stores,
            help="èµ·å‹•æ™‚ã«OpenAI APIã‹ã‚‰æœ€æ–°ã®Vector Storeæƒ…å ±ã‚’å–å¾—"
        )
        st.session_state.auto_refresh_stores = auto_refresh


def generate_enhanced_response(query: str, search_result: str, has_result: bool = True) -> Tuple[str, Dict[str, Any]]:
    """æ¤œç´¢çµæœã‚’åŸºã«ã€ã‚ˆã‚Šè‡ªç„¶ãªæ—¥æœ¬èªå›ç­”ã‚’ç”Ÿæˆ"""
    try:
        # ãƒ¢ãƒ‡ãƒ«é¸æŠ
        selected_model = st.session_state.get('selected_model', 'gpt-4o-mini')
        
        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®æ§‹æˆ
        if has_result and search_result and search_result.strip():
            # æ¤œç´¢çµæœãŒã‚ã‚‹å ´åˆ
            system_prompt = """ã‚ãªãŸã¯è¦ªåˆ‡ãªã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚æä¾›ã•ã‚ŒãŸæ¤œç´¢çµæœã‚’åŸºã«ã€
ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã«å¯¾ã—ã¦æ­£ç¢ºã§åˆ†ã‹ã‚Šã‚„ã™ã„æ—¥æœ¬èªã®å›ç­”ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚
æ¤œç´¢çµæœã‹ã‚‰é–¢é€£ã™ã‚‹æƒ…å ±ã‚’æŠ½å‡ºã—ã€è‡ªç„¶ãªæ—¥æœ¬èªã§èª¬æ˜ã—ã¦ãã ã•ã„ã€‚"""
            
            user_prompt = f"""ä»¥ä¸‹ã®æ¤œç´¢çµæœã‚’å‚è€ƒã«ã—ã¦ã€è³ªå•ã«æ—¥æœ¬èªã§å›ç­”ã—ã¦ãã ã•ã„ã€‚

ã€æ¤œç´¢çµæœã€‘
{search_result}

ã€è³ªå•ã€‘
{query}

ã“ã®æ¤œç´¢çµæœã‹ã‚‰å–ã‚Šå‡ºã—ã¦ã€æ—¥æœ¬èªã§å›ç­”ã—ã¦ãã ã•ã„ã€‚"""
        else:
            # æ¤œç´¢çµæœãŒãªã„å ´åˆ
            system_prompt = """ã‚ãªãŸã¯è¦ªåˆ‡ãªã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚
ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã«å¯¾ã—ã¦ã€ã‚ãªãŸã®çŸ¥è­˜ã‚’åŸºã«æ­£ç¢ºã§åˆ†ã‹ã‚Šã‚„ã™ã„æ—¥æœ¬èªã®å›ç­”ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚"""
            
            user_prompt = f"""Vector Storeã‹ã‚‰ã®æ¤œç´¢çµæœãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚
ä¸€èˆ¬çš„ãªçŸ¥è­˜ã‚’åŸºã«ã€ä»¥ä¸‹ã®è³ªå•ã«æ—¥æœ¬èªã§å›ç­”ã—ã¦ãã ã•ã„ã€‚

ã€è³ªå•ã€‘
{query}"""

        # ChatCompletion APIã‚’å‘¼ã³å‡ºã—
        # ãƒ¢ãƒ‡ãƒ«ã«å¿œã˜ã¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’èª¿æ•´
        model_lower = selected_model.lower()

        # åŸºæœ¬ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        completion_params = {
            "model": selected_model,
            "messages": [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ]
        }

        # o-seriesãƒ¢ãƒ‡ãƒ«ã¨ gpt-5-mini ã¯ temperature ã‚’ã‚µãƒãƒ¼ãƒˆã—ãªã„ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ1.0ã®ã¿ï¼‰
        no_temperature_models = any(prefix in model_lower for prefix in ['o1-', 'o3-', 'o4-', 'gpt-5-mini'])
        if not no_temperature_models:
            completion_params["temperature"] = 0.7

        # gpt-4.1, gpt-5, gpt-5-mini, o-seriesãƒ¢ãƒ‡ãƒ«ã¯ max_completion_tokens ã‚’ä½¿ç”¨
        if any(prefix in model_lower for prefix in ['gpt-5-mini', 'gpt-4.1', 'gpt-5', 'o1-', 'o3-', 'o4-']):
            completion_params["max_completion_tokens"] = 2000
        else:
            completion_params["max_tokens"] = 2000

        response = openai_client.chat.completions.create(**completion_params)
        
        # å›ç­”ã®æŠ½å‡º
        enhanced_response = response.choices[0].message.content
        
        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®æ§‹ç¯‰
        metadata = {
            "model": selected_model,
            "has_search_result": has_result,
            "timestamp": datetime.now().isoformat(),
            "usage": {
                "prompt_tokens": response.usage.prompt_tokens if response.usage else 0,
                "completion_tokens": response.usage.completion_tokens if response.usage else 0,
                "total_tokens": response.usage.total_tokens if response.usage else 0
            }
        }
        
        return enhanced_response, metadata
        
    except Exception as e:
        error_msg = f"å›ç­”ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {str(e)}"
        logger.error(error_msg)
        logger.error(traceback.format_exc())
        return error_msg, {"error": str(e), "timestamp": datetime.now().isoformat()}


def display_search_results(response_text: str, metadata: Dict[str, Any], original_query: str):
    """æ¤œç´¢çµæœã®è¡¨ç¤ºï¼ˆæ—¥æœ¬èªå›ç­”ç”Ÿæˆæ©Ÿèƒ½ä»˜ãï¼‰"""
    st.markdown("### ğŸ¤– å›ç­”")
    st.markdown(response_text)

    # ãƒ•ã‚¡ã‚¤ãƒ«å¼•ç”¨ã®è¡¨ç¤º
    if metadata.get('citations') and st.session_state.search_options['show_citations']:
        st.markdown("### ğŸ“š å¼•ç”¨ãƒ•ã‚¡ã‚¤ãƒ«")
        citations = metadata['citations']
        for i, citation in enumerate(citations, 1):
            st.markdown(f"{i}. **{citation.get('filename', 'Unknown file')}** (ID: `{citation.get('file_id', '')}`)")

    # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿è¡¨ç¤º
    st.markdown("---")
    st.markdown("### ğŸ“Š æ¤œç´¢æƒ…å ±")
    col1, col2 = st.columns(2)

    with col1:
        st.markdown(f"**Vector Store:** {metadata.get('store_name', '')}")
        st.markdown(f"**Store ID:** `{metadata.get('store_id', '')}`")
        st.markdown(f"**æ¤œç´¢æ–¹æ³•:** {metadata.get('method', '')}")

    with col2:
        st.markdown(f"**ãƒ¢ãƒ‡ãƒ«:** {metadata.get('model', '')}")
        st.markdown(f"**å®Ÿè¡Œæ™‚é–“:** {metadata.get('timestamp', '')}")
        if 'tool_calls' in metadata and metadata['tool_calls']:
            st.markdown(f"**ãƒ„ãƒ¼ãƒ«å‘¼ã³å‡ºã—:** {len(metadata['tool_calls'])}å›")

    # è©³ç´°æƒ…å ±
    with st.expander("ğŸ” è©³ç´°æƒ…å ±", expanded=False):
        st.json(metadata)
    
    # æ—¥æœ¬èªã§ã®è¿½åŠ å›ç­”ç”Ÿæˆ
    st.markdown("---")
    st.markdown("### ğŸ‡¯ğŸ‡µ æ—¥æœ¬èªå›ç­”ï¼ˆæ¤œç´¢çµæœã‚’åŸºã«ç”Ÿæˆï¼‰")
    
    with st.spinner("æ—¥æœ¬èªå›ç­”ã‚’ç”Ÿæˆä¸­..."):
        # æ¤œç´¢çµæœã®æœ‰ç„¡ã‚’åˆ¤å®š
        has_result = bool(response_text and 
                         response_text.strip() and 
                         "ã‚¨ãƒ©ãƒ¼" not in response_text and
                         "è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“" not in response_text)
        
        # æ—¥æœ¬èªå›ç­”ã‚’ç”Ÿæˆ
        enhanced_response, enhanced_metadata = generate_enhanced_response(
            original_query, 
            response_text,
            has_result
        )
        
        # æ—¥æœ¬èªå›ç­”ã‚’è¡¨ç¤º
        if not has_result:
            st.info("â„¹ï¸ Vector Storeã«é–¢é€£æƒ…å ±ãŒè¦‹ã¤ã‹ã‚‰ãªã‹ã£ãŸãŸã‚ã€ä¸€èˆ¬çš„ãªçŸ¥è­˜ã‹ã‚‰å›ç­”ã—ã¾ã™ã€‚")
        
        st.markdown(enhanced_response)
        
        # ç”Ÿæˆæƒ…å ±ã‚’è¡¨ç¤º
        with st.expander("ğŸ“Š æ—¥æœ¬èªå›ç­”ç”Ÿæˆæƒ…å ±", expanded=False):
            col1, col2 = st.columns(2)
            with col1:
                st.markdown(f"**ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«:** {enhanced_metadata.get('model', '')}")
                st.markdown(f"**æ¤œç´¢çµæœåˆ©ç”¨:** {'ã‚ã‚Š' if enhanced_metadata.get('has_search_result') else 'ãªã—'}")
            with col2:
                if 'usage' in enhanced_metadata:
                    usage = enhanced_metadata['usage']
                    st.markdown(f"**ãƒˆãƒ¼ã‚¯ãƒ³ä½¿ç”¨é‡:**")
                    st.markdown(f"- å…¥åŠ›: {usage.get('prompt_tokens', 0):,}")
                    st.markdown(f"- å‡ºåŠ›: {usage.get('completion_tokens', 0):,}")
                    st.markdown(f"- åˆè¨ˆ: {usage.get('total_tokens', 0):,}")


def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    st.set_page_config(
        page_title="æœ€æ–°RAGæ¤œç´¢ã‚¢ãƒ—ãƒªï¼ˆé‡è¤‡å•é¡Œä¿®æ­£ç‰ˆï¼‰",
        page_icon="ğŸ”",
        layout="wide"
    )

    # ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã®åˆæœŸåŒ–ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤è¨­å®šï¼‰
    initialize_session_state()

    # RAGãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ã®å–å¾—
    rag_manager = get_rag_manager()

    # Vector Storeè¨­å®šã®å–å¾—ï¼ˆå¼·åˆ¶ãƒªãƒ•ãƒ¬ãƒƒã‚·ãƒ¥ã¯åˆå›ã®ã¿ï¼‰
    force_refresh = st.session_state.get('force_initial_refresh', True)
    if force_refresh:
        st.session_state['force_initial_refresh'] = False

    vector_stores, vector_store_list = get_current_vector_stores(force_refresh=force_refresh)

    # ãƒ¡ã‚¤ãƒ³ã‚¿ã‚¤ãƒˆãƒ«ã¨ä½¿ã„æ–¹ã‚’æœ€ä¸Šéƒ¨ã«é…ç½®
    st.header("ğŸ” RAGæ¤œç´¢ï¼ˆCloud:OpenAI Vector Storeç‰ˆï¼‰")
    
    # ä½¿ã„æ–¹ã‚’Expanderã§è¡¨ç¤º
    with st.expander("ğŸ“– ä½¿ã„æ–¹", expanded=False):
        st.markdown("""
        **RAGæ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ ã®ä½¿ã„æ–¹:**
        
        1. **Vector Storeã®é¸æŠ**
           - å·¦ãƒšã‚¤ãƒ³ã§ä½¿ã„ãŸã„ Vector Store ã‚’é¸æŠã—ã¾ã™
           - å„Vector Storeã«ã¯ç•°ãªã‚‹ãƒ‰ãƒ¡ã‚¤ãƒ³ã®çŸ¥è­˜ãŒæ ¼ç´ã•ã‚Œã¦ã„ã¾ã™
        
        2. **è³ªå•ã®å…¥åŠ›æ–¹æ³•**
           - **æ–¹æ³•1**: å·¦ãƒšã‚¤ãƒ³ã®ãƒ†ã‚¹ãƒˆç”¨è³ªå•ã‹ã‚‰é¸æŠ
           - **æ–¹æ³•2**: ä¸‹ã®å…¥åŠ›æ¬„ã§ç›´æ¥è³ªå•ã‚’å…¥åŠ›
        
        3. **æ¤œç´¢ã®å®Ÿè¡Œ**
           - è³ªå•ã‚’å…¥åŠ›å¾Œã€ã€ŒğŸ” æ¤œç´¢å®Ÿè¡Œã€ãƒœã‚¿ãƒ³ã‚’æŠ¼ä¸‹
           - OpenAI Vector Storeã‹ã‚‰é–¢é€£æƒ…å ±ã‚’æ¤œç´¢ã—ã€å›ç­”ã‚’ç”Ÿæˆã—ã¾ã™
        
        **ğŸ’¡ ãƒ’ãƒ³ãƒˆ:**
        - RAGãƒ‡ãƒ¼ã‚¿ã¯è‹±èªã§ä½œæˆã•ã‚Œã¦ã„ã‚‹ãŸã‚ã€è‹±èªã§ã®è³ªå•ã‚’æ¨å¥¨
        - å„Vector Storeã«é©ã—ãŸè³ªå•å†…å®¹ã‚’é¸æŠã™ã‚‹ã¨ç²¾åº¦ãŒå‘ä¸Šã—ã¾ã™
        """)
    
    # ã‚µãƒ–ãƒ˜ãƒƒãƒ€ãƒ¼
    st.write("ğŸ” æœ€æ–°RAGæ¤œç´¢ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆé‡è¤‡å•é¡Œä¿®æ­£ãƒ»æœ€æ–°IDå„ªå…ˆç‰ˆï¼‰")

    # APIçŠ¶æ³è¡¨ç¤º
    col1, col2, col3 = st.columns(3)
    with col1:
        st.success("âœ… OpenAI Responses API åˆ©ç”¨å¯èƒ½")
        st.success("âœ… file_search ãƒ„ãƒ¼ãƒ«å¯¾å¿œ")
    with col2:
        if AGENT_SDK_AVAILABLE:
            st.success("âœ… Agent SDK åˆ©ç”¨å¯èƒ½")
        else:
            st.info("â„¹ï¸ Agent SDK æœªåˆ©ç”¨ï¼ˆResponses APIã®ã¿ï¼‰")
    with col3:
        st.success(f"âœ… å‹•çš„Vector Storeç®¡ç†")
        st.success(f"ğŸ”„ é‡è¤‡IDè§£æ±ºï¼ˆæœ€æ–°å„ªå…ˆï¼‰")
        st.info(f"ğŸ“Š åˆ©ç”¨å¯èƒ½åº—èˆ—: {len(vector_stores)}ä»¶")

    st.markdown("---")

    # ã‚µã‚¤ãƒ‰ãƒãƒ¼
    with st.sidebar:
        st.header("âš™ï¸ è¨­å®š")

        # Vector Storeé¸æŠï¼ˆå‹•çš„ï¼‰
        if vector_store_list:
            # ç¾åœ¨ã®é¸æŠãŒåˆ©ç”¨å¯èƒ½ã‹ãƒã‚§ãƒƒã‚¯
            current_selected = st.session_state.get('selected_store', vector_store_list[0])
            if current_selected not in vector_store_list:
                current_selected = vector_store_list[0]
                st.session_state.selected_store = current_selected

            selected_store = st.selectbox(
                "Vector Store ã‚’é¸æŠ",
                options=vector_store_list,
                index=vector_store_list.index(current_selected),
                key="store_selection"
            )
            st.session_state.selected_store = selected_store

            # é¸æŠã•ã‚ŒãŸVector Store IDã‚’è¡¨ç¤º
            selected_store_id = vector_stores.get(selected_store, "æœªçŸ¥ã®ID")
            st.code(selected_store_id)

            # IDæ›´æ–°çŠ¶æ³è¡¨ç¤º
            if st.session_state.get('vector_stores_updated'):
                update_time = st.session_state['vector_stores_updated']
                update_dt = datetime.fromisoformat(update_time)
                st.caption(f"æœ€çµ‚æ›´æ–°: {update_dt.strftime('%H:%M:%S')}")
        else:
            st.error("âŒ åˆ©ç”¨å¯èƒ½ãªVector StoreãŒã‚ã‚Šã¾ã›ã‚“")
            st.stop()

        # ãƒ¢ãƒ‡ãƒ«é¸æŠ
        st.markdown("---")
        st.subheader("ğŸ¤– ãƒ¢ãƒ‡ãƒ«é¸æŠ")
        
        if HELPER_AVAILABLE:
            # helper_ragã®select_modelé–¢æ•°ã‚’ã‚µã‚¤ãƒ‰ãƒãƒ¼å¤–ã§ä½¿ç”¨
            models = AppConfig.AVAILABLE_MODELS
            default_model = st.session_state.get('selected_model', AppConfig.DEFAULT_MODEL)
            
            try:
                default_index = models.index(default_model)
            except ValueError:
                default_index = 0
            
            selected_model = st.selectbox(
                "ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«",
                models,
                index=default_index,
                key="model_selection",
                help="RAGæ¤œç´¢ã«ä½¿ç”¨ã™ã‚‹OpenAIãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠã—ã¦ãã ã•ã„"
            )
            st.session_state.selected_model = selected_model
            
            # ãƒ¢ãƒ‡ãƒ«æƒ…å ±è¡¨ç¤º
            limits = AppConfig.get_model_limits(selected_model)
            pricing = AppConfig.get_model_pricing(selected_model)
            
            st.info(f"""
            ğŸ“„ **{selected_model}**
            - æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³: {limits['max_tokens']:,}
            - æœ€å¤§å‡ºåŠ›: {limits['max_output']:,}
            - æ–™é‡‘: ${pricing['input']:.4f} / ${pricing['output']:.4f} (input/output per 1K tokens)
            """)
        else:
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼šã‚·ãƒ³ãƒ—ãƒ«ãªãƒ¢ãƒ‡ãƒ«é¸æŠ
            models = ["gpt-4o", "gpt-4o-mini", "gpt-4.1", "gpt-4.1-mini"]
            selected_model = st.selectbox(
                "ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«",
                models,
                index=models.index(st.session_state.get('selected_model', 'gpt-4o-mini')),
                key="model_selection",
                help="RAGæ¤œç´¢ã«ä½¿ç”¨ã™ã‚‹OpenAIãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠã—ã¦ãã ã•ã„"
            )
            st.session_state.selected_model = selected_model

        # æ¤œç´¢ã‚ªãƒ—ã‚·ãƒ§ãƒ³
        display_search_options()

        # Vector Storeç®¡ç†
        st.markdown("---")
        with st.expander("ğŸ—„ï¸ Vector Storeç®¡ç†", expanded=False):
            display_vector_store_management()

        # ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±
        display_system_info()

        # ãƒ†ã‚¹ãƒˆç”¨è³ªå•ï¼ˆé¸æŠã•ã‚ŒãŸVector Storeã«å¯¾å¿œï¼‰
        with st.expander("ğŸ’¡ ãƒ†ã‚¹ãƒˆç”¨è³ªå•", expanded=True):
            display_test_questions()

    # è³ªå•å…¥åŠ›éƒ¨åˆ†ï¼ˆæ¨ª1æ®µæ§‹æˆï¼‰
    st.header("â“ è³ªå•å…¥åŠ›")

    # è³ªå•å…¥åŠ›ãƒ•ã‚©ãƒ¼ãƒ 
    with st.form("search_form"):
        query = st.text_area(
            "è³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„",
            value=st.session_state.current_query,
            height=100,
            key="query_input",
            help="è‹±èªã§ã®è³ªå•ãŒRAGãƒ‡ãƒ¼ã‚¿ã«æœ€é©åŒ–ã•ã‚Œã¦ã„ã¾ã™"
        )

        col_left, col_center, col_right = st.columns([1, 1, 1])
        with col_center:
            submitted = st.form_submit_button("ğŸ” æ¤œç´¢å®Ÿè¡Œ", type="primary", use_container_width=True)
    
    # è³ªå•ä¾‹ã‚’è¿½åŠ 
    with st.expander("ğŸ’¡ è³ªå•ä¾‹", expanded=False):
        st.markdown("**è³ªå•ä¾‹:**")
        example_questions = [
            "How can I create an account?",
            "æ–°è¦ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã¯ã€ã©ã†ä½œæˆã™ã‚Œã°ã‚ˆã„ã‹ï¼Ÿã€€æ—¥æœ¬èªã§å›ç­”ã—ã¦ãã ã•ã„ã€‚",
            "What are the symptoms of diabetes?",
            "What is quantum computing?"
        ]
        for example_question in example_questions:
            if st.button(f"ğŸ“ {example_question}", use_container_width=True, key=f"example_{hash(example_question)}"):
                st.session_state.current_query = example_question
                st.rerun()

    # æ¤œç´¢çµæœè¡¨ç¤ºéƒ¨åˆ†
    if submitted and query.strip():
        st.session_state.current_query = query
        
        st.markdown("---")
        st.header("ğŸ¤– æ¤œç´¢çµæœ")

        with st.spinner("ğŸ” Vector Storeæ¤œç´¢ä¸­..."):
            # é¸æŠã•ã‚ŒãŸVector Storeã®IDã‚’å–å¾—
            selected_store_id = vector_stores.get(selected_store, "")
            if not selected_store_id:
                st.error(f"âŒ Vector Store ID ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {selected_store}")
            else:
                # æ¤œç´¢ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã®å–å¾—
                search_options = st.session_state.search_options

                # æ¤œç´¢å®Ÿè¡Œï¼ˆstore_idã¨é¸æŠã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã‚’æ¸¡ã™ï¼‰
                final_result, final_metadata = rag_manager.search(
                    query,
                    selected_store,
                    selected_store_id,
                    use_agent_sdk=st.session_state.use_agent_sdk,
                    max_results=search_options['max_results'],
                    include_results=search_options['include_results'],
                    selected_model=st.session_state.selected_model  # é¸æŠã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã‚’æ¸¡ã™
                )

                # çµæœè¡¨ç¤ºï¼ˆå…ƒã®è³ªå•ã‚‚æ¸¡ã™ï¼‰
                display_search_results(final_result, final_metadata, query)

                # æ¤œç´¢å±¥æ­´ã«è¿½åŠ ï¼ˆå‹å®‰å…¨ï¼‰
                history_item: Dict[str, Any] = {
                    "query"         : query,
                    "store_name"    : selected_store,
                    "store_id"      : selected_store_id,
                    "timestamp"     : datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                    "method"        : final_metadata.get('method', 'unknown'),
                    "citations"     : final_metadata.get('citations', []),
                    "result_preview": final_result[:200] + "..." if len(final_result) > 200 else final_result
                }

                # é‡è¤‡ãƒã‚§ãƒƒã‚¯
                if not any(item['query'] == query and item['store_name'] == selected_store
                           for item in st.session_state.search_history):
                    st.session_state.search_history.insert(0, history_item)
                    st.session_state.search_history = st.session_state.search_history[:50]  # æœ€æ–°50ä»¶ä¿æŒ

    elif submitted and not query.strip():
        st.error("è³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„")

    # åˆæœŸçŠ¶æ…‹ã®è¡¨ç¤º
    if not st.session_state.current_query:
        st.info("è³ªå•ã‚’å…¥åŠ›ã—ã¦æ¤œç´¢ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„")

        # APIæ©Ÿèƒ½èª¬æ˜
        st.markdown("### ğŸš€ åˆ©ç”¨å¯èƒ½ãªæ©Ÿèƒ½")
        st.markdown("""
        - **æœ€æ–°Responses API**: OpenAIã®æœ€æ–°API
        - **file_search ãƒ„ãƒ¼ãƒ«**: Vector Storeã‹ã‚‰ã®é«˜ç²¾åº¦æ¤œç´¢
        - **å‹•çš„Vector Storeç®¡ç†**: è‡ªå‹•IDæ›´æ–°ãƒ»è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«é€£æº
        - **é‡è¤‡IDè§£æ±º**: åŒåVector Storeã®æœ€æ–°ä½œæˆæ—¥æ™‚å„ªå…ˆ
        - **ãƒ•ã‚¡ã‚¤ãƒ«å¼•ç”¨**: æ¤œç´¢çµæœã®å‡ºå…¸è¡¨ç¤º
        - **ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚ºå¯èƒ½**: çµæœæ•°ã€ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ç­‰
        - **Agent SDKé€£æº**: ã‚»ãƒƒã‚·ãƒ§ãƒ³ç®¡ç†ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
        - **å‹å®‰å…¨å®Ÿè£…**: å‹ã‚¨ãƒ©ãƒ¼ä¿®æ­£æ¸ˆã¿
        - **ç’°å¢ƒå¤‰æ•°APIã‚­ãƒ¼**: ã‚»ã‚­ãƒ¥ã‚¢ãªè¨­å®šæ–¹æ³•
        """)

        # é‡è¤‡å•é¡Œä¿®æ­£ã®èª¬æ˜
        with st.expander("ğŸ”„ é‡è¤‡IDå•é¡Œä¿®æ­£ã«ã¤ã„ã¦", expanded=False):
            st.markdown("""
            **ä¿®æ­£å†…å®¹: åŒåVector Storeã®é‡è¤‡å•é¡Œè§£æ±º**

            **å•é¡Œ:**
            - åŒã˜åå‰ã§è¤‡æ•°ã®Vector StoreãŒå­˜åœ¨
            - å¤ã„IDãŒé¸æŠã•ã‚Œã‚‹ãƒã‚°
            - ä½œæˆæ—¥æ™‚ã§ã®å„ªå…ˆåº¦ãŒæœªå®Ÿè£…

            **ä¿®æ­£:**
            - **ä½œæˆæ—¥æ™‚ã‚½ãƒ¼ãƒˆ**: Vector Storeä¸€è¦§ã‚’ä½œæˆæ—¥æ™‚é †ï¼ˆæ–°ã—ã„é †ï¼‰ã§ã‚½ãƒ¼ãƒˆ
            - **æœ€æ–°å„ªå…ˆé¸æŠ**: åŒåã®å ´åˆã¯`created_at`ãŒæœ€æ–°ã®ã‚‚ã®ã‚’å„ªå…ˆ
            - **è©³ç´°ãƒ­ã‚°å‡ºåŠ›**: ã©ã®IDãŒé¸æŠã•ã‚ŒãŸã‹ã‚’ãƒ­ã‚°ã§ç¢ºèªå¯èƒ½
            - **ãƒ‡ãƒãƒƒã‚°æ©Ÿèƒ½**: ã‚µã‚¤ãƒ‰ãƒãƒ¼ã§è©³ç´°æƒ…å ±ã‚’ç¢ºèªå¯èƒ½

            **é¸æŠãƒ­ã‚¸ãƒƒã‚¯:**
            1. OpenAI APIã‹ã‚‰Vector Storeä¸€è¦§ã‚’å–å¾—
            2. ä½œæˆæ—¥æ™‚(`created_at`)ã§é™é †ã‚½ãƒ¼ãƒˆ
            3. åŒåStoreå€™è£œã®ä¸­ã‹ã‚‰æœ€æ–°ã‚’é¸æŠ
            4. è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ãƒ»ã‚­ãƒ£ãƒƒã‚·ãƒ¥

            **ç¢ºèªæ–¹æ³•:**
            - ã‚µã‚¤ãƒ‰ãƒãƒ¼ã€ŒVector Storeç®¡ç†ã€â†’ã€Œãƒ‡ãƒãƒƒã‚°æƒ…å ±è¡¨ç¤ºã€
            - ãƒ­ã‚°ã§ã©ã®IDãŒé¸æŠã•ã‚ŒãŸã‹ã‚’ç¢ºèª
            """)

        # Vector Storeå‹•çš„ç®¡ç†ã®èª¬æ˜
        with st.expander("ğŸ—„ï¸ å‹•çš„Vector Storeç®¡ç†ã«ã¤ã„ã¦", expanded=False):
            st.markdown("""
            **æ–°æ©Ÿèƒ½: å‹•çš„Vector Storeç®¡ç†**

            - **è‡ªå‹•æ›´æ–°**: OpenAI APIã‹ã‚‰æœ€æ–°ã®Vector Storeä¸€è¦§ã‚’å–å¾—
            - **è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«é€£æº**: `vector_stores.json` ã§æ°¸ç¶šåŒ–
            - **a30_020_make_vsid.py é€£æº**: æ–°è¦ä½œæˆã•ã‚ŒãŸVector Storeã‚’è‡ªå‹•èªè­˜
            - **ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯**: è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ãŒãªã„å ´åˆã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’ä½¿ç”¨

            **è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼:**
            ```json
            {
              "vector_stores": {
                "Customer Support FAQ": "vs_xxx...",
                "Medical Q&A": "vs_yyy...",
                ...
              },
              "last_updated": "2025-01-XX...",
              "source": "a34_rag_search_cloud_vs.py",
              "version": "1.1"
            }
            ```

            **æ›´æ–°æ–¹æ³•:**
            1. ã‚µã‚¤ãƒ‰ãƒãƒ¼ã®ã€ŒVector Storeç®¡ç†ã€ã§ã€Œæœ€æ–°æƒ…å ±ã«æ›´æ–°ã€ã‚’ã‚¯ãƒªãƒƒã‚¯
            2. è‡ªå‹•ã§OpenAI APIã‹ã‚‰æœ€æ–°ä¸€è¦§ã‚’å–å¾—ï¼ˆé‡è¤‡è§£æ±ºæ¸ˆã¿ï¼‰
            3. è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ã—ã¦æ°¸ç¶šåŒ–
            """)

        # ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°
        with st.expander("ğŸš¨ ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°", expanded=False):
            st.markdown("""
            **é‡è¤‡IDå•é¡Œã®å ´åˆ:**
            - ã‚µã‚¤ãƒ‰ãƒãƒ¼ã€ŒVector Storeç®¡ç†ã€â†’ã€Œæœ€æ–°æƒ…å ±ã«æ›´æ–°ã€ã‚’ã‚¯ãƒªãƒƒã‚¯
            - ã€Œãƒ‡ãƒãƒƒã‚°æƒ…å ±è¡¨ç¤ºã€ã§é¸æŠã•ã‚ŒãŸIDã‚’ç¢ºèª
            - ãƒ­ã‚°ã§æœ€æ–°ä½œæˆæ—¥æ™‚ã®IDãŒé¸æŠã•ã‚Œã¦ã„ã‚‹ã‹ã‚’ç¢ºèª

            **APIã‚­ãƒ¼ã‚¨ãƒ©ãƒ¼ã®å ´åˆ:**
            ```bash
            # ç’°å¢ƒå¤‰æ•°ç¢ºèª
            echo $OPENAI_API_KEY

            # è¨­å®šæ–¹æ³•
            export OPENAI_API_KEY='your-api-key-here'

            # æ°¸ç¶šåŒ–ï¼ˆ.bashrc/.zshrcã«è¿½åŠ ï¼‰
            echo 'export OPENAI_API_KEY="your-api-key-here"' >> ~/.bashrc
            ```

            **Vector Storeé–¢é€£ã‚¨ãƒ©ãƒ¼:**
            - Vector Store IDãŒæ­£ã—ã„ã‹ç¢ºèª
            - ã€Œæœ€æ–°æƒ…å ±ã«æ›´æ–°ã€ãƒœã‚¿ãƒ³ã§å†å–å¾—
            - a30_020_make_vsid.py ã§æ–°è¦ä½œæˆå¾Œã¯æ›´æ–°ãŒå¿…è¦

            **ãã®ä»–ã®ã‚¨ãƒ©ãƒ¼:**
            - OpenAI SDKãŒæœ€æ–°ç‰ˆã‹ç¢ºèª: `pip install --upgrade openai`
            - ã‚¤ãƒ³ã‚¿ãƒ¼ãƒãƒƒãƒˆæ¥ç¶šã‚’ç¢ºèª
            - vector_stores.json ãƒ•ã‚¡ã‚¤ãƒ«ã®å½¢å¼ã‚’ç¢ºèª
            """)

    # æ¤œç´¢å±¥æ­´ã‚»ã‚¯ã‚·ãƒ§ãƒ³
    st.markdown("---")
    display_search_history()

    # ãƒ•ãƒƒã‚¿ãƒ¼
    st.markdown("---")
    st.markdown("#### æœ€æ–°RAGæ¤œç´¢ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆé‡è¤‡å•é¡Œä¿®æ­£ãƒ»æœ€æ–°IDå„ªå…ˆç‰ˆï¼‰")
    st.markdown("ğŸš€ **OpenAI Responses API + file_search ãƒ„ãƒ¼ãƒ«** ã«ã‚ˆã‚‹æ¬¡ä¸–ä»£RAG")
    st.markdown("âœ¨ **ä¿®æ­£æ©Ÿèƒ½**: é‡è¤‡Vector Store IDå•é¡Œè§£æ±ºã€æœ€æ–°ä½œæˆæ—¥æ™‚å„ªå…ˆ")
    st.markdown("ğŸ”— **a30_020_make_vsid.py é€£æº**: æ–°è¦Vector Storeè‡ªå‹•èªè­˜")
    st.markdown("ğŸ”‘ **ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£**: ç’°å¢ƒå¤‰æ•°ã§ã®APIã‚­ãƒ¼ç®¡ç†")
    if AGENT_SDK_AVAILABLE:
        st.markdown("ğŸ”§ **Agent SDK**: ã‚»ãƒƒã‚·ãƒ§ãƒ³ç®¡ç†ã‚µãƒãƒ¼ãƒˆï¼ˆç°¡æ˜“ç‰ˆï¼‰")
    else:
        st.markdown("âš¡ **é«˜æ€§èƒ½**: ç›´æ¥Responses APIä½¿ç”¨")


if __name__ == "__main__":
    main()

# streamlit run a34_rag_search_cloud_vs.py --server.port=8501
