#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
Qdrant ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ - ç°¡ç•¥ç‰ˆãƒ‡ãƒ¼ã‚¿æŠ•å…¥ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
ä½¿ç”¨æ–¹æ³•:
  python qdrant_data_loader.py --recreate --limit 100
"""

import os
import sys
import argparse
from pathlib import Path
from typing import List, Dict, Any
import pandas as pd
from datetime import datetime, timezone

try:
    import yaml
except ImportError:
    print("PyYAMLãŒå¿…è¦ã§ã™: pip install pyyaml")
    sys.exit(1)

from qdrant_client import QdrantClient
from qdrant_client.http import models
from openai import OpenAI

# è¨­å®šèª­ã¿è¾¼ã¿
def load_config(path: str = "config.yml") -> Dict[str, Any]:
    """config.yml ã‹ã‚‰è¨­å®šã‚’èª­ã¿è¾¼ã‚€"""
    defaults = {
        "rag": {"collection": "qa_corpus"},
        "embeddings": {
            "primary": {
                "provider": "openai", 
                "model": "text-embedding-3-small", 
                "dims": 1536
            }
        },
        "qdrant": {"url": "http://localhost:6333"},
    }
    
    if os.path.exists(path):
        with open(path, "r", encoding="utf-8") as f:
            cfg = yaml.safe_load(f) or {}
            # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¨ãƒãƒ¼ã‚¸
            for key in defaults:
                if key not in cfg:
                    cfg[key] = defaults[key]
                elif isinstance(defaults[key], dict):
                    for subkey in defaults[key]:
                        if subkey not in cfg[key]:
                            cfg[key][subkey] = defaults[key][subkey]
    else:
        cfg = defaults
    
    return cfg

def get_data_files() -> Dict[str, str]:
    """å‡¦ç†ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ã‚’å–å¾—"""
    base_path = Path("OUTPUT")
    
    # å›ºå®šãƒ•ã‚¡ã‚¤ãƒ«åã¾ãŸã¯ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒãƒ³ã‚°
    files = {
        "customer": "preprocessed_customer_support_faq_89rows_20250721_092004.csv",
        "medical": "preprocessed_medical_qa_19704rows_20250721_092658.csv",
        "legal": "preprocessed_legal_qa_4rows_20250721_100302.csv",
        "sciq": "preprocessed_sciq_qa_11679rows_20250721_095451.csv"
    }
    
    # å®Ÿéš›ã«å­˜åœ¨ã™ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç¢ºèª
    available_files = {}
    for domain, filename in files.items():
        filepath = base_path / filename
        if filepath.exists():
            available_files[domain] = str(filepath)
        else:
            # ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒãƒ³ã‚°ã§æ¢ã™
            pattern = f"preprocessed_{domain}*qa*.csv"
            matching = list(base_path.glob(pattern))
            if matching:
                available_files[domain] = str(matching[0])
    
    return available_files

def load_and_prepare_data(filepath: str, limit: int = 0) -> pd.DataFrame:
    """CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°"""
    df = pd.read_csv(filepath)
    
    # åˆ—åã‚’ãƒãƒƒãƒ”ãƒ³ã‚°
    column_mappings = {
        'Question': 'question',
        'Response': 'answer',
        'Answer': 'answer',
        'correct_answer': 'answer'
    }
    df = df.rename(columns=column_mappings)
    
    # å¿…è¦ãªåˆ—ãŒå­˜åœ¨ã™ã‚‹ã“ã¨ã‚’ç¢ºèª
    required_cols = ['question', 'answer']
    for col in required_cols:
        if col not in df.columns:
            # answerãŒãªã„å ´åˆã¯ç©ºæ–‡å­—ã§ä½œæˆ
            df[col] = ""
    
    # ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°
    df = df.fillna("")
    df = df[df['question'].str.len() > 0]  # ç©ºã®è³ªå•ã‚’é™¤å¤–
    
    if limit > 0:
        df = df.head(limit)
    
    return df

def create_embeddings(texts: List[str], model: str = "text-embedding-3-small") -> List[List[float]]:
    """OpenAI APIã‚’ä½¿ç”¨ã—ã¦åŸ‹ã‚è¾¼ã¿ã‚’ç”Ÿæˆ"""
    client = OpenAI()
    embeddings = []
    
    # ãƒãƒƒãƒå‡¦ç†
    batch_size = 100
    for i in range(0, len(texts), batch_size):
        batch = texts[i:i+batch_size]
        response = client.embeddings.create(model=model, input=batch)
        embeddings.extend([data.embedding for data in response.data])
    
    return embeddings

def setup_qdrant_collection(client: QdrantClient, collection_name: str, vector_size: int, recreate: bool = False):
    """Qdrantã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—"""
    if recreate:
        try:
            client.delete_collection(collection_name)
            print(f"æ—¢å­˜ã®ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ '{collection_name}' ã‚’å‰Šé™¤ã—ã¾ã—ãŸ")
        except:
            pass
    
    # ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ãŒå­˜åœ¨ã—ãªã„å ´åˆã¯ä½œæˆ
    try:
        client.get_collection(collection_name)
        print(f"ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ '{collection_name}' ã¯æ—¢ã«å­˜åœ¨ã—ã¾ã™")
    except:
        client.create_collection(
            collection_name=collection_name,
            vectors_config=models.VectorParams(
                size=vector_size,
                distance=models.Distance.COSINE
            )
        )
        print(f"ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ '{collection_name}' ã‚’ä½œæˆã—ã¾ã—ãŸ")
        
        # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ä½œæˆ
        client.create_payload_index(
            collection_name=collection_name,
            field_name="domain",
            field_type="keyword"
        )

def insert_data_to_qdrant(
    client: QdrantClient, 
    collection_name: str,
    df: pd.DataFrame,
    embeddings: List[List[float]],
    domain: str,
    offset: int = 0
):
    """ãƒ‡ãƒ¼ã‚¿ã‚’Qdrantã«æŠ•å…¥"""
    points = []
    timestamp = datetime.now(timezone.utc).isoformat()
    
    for idx, (_, row) in enumerate(df.iterrows()):
        point_id = offset + idx
        
        payload = {
            "domain": domain,
            "question": row.get("question", ""),
            "answer": row.get("answer", ""),
            "created_at": timestamp,
            "schema": "qa:v1"
        }
        
        points.append(models.PointStruct(
            id=point_id,
            vector=embeddings[idx],
            payload=payload
        ))
    
    # ãƒãƒƒãƒã§æŠ•å…¥
    batch_size = 100
    for i in range(0, len(points), batch_size):
        batch = points[i:i+batch_size]
        client.upsert(collection_name=collection_name, points=batch)
    
    print(f"  {domain}: {len(points)}ä»¶ã®ãƒ‡ãƒ¼ã‚¿ã‚’æŠ•å…¥ã—ã¾ã—ãŸ")

def main():
    parser = argparse.ArgumentParser(description="Qdrantã«ãƒ‡ãƒ¼ã‚¿ã‚’æŠ•å…¥")
    parser.add_argument("--recreate", action="store_true", help="ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã‚’å†ä½œæˆ")
    parser.add_argument("--limit", type=int, default=0, help="å„ãƒ‰ãƒ¡ã‚¤ãƒ³ã®æœ€å¤§ä»¶æ•°ï¼ˆ0=åˆ¶é™ãªã—ï¼‰")
    parser.add_argument("--collection", type=str, default=None, help="ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³å")
    parser.add_argument("--qdrant-url", type=str, default=None, help="Qdrant URL")
    args = parser.parse_args()
    
    # è¨­å®šèª­ã¿è¾¼ã¿
    config = load_config()
    collection_name = args.collection or config.get("rag", {}).get("collection", "qa_corpus")
    qdrant_url = args.qdrant_url or config.get("qdrant", {}).get("url", "http://localhost:6333")
    embedding_config = config.get("embeddings", {}).get("primary", {})
    embedding_model = embedding_config.get("model", "text-embedding-3-small")
    vector_size = embedding_config.get("dims", 1536)
    
    print(f"Qdrantãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼é–‹å§‹")
    print(f"  URL: {qdrant_url}")
    print(f"  ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³: {collection_name}")
    print(f"  åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«: {embedding_model}")
    
    # Qdrantã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåˆæœŸåŒ–
    try:
        client = QdrantClient(url=qdrant_url, timeout=30)
        # æ¥ç¶šãƒ†ã‚¹ãƒˆ
        client.get_collections()
        print("âœ… Qdrantã‚µãƒ¼ãƒãƒ¼ã«æ¥ç¶šæˆåŠŸ")
    except Exception as e:
        print(f"âŒ Qdrantã‚µãƒ¼ãƒãƒ¼ã«æ¥ç¶šã§ãã¾ã›ã‚“: {e}")
        print("Qdrantã‚µãƒ¼ãƒãƒ¼ã‚’èµ·å‹•ã—ã¦ãã ã•ã„:")
        print("  cd docker-compose && docker-compose up -d")
        return 1
    
    # ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—
    setup_qdrant_collection(client, collection_name, vector_size, args.recreate)
    
    # ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«å–å¾—
    data_files = get_data_files()
    if not data_files:
        print("âŒ å‡¦ç†ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        print("ã¾ãšä»¥ä¸‹ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã§ãƒ‡ãƒ¼ã‚¿ã‚’æº–å‚™ã—ã¦ãã ã•ã„:")
        print("  python a30_011_make_rag_data_customer.py")
        print("  python a30_013_make_rag_data_medical.py")
        return 1
    
    print(f"\nğŸ“Š {len(data_files)}å€‹ã®ãƒ‰ãƒ¡ã‚¤ãƒ³ãƒ‡ãƒ¼ã‚¿ã‚’å‡¦ç†ã—ã¾ã™")
    
    # å„ãƒ‰ãƒ¡ã‚¤ãƒ³ã®ãƒ‡ãƒ¼ã‚¿ã‚’å‡¦ç†
    total_points = 0
    point_offset = 0
    
    for domain, filepath in data_files.items():
        print(f"\nå‡¦ç†ä¸­: {domain}")
        print(f"  ãƒ•ã‚¡ã‚¤ãƒ«: {filepath}")
        
        # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
        df = load_and_prepare_data(filepath, args.limit)
        if len(df) == 0:
            print(f"  âš ï¸ æœ‰åŠ¹ãªãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
            continue
        
        # åŸ‹ã‚è¾¼ã¿ã‚’ä½œæˆ
        print(f"  åŸ‹ã‚è¾¼ã¿ç”Ÿæˆä¸­... ({len(df)}ä»¶)")
        texts = df['question'].tolist()
        embeddings = create_embeddings(texts, embedding_model)
        
        # Qdrantã«æŠ•å…¥
        insert_data_to_qdrant(
            client, 
            collection_name, 
            df, 
            embeddings, 
            domain,
            point_offset
        )
        
        point_offset += len(df)
        total_points += len(df)
    
    print(f"\nâœ… å®Œäº†ï¼åˆè¨ˆ {total_points} ä»¶ã®ãƒ‡ãƒ¼ã‚¿ã‚’æŠ•å…¥ã—ã¾ã—ãŸ")
    
    # çµ±è¨ˆæƒ…å ±ã‚’è¡¨ç¤º
    try:
        collection_info = client.get_collection(collection_name)
        print(f"\nã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³çµ±è¨ˆ:")
        print(f"  ç·ãƒã‚¤ãƒ³ãƒˆæ•°: {collection_info.points_count}")
        print(f"  ãƒ™ã‚¯ãƒˆãƒ«æ¬¡å…ƒ: {collection_info.config.params.vectors.size}")
    except:
        pass
    
    print("\næ¤œç´¢UIã‚’èµ·å‹•ã™ã‚‹ã«ã¯:")
    print("  streamlit run a50_rag_search_local_qdrant.py")
    
    return 0

if __name__ == "__main__":
    sys.exit(main())
