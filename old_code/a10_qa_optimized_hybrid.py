#!/usr/bin/env python3
"""
ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰Q&Aç”Ÿæˆã‚·ã‚¹ãƒ†ãƒ ï¼ˆæœ€é©åŒ–ç‰ˆï¼‰
ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹æŠ½å‡º + LLMå“è³ªå‘ä¸Š + ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯ã‚«ãƒãƒ¬ãƒ¼ã‚¸è¨ˆç®—

ä½¿ç”¨æ–¹æ³•:
    # åŸºæœ¬ä½¿ç”¨ï¼ˆcc_newsãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã€gpt-5-miniä½¿ç”¨ï¼‰
    python a10_qa_optimized_hybrid.py --dataset cc_news --output qa_output

    # ãƒ¢ãƒ‡ãƒ«æŒ‡å®š
    python a10_qa_optimized_hybrid.py --dataset cc_news --model gpt-4o-mini

    # LLMãªã—ï¼ˆãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹ã®ã¿ï¼‰
    python a10_qa_optimized_hybrid.py --dataset cc_news --no-llm

    # ã‚«ãƒãƒ¬ãƒ¼ã‚¸è¨ˆç®—ãªã—
    python a10_qa_optimized_hybrid.py --dataset cc_news --no-coverage

    # æ–‡æ›¸ã‚¿ã‚¤ãƒ—æŒ‡å®š
    python a10_qa_optimized_hybrid.py --dataset cc_news --doc-type news
"""

import os
import sys
import json
import argparse
import pandas as pd
from pathlib import Path
from typing import List, Dict, Optional
from datetime import datetime
import logging
from tqdm import tqdm

# helper_rag_qa ã‹ã‚‰æ–°ã—ã„ã‚¯ãƒ©ã‚¹ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from helper_rag_qa import OptimizedHybridQAGenerator

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# ==========================================
# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆè¨­å®š
# ==========================================

DATASET_CONFIGS = {
    "cc_news": {
        "name": "CC-Newsè‹±èªãƒ‹ãƒ¥ãƒ¼ã‚¹",
        "file": "OUTPUT/preprocessed_cc_news.csv",
        "text_column": "Combined_Text",
        "title_column": "title",
        "lang": "en",
        "default_doc_type": "news"
    },
    "japanese_text": {
        "name": "æ—¥æœ¬èªWebãƒ†ã‚­ã‚¹ãƒˆ",
        "file": "OUTPUT/preprocessed_japanese_text.csv",
        "text_column": "Combined_Text",
        "title_column": None,
        "lang": "ja",
        "default_doc_type": "auto"
    },
    "wikipedia_ja": {
        "name": "Wikipediaæ—¥æœ¬èªç‰ˆ",
        "file": "OUTPUT/preprocessed_wikipedia_ja.csv",
        "text_column": "Combined_Text",
        "title_column": "title",
        "lang": "ja",
        "default_doc_type": "academic"
    }
}

# ==========================================
# ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
# ==========================================

def load_preprocessed_data(dataset_type: str) -> pd.DataFrame:
    """preprocessedãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿"""
    config = DATASET_CONFIGS.get(dataset_type)
    if not config:
        raise ValueError(f"æœªå¯¾å¿œã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ: {dataset_type}")

    file_path = config["file"]
    if not Path(file_path).exists():
        raise FileNotFoundError(f"ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {file_path}")

    logger.info(f"ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ä¸­: {file_path}")
    df = pd.read_csv(file_path)

    # å¿…è¦ãªã‚«ãƒ©ãƒ ã®ç¢ºèª
    text_col = config["text_column"]
    if text_col not in df.columns:
        raise ValueError(f"ãƒ†ã‚­ã‚¹ãƒˆã‚«ãƒ©ãƒ  '{text_col}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")

    # ç©ºã®ãƒ†ã‚­ã‚¹ãƒˆã‚’é™¤å¤–
    df = df[df[text_col].notna() & (df[text_col].str.strip() != '')]

    logger.info(f"èª­ã¿è¾¼ã¿å®Œäº†: {len(df)}ä»¶ã®ãƒ‡ãƒ¼ã‚¿")
    return df

# ==========================================
# ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰Q/Aç”Ÿæˆå‡¦ç†
# ==========================================

def generate_hybrid_qa_from_dataset(
    df: pd.DataFrame,
    dataset_type: str,
    model: str = "gpt-5-mini",
    max_docs: Optional[int] = None,
    qa_count: Optional[int] = None,
    use_llm: bool = True,
    calculate_coverage: bool = True,
    doc_type: Optional[str] = None,
    output_dir: str = "qa_output"
) -> Dict:
    """ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‹ã‚‰ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰Q/Aç”Ÿæˆ

    Args:
        df: ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ 
        dataset_type: ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚¿ã‚¤ãƒ—
        model: ä½¿ç”¨ã™ã‚‹LLMãƒ¢ãƒ‡ãƒ«
        max_docs: å‡¦ç†ã™ã‚‹æœ€å¤§æ–‡æ›¸æ•°
        qa_count: ç”Ÿæˆã™ã‚‹Q/Aæ•°ï¼ˆNoneã§è‡ªå‹•æ±ºå®šï¼‰
        use_llm: LLMã‚’ä½¿ç”¨ã™ã‚‹ã‹
        calculate_coverage: ã‚«ãƒãƒ¬ãƒ¼ã‚¸è¨ˆç®—ã™ã‚‹ã‹
        doc_type: æ–‡æ›¸ã‚¿ã‚¤ãƒ—ï¼ˆNoneã®å ´åˆã¯ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼‰
        output_dir: å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª

    Returns:
        ç”Ÿæˆçµæœã®è¾æ›¸
    """
    config = DATASET_CONFIGS[dataset_type]
    text_col = config["text_column"]
    title_col = config.get("title_column")

    # æ–‡æ›¸ã‚¿ã‚¤ãƒ—ã®æ±ºå®š
    if doc_type is None:
        doc_type = config.get("default_doc_type", "auto")

    # å‡¦ç†ã™ã‚‹æ–‡æ›¸æ•°ã‚’åˆ¶é™
    docs_to_process = df.head(max_docs) if max_docs else df

    logger.info(f"ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰Q/Aç”Ÿæˆé–‹å§‹: {len(docs_to_process)}ä»¶ã®æ–‡æ›¸")
    logger.info(f"ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«: {model if use_llm else 'ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹ã®ã¿'}")

    # Q/Aç”Ÿæˆå™¨ã®åˆæœŸåŒ–
    generator = OptimizedHybridQAGenerator(model=model)

    all_results = []
    total_api_calls = 0
    total_tokens = 0
    total_cost = 0.0
    total_qa_generated = 0
    coverage_scores = []

    # ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼ä»˜ãã§å‡¦ç†
    for idx, row in tqdm(docs_to_process.iterrows(), total=len(docs_to_process), desc="Q/Aç”Ÿæˆ"):
        # ãƒ†ã‚­ã‚¹ãƒˆå–å¾—
        text = str(row[text_col]) if pd.notna(row[text_col]) else ""

        # ã‚¿ã‚¤ãƒˆãƒ«å–å¾—ï¼ˆã‚ã‚Œã°ï¼‰
        title = ""
        if title_col and title_col in row and pd.notna(row[title_col]):
            title = str(row[title_col])

        # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆIDä½œæˆ
        doc_id = f"{dataset_type}_{idx}"
        if title:
            doc_id += f"_{title[:30].replace(' ', '_')}"

        logger.debug(f"å‡¦ç†ä¸­: {doc_id}")

        # ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰Q/Aç”Ÿæˆ
        try:
            result = generator.generate_hybrid_qa(
                text=text,
                qa_count=qa_count,
                use_llm=use_llm,
                calculate_coverage=calculate_coverage,
                document_type=doc_type
            )

            # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿è¿½åŠ 
            result['doc_id'] = doc_id
            result['doc_idx'] = idx
            result['title'] = title
            result['text_length'] = len(text)

            # çµ±è¨ˆæ›´æ–°
            total_api_calls += result['api_usage']['calls']
            total_tokens += result['api_usage']['tokens']
            total_cost += result['api_usage']['cost']
            total_qa_generated += len(result['qa_pairs'])

            if calculate_coverage:
                coverage_scores.append(result['coverage'].get('coverage_percentage', 0))

            all_results.append(result)

        except Exception as e:
            logger.error(f"æ–‡æ›¸ {doc_id} ã®å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
            continue

    # å…¨ä½“ã®ã‚µãƒãƒªãƒ¼ä½œæˆ
    summary = {
        "dataset_type": dataset_type,
        "dataset_name": config["name"],
        "model_used": model if use_llm else "rule-based",
        "documents_processed": len(all_results),
        "total_qa_generated": total_qa_generated,
        "avg_qa_per_doc": total_qa_generated / len(all_results) if all_results else 0,
        "api_usage": {
            "total_calls": total_api_calls,
            "total_tokens": total_tokens,
            "total_cost": total_cost,
            "cost_per_doc": total_cost / len(all_results) if all_results else 0
        },
        "coverage": {
            "calculated": calculate_coverage,
            "avg_coverage": sum(coverage_scores) / len(coverage_scores) if coverage_scores else 0,
            "min_coverage": min(coverage_scores) if coverage_scores else 0,
            "max_coverage": max(coverage_scores) if coverage_scores else 0
        },
        "generation_timestamp": datetime.now().isoformat()
    }

    return {
        "summary": summary,
        "results": all_results
    }

# ==========================================
# çµæœä¿å­˜
# ==========================================

def save_results(
    generation_results: Dict,
    dataset_type: str,
    model: str,
    output_dir: str = "qa_output"
) -> Dict[str, str]:
    """çµæœã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜

    Args:
        generation_results: ç”Ÿæˆçµæœ
        dataset_type: ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚¿ã‚¤ãƒ—
        model: ä½¿ç”¨ã—ãŸãƒ¢ãƒ‡ãƒ«
        output_dir: å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª

    Returns:
        ä¿å­˜ã—ãŸãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
    """
    output_path = Path(output_dir)
    output_path.mkdir(exist_ok=True)

    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    model_suffix = model.replace("-", "_").replace(".", "_")

    # 1. ã‚µãƒãƒªãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«
    summary_file = output_path / f"hybrid_summary_{dataset_type}_{model_suffix}_{timestamp}.json"
    with open(summary_file, 'w', encoding='utf-8') as f:
        json.dump(generation_results['summary'], f, ensure_ascii=False, indent=2)

    # 2. è©³ç´°çµæœãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆJSONï¼‰
    details_file = output_path / f"hybrid_details_{dataset_type}_{model_suffix}_{timestamp}.json"
    with open(details_file, 'w', encoding='utf-8') as f:
        json.dump(generation_results['results'], f, ensure_ascii=False, indent=2)

    # 3. Q/Aãƒšã‚¢ï¼ˆCSVï¼‰
    qa_data = []
    for doc_result in generation_results['results']:
        doc_id = doc_result['doc_id']
        for qa in doc_result.get('qa_pairs', []):
            qa_data.append({
                'doc_id': doc_id,
                'question': qa.get('question', ''),
                'answer': qa.get('answer', ''),
                'doc_title': doc_result.get('title', ''),
                'text_length': doc_result.get('text_length', 0)
            })

    if qa_data:
        qa_df = pd.DataFrame(qa_data)
        qa_csv = output_path / f"hybrid_qa_pairs_{dataset_type}_{model_suffix}_{timestamp}.csv"
        qa_df.to_csv(qa_csv, index=False, encoding='utf-8')
    else:
        qa_csv = None

    # 4. ã‚«ãƒãƒ¬ãƒ¼ã‚¸ãƒ¬ãƒãƒ¼ãƒˆï¼ˆCSVï¼‰
    if generation_results['summary']['coverage']['calculated']:
        coverage_data = []
        for doc_result in generation_results['results']:
            if 'coverage' in doc_result:
                coverage_data.append({
                    'doc_id': doc_result['doc_id'],
                    'total_chunks': doc_result['coverage'].get('total_chunks', 0),
                    'covered_chunks': doc_result['coverage'].get('covered_chunks', 0),
                    'coverage_percentage': doc_result['coverage'].get('coverage_percentage', 0),
                    'average_similarity': doc_result['coverage'].get('average_similarity', 0)
                })

        if coverage_data:
            coverage_df = pd.DataFrame(coverage_data)
            coverage_csv = output_path / f"hybrid_coverage_{dataset_type}_{model_suffix}_{timestamp}.csv"
            coverage_df.to_csv(coverage_csv, index=False, encoding='utf-8')
        else:
            coverage_csv = None
    else:
        coverage_csv = None

    logger.info(f"çµæœã‚’ä¿å­˜ã—ã¾ã—ãŸ: {output_path}")

    return {
        "summary": str(summary_file),
        "details": str(details_file),
        "qa_pairs_csv": str(qa_csv) if qa_csv else None,
        "coverage_csv": str(coverage_csv) if coverage_csv else None
    }

# ==========================================
# ã‚³ã‚¹ãƒˆè¦‹ç©ã‚‚ã‚Š
# ==========================================

def estimate_cost(dataset_type: str, model: str, use_llm: bool = True) -> Dict:
    """å‡¦ç†ã‚³ã‚¹ãƒˆã®è¦‹ç©ã‚‚ã‚Š"""

    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚µã‚¤ã‚ºã®å–å¾—
    config = DATASET_CONFIGS[dataset_type]
    file_path = config["file"]

    if Path(file_path).exists():
        df = pd.read_csv(file_path)
        doc_count = len(df)
    else:
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
        doc_count = 497 if dataset_type == "cc_news" else 100

    # ãƒ¢ãƒ‡ãƒ«åˆ¥ã®æ–™é‡‘ï¼ˆ1Mãƒˆãƒ¼ã‚¯ãƒ³ã‚ãŸã‚Šã€ãƒ‰ãƒ«ï¼‰
    pricing = {
        "gpt-5-mini": {"input": 0.15, "output": 0.60},
        "gpt-5": {"input": 1.50, "output": 6.00},
        "gpt-4o-mini": {"input": 0.15, "output": 0.60},
        "gpt-4o": {"input": 2.50, "output": 10.00},
        "gpt-4": {"input": 30.00, "output": 60.00},
        "o1-mini": {"input": 3.00, "output": 12.00},
        "o1": {"input": 15.00, "output": 60.00},
        "o3-mini": {"input": 3.00, "output": 12.00}
    }

    if not use_llm:
        return {
            "document_count": doc_count,
            "model": "rule-based",
            "estimated_cost": 0.0,
            "api_calls": 0,
            "note": "ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹ã®ã¿ï¼ˆAPIä½¿ç”¨ãªã—ï¼‰"
        }

    model_pricing = pricing.get(model, pricing["gpt-5-mini"])

    # 1æ–‡æ›¸ã‚ãŸã‚Šã®æ¨å®šãƒˆãƒ¼ã‚¯ãƒ³æ•°
    avg_tokens_per_doc = 500  # å…¥åŠ›300 + å‡ºåŠ›200

    # LLMå‘¼ã³å‡ºã—ï¼ˆQ/Aç”Ÿæˆï¼‰
    llm_calls = doc_count
    llm_tokens = doc_count * avg_tokens_per_doc
    llm_cost = (llm_tokens * 0.7 * model_pricing["input"] +
                llm_tokens * 0.3 * model_pricing["output"]) / 1_000_000

    # åŸ‹ã‚è¾¼ã¿ç”Ÿæˆï¼ˆã‚«ãƒãƒ¬ãƒ¼ã‚¸è¨ˆç®—ç”¨ï¼‰
    embedding_calls = doc_count * 2  # ãƒãƒ£ãƒ³ã‚¯ + Q/A
    embedding_tokens = doc_count * 200  # ç°¡ç•¥åŒ–
    embedding_cost = embedding_tokens * 0.00002  # text-embedding-3-smallã®æ–™é‡‘

    total_cost = llm_cost + embedding_cost

    return {
        "document_count": doc_count,
        "model": model,
        "llm_calls": llm_calls,
        "embedding_calls": embedding_calls,
        "total_api_calls": llm_calls + embedding_calls,
        "estimated_tokens": llm_tokens + embedding_tokens,
        "llm_cost": round(llm_cost, 4),
        "embedding_cost": round(embedding_cost, 4),
        "estimated_total_cost": round(total_cost, 4),
        "cost_per_document": round(total_cost / doc_count, 6) if doc_count > 0 else 0
    }

# ==========================================
# ãƒ¡ã‚¤ãƒ³å‡¦ç†
# ==========================================

def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    parser = argparse.ArgumentParser(
        description="ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰Q&Aç”Ÿæˆã‚·ã‚¹ãƒ†ãƒ ï¼ˆLLM + ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹ + ã‚«ãƒãƒ¬ãƒ¼ã‚¸è¨ˆç®—ï¼‰"
    )
    parser.add_argument(
        "--dataset",
        type=str,
        choices=list(DATASET_CONFIGS.keys()),
        default="cc_news",
        help="å‡¦ç†ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ"
    )
    parser.add_argument(
        "--model",
        type=str,
        default="gpt-5-mini",
        help="ä½¿ç”¨ã™ã‚‹LLMãƒ¢ãƒ‡ãƒ«ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: gpt-5-miniï¼‰"
    )
    parser.add_argument(
        "--max-docs",
        type=int,
        default=None,
        help="å‡¦ç†ã™ã‚‹æœ€å¤§æ–‡æ›¸æ•°ï¼ˆãƒ†ã‚¹ãƒˆç”¨ï¼‰"
    )
    parser.add_argument(
        "--qa-count",
        type=int,
        default=None,
        help="æ–‡æ›¸ã‚ãŸã‚Šã®Q/Aæ•°ï¼ˆæœªæŒ‡å®šã®å ´åˆã¯è‡ªå‹•æ±ºå®šï¼‰"
    )
    parser.add_argument(
        "--doc-type",
        type=str,
        choices=["news", "technical", "academic", "auto"],
        default=None,
        help="æ–‡æ›¸ã‚¿ã‚¤ãƒ—ï¼ˆæœªæŒ‡å®šã®å ´åˆã¯ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼‰"
    )
    parser.add_argument(
        "--no-llm",
        action="store_true",
        help="LLMã‚’ä½¿ç”¨ã—ãªã„ï¼ˆãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹ã®ã¿ï¼‰"
    )
    parser.add_argument(
        "--no-coverage",
        action="store_true",
        help="ã‚«ãƒãƒ¬ãƒ¼ã‚¸è¨ˆç®—ã‚’è¡Œã‚ãªã„"
    )
    parser.add_argument(
        "--output",
        type=str,
        default="qa_output",
        help="å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª"
    )
    parser.add_argument(
        "--estimate-only",
        action="store_true",
        help="ã‚³ã‚¹ãƒˆè¦‹ç©ã‚‚ã‚Šã®ã¿å®Ÿè¡Œ"
    )

    args = parser.parse_args()

    # ã‚³ã‚¹ãƒˆè¦‹ç©ã‚‚ã‚Šãƒ¢ãƒ¼ãƒ‰
    if args.estimate_only:
        logger.info("ã‚³ã‚¹ãƒˆè¦‹ç©ã‚‚ã‚Šãƒ¢ãƒ¼ãƒ‰")
        estimate = estimate_cost(args.dataset, args.model, not args.no_llm)

        print("\n" + "=" * 80)
        print("ğŸ“Š å‡¦ç†ã‚³ã‚¹ãƒˆè¦‹ç©ã‚‚ã‚Š")
        print("=" * 80)
        print(f"ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ: {DATASET_CONFIGS[args.dataset]['name']}")
        print(f"æ–‡æ›¸æ•°: {estimate['document_count']}")
        print(f"ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«: {estimate['model']}")

        if not args.no_llm:
            print(f"\nAPIå‘¼ã³å‡ºã—:")
            print(f"  - LLMå‘¼ã³å‡ºã—: {estimate['llm_calls']}å›")
            print(f"  - åŸ‹ã‚è¾¼ã¿ç”Ÿæˆ: {estimate['embedding_calls']}å›")
            print(f"  - åˆè¨ˆ: {estimate['total_api_calls']}å›")

            print(f"\nã‚³ã‚¹ãƒˆå†…è¨³:")
            print(f"  - LLM: ${estimate['llm_cost']:.4f}")
            print(f"  - åŸ‹ã‚è¾¼ã¿: ${estimate['embedding_cost']:.4f}")
            print(f"  - åˆè¨ˆ: ${estimate['estimated_total_cost']:.4f}")
            print(f"  - 1æ–‡æ›¸ã‚ãŸã‚Š: ${estimate['cost_per_document']:.6f}")
        else:
            print(f"\nã‚³ã‚¹ãƒˆ: $0.00 ï¼ˆãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹ã®ã¿ï¼‰")

        print("=" * 80)
        return

    # é€šå¸¸ã®å‡¦ç†
    logger.info(f"""
    =====================================
    ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰Q&Aç”Ÿæˆé–‹å§‹
    =====================================
    ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ: {DATASET_CONFIGS[args.dataset]['name']}
    ãƒ¢ãƒ‡ãƒ«: {args.model if not args.no_llm else 'ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹ã®ã¿'}
    å‡ºåŠ›å…ˆ: {args.output}
    æœ€å¤§æ–‡æ›¸æ•°: {args.max_docs if args.max_docs else 'åˆ¶é™ãªã—'}
    Q/Aæ•°: {args.qa_count if args.qa_count else 'è‡ªå‹•æ±ºå®š'}
    LLMä½¿ç”¨: {'ã¯ã„' if not args.no_llm else 'ã„ã„ãˆ'}
    ã‚«ãƒãƒ¬ãƒ¼ã‚¸è¨ˆç®—: {'ã¯ã„' if not args.no_coverage else 'ã„ã„ãˆ'}
    æ–‡æ›¸ã‚¿ã‚¤ãƒ—: {args.doc_type if args.doc_type else 'ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ'}
    """)

    # OpenAI APIã‚­ãƒ¼ã®ç¢ºèª
    if not args.no_llm and not os.getenv('OPENAI_API_KEY'):
        logger.error("OpenAI APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚ç’°å¢ƒå¤‰æ•° OPENAI_API_KEY ã‚’è¨­å®šã—ã¦ãã ã•ã„ã€‚")
        sys.exit(1)

    try:
        # 1. ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
        logger.info("\n[1/3] ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿...")
        df = load_preprocessed_data(args.dataset)

        # 2. ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰Q/Aç”Ÿæˆ
        logger.info("\n[2/3] ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰Q/Aç”Ÿæˆ...")
        generation_results = generate_hybrid_qa_from_dataset(
            df,
            args.dataset,
            model=args.model,
            max_docs=args.max_docs,
            qa_count=args.qa_count,
            use_llm=not args.no_llm,
            calculate_coverage=not args.no_coverage,
            doc_type=args.doc_type,
            output_dir=args.output
        )

        # 3. çµæœä¿å­˜
        logger.info("\n[3/3] çµæœä¿å­˜...")
        saved_files = save_results(
            generation_results,
            args.dataset,
            args.model,
            args.output
        )

        # å®Œäº†ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
        summary = generation_results['summary']
        logger.info(f"""
        =====================================
        å‡¦ç†å®Œäº†
        =====================================
        å‡¦ç†æ–‡æ›¸æ•°: {summary['documents_processed']}
        ç”ŸæˆQ/Aç·æ•°: {summary['total_qa_generated']}
        å¹³å‡Q/Aæ•°/æ–‡æ›¸: {summary['avg_qa_per_doc']:.1f}

        APIä½¿ç”¨çŠ¶æ³:
        - ç·å‘¼ã³å‡ºã—å›æ•°: {summary['api_usage']['total_calls']}
        - ç·ãƒˆãƒ¼ã‚¯ãƒ³æ•°: {summary['api_usage']['total_tokens']}
        - ç·ã‚³ã‚¹ãƒˆ: ${summary['api_usage']['total_cost']:.4f}
        - æ–‡æ›¸ã‚ãŸã‚Šã‚³ã‚¹ãƒˆ: ${summary['api_usage']['cost_per_doc']:.6f}

        ã‚«ãƒãƒ¬ãƒ¼ã‚¸:
        - å¹³å‡ã‚«ãƒãƒ¬ãƒ¼ã‚¸: {summary['coverage']['avg_coverage']:.1f}%
        - æœ€å°: {summary['coverage']['min_coverage']:.1f}%
        - æœ€å¤§: {summary['coverage']['max_coverage']:.1f}%

        ä¿å­˜ãƒ•ã‚¡ã‚¤ãƒ«:
        - ã‚µãƒãƒªãƒ¼: {saved_files['summary']}
        - è©³ç´°: {saved_files['details']}
        - Q/A CSV: {saved_files['qa_pairs_csv']}
        - ã‚«ãƒãƒ¬ãƒ¼ã‚¸CSV: {saved_files['coverage_csv']}
        """)

        # æ”¹å–„åŠ¹æœã®èª¬æ˜
        print("\n" + "=" * 80)
        print("ğŸš€ ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã®åŠ¹æœ")
        print("=" * 80)
        print("""
ã“ã®ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ç‰ˆã§ã¯ã€ä»¥ä¸‹ã®æ”¹å–„ãŒå®Ÿç¾ã•ã‚Œã¾ã—ãŸï¼š

1. **å“è³ªå‘ä¸Š**ï¼ˆLLMä½¿ç”¨æ™‚ï¼‰
   - ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆãƒ™ãƒ¼ã‚¹ã‹ã‚‰è‡ªç„¶ãªè³ªå•æ–‡ã¸
   - æ–‡æ›¸ã‚¿ã‚¤ãƒ—ã«å¿œã˜ãŸé©åˆ‡ãªè³ªå•ç”Ÿæˆ
   - æ–‡è„ˆã‚’è€ƒæ…®ã—ãŸåŒ…æ‹¬çš„ãªå›ç­”

2. **ã‚«ãƒãƒ¬ãƒ¼ã‚¸æ¸¬å®š**
   - ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯ãªé¡ä¼¼åº¦ã«ã‚ˆã‚‹æ­£ç¢ºãªæ¸¬å®š
   - ãƒãƒ£ãƒ³ã‚¯ãƒ¬ãƒ™ãƒ«ã§ã®è©³ç´°ãªåˆ†æ
   - Q/Aãƒšã‚¢ã®ç¶²ç¾…æ€§ã®å®šé‡åŒ–

3. **ã‚³ã‚¹ãƒˆæœ€é©åŒ–**
   - ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹ã§å€™è£œã‚’çµã‚Šè¾¼ã¿
   - LLMã¯å“è³ªå‘ä¸Šã®ã¿ã«ä½¿ç”¨
   - å¿…è¦ã«å¿œã˜ã¦ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹ã®ã¿ã‚‚é¸æŠå¯èƒ½

4. **æŸ”è»Ÿæ€§**
   - è¤‡æ•°ã®LLMãƒ¢ãƒ‡ãƒ«ã‹ã‚‰é¸æŠå¯èƒ½
   - æ–‡æ›¸ã‚¿ã‚¤ãƒ—ã”ã¨ã®æœ€é©åŒ–
   - ã‚«ãƒãƒ¬ãƒ¼ã‚¸ç›®æ¨™ã®èª¿æ•´å¯èƒ½
""")

        if not args.no_llm:
            print(f"å®Ÿéš›ã®APIä½¿ç”¨ã‚³ã‚¹ãƒˆ: ${summary['api_usage']['total_cost']:.4f}")
        else:
            print("APIä½¿ç”¨ã‚³ã‚¹ãƒˆ: $0.00ï¼ˆãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹ã®ã¿ï¼‰")

    except Exception as e:
        logger.error(f"å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()