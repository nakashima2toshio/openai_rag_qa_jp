#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
a01_load_set_rag_data.py - çµ±åˆRAGãƒ‡ãƒ¼ã‚¿å‡¦ç†ãƒ„ãƒ¼ãƒ«
===============================================
èµ·å‹•: streamlit run a01_load_set_rag_data.py --server.port=8501

ã€ä¸»è¦æ©Ÿèƒ½ã€‘
âœ… 5ç¨®é¡ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚¿ã‚¤ãƒ—ã‚’çµ±åˆå‡¦ç†
   - ã‚«ã‚¹ã‚¿ãƒãƒ¼ã‚µãƒãƒ¼ãƒˆãƒ»FAQ
   - åŒ»ç™‚QAãƒ‡ãƒ¼ã‚¿
   - ç§‘å­¦ãƒ»æŠ€è¡“QAï¼ˆSciQï¼‰
   - æ³•å¾‹ãƒ»åˆ¤ä¾‹QA
   - é›‘å­¦QAï¼ˆTriviaQAï¼‰
âœ… ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼ãƒ»å“è³ªãƒã‚§ãƒƒã‚¯
âœ… RAGç”¨ãƒ†ã‚­ã‚¹ãƒˆçµåˆãƒ»å‰å‡¦ç†
âœ… ãƒˆãƒ¼ã‚¯ãƒ³ä½¿ç”¨é‡æ¨å®š
âœ… CSV/TXT/JSONãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå‡ºåŠ›

ã€å¯¾å¿œãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã€‘
1. customer_support_faq: ã‚«ã‚¹ã‚¿ãƒãƒ¼ã‚µãƒãƒ¼ãƒˆå‘ã‘FAQãƒ‡ãƒ¼ã‚¿
2. medical_qa: åŒ»ç™‚åˆ†é‡ã®Q&Aãƒ‡ãƒ¼ã‚¿ï¼ˆæ¨è«–éç¨‹ä»˜ãï¼‰
3. sciq_qa: ç§‘å­¦ãƒ»æŠ€è¡“åˆ†é‡ã®Q&Aãƒ‡ãƒ¼ã‚¿ï¼ˆé¸æŠè‚¢ä»˜ãï¼‰
4. legal_qa: æ³•å¾‹ãƒ»åˆ¤ä¾‹ã«é–¢ã™ã‚‹Q&Aãƒ‡ãƒ¼ã‚¿
5. trivia_qa: é›‘å­¦ã®QA
"""

import streamlit as st
import pandas as pd
import json
import io
from datetime import datetime
from pathlib import Path
import logging
from typing import Dict, List, Optional, Any

# ãƒ­ãƒ¼ã‚«ãƒ«ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
from helper_rag import (
    setup_page_config,
    setup_page_header,
    setup_sidebar_header,
    select_model,
    show_model_info,
    validate_data,
    load_dataset,
    process_rag_data,
    estimate_token_usage,
    create_download_data,
    display_statistics,
    save_files_to_output,
    show_usage_instructions,
    RAGConfig
)

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# ===================================================================
# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆåˆ¥æ¤œè¨¼é–¢æ•°
# ===================================================================

def validate_customer_support_data_specific(df: pd.DataFrame) -> List[str]:
    """ã‚«ã‚¹ã‚¿ãƒãƒ¼ã‚µãƒãƒ¼ãƒˆãƒ‡ãƒ¼ã‚¿å›ºæœ‰ã®æ¤œè¨¼"""
    issues = []
    
    # å¿…é ˆåˆ—ã®æ¤œè¨¼
    if 'question' in df.columns and 'answer' in df.columns:
        # ã‚µãƒãƒ¼ãƒˆé–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®ç¢ºèª
        support_keywords = ['å•é¡Œ', 'è§£æ±º', 'ãƒˆãƒ©ãƒ–ãƒ«', 'ã‚¨ãƒ©ãƒ¼', 'help', 'support']
        has_support_content = df['question'].str.contains('|'.join(support_keywords), na=False, case=False).any()
        if not has_support_content:
            issues.append("âš ï¸ ã‚µãƒãƒ¼ãƒˆé–¢é€£ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        
        # å›ç­”ã®é•·ã•ãƒã‚§ãƒƒã‚¯
        avg_answer_length = df['answer'].str.len().mean()
        if avg_answer_length < 50:
            issues.append(f"âš ï¸ å›ç­”ã®å¹³å‡æ–‡å­—æ•°ãŒçŸ­ã™ãã¾ã™ï¼ˆ{avg_answer_length:.0f}æ–‡å­—ï¼‰")
        
        # è³ªå•ã‚¿ã‚¤ãƒ—ã®åˆ†æ
        question_types = df['question'].str.contains('ï¼Ÿ|\\?', na=False).sum()
        st.info(f"ğŸ’¡ ç–‘å•å½¢å¼ã®è³ªå•: {question_types}/{len(df)} ({question_types/len(df)*100:.1f}%)")
    
    return issues

def validate_medical_data_specific(df: pd.DataFrame) -> List[str]:
    """åŒ»ç™‚QAãƒ‡ãƒ¼ã‚¿å›ºæœ‰ã®æ¤œè¨¼"""
    issues = []
    
    if 'Question' in df.columns and 'Response' in df.columns:
        # åŒ»ç™‚ç”¨èªã®ãƒã‚§ãƒƒã‚¯
        medical_keywords = ['ç—‡çŠ¶', 'è¨ºæ–­', 'æ²»ç™‚', 'è–¬', 'ç—…æ°—', 'disease', 'treatment', 'symptom']
        has_medical_content = df['Question'].str.contains('|'.join(medical_keywords), na=False, case=False).any()
        if not has_medical_content:
            issues.append("âš ï¸ åŒ»ç™‚é–¢é€£ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        
        # Complex_CoTã®å­˜åœ¨ç¢ºèª
        if 'Complex_CoT' in df.columns:
            cot_non_empty = df['Complex_CoT'].notna().sum()
            st.info(f"ğŸ’¡ æ¨è«–éç¨‹(Complex_CoT)ä»˜ããƒ‡ãƒ¼ã‚¿: {cot_non_empty}/{len(df)} ({cot_non_empty/len(df)*100:.1f}%)")
        
        # å›ç­”ã®è©³ç´°åº¦ãƒã‚§ãƒƒã‚¯
        avg_response_length = df['Response'].str.len().mean()
        if avg_response_length < 100:
            issues.append(f"âš ï¸ å›ç­”ã®å¹³å‡æ–‡å­—æ•°ãŒçŸ­ã™ãã¾ã™ï¼ˆ{avg_response_length:.0f}æ–‡å­—ï¼‰")
    
    return issues

def validate_sciq_data_specific(df: pd.DataFrame) -> List[str]:
    """ç§‘å­¦ãƒ»æŠ€è¡“QAãƒ‡ãƒ¼ã‚¿å›ºæœ‰ã®æ¤œè¨¼"""
    issues = []
    
    if 'question' in df.columns and 'correct_answer' in df.columns:
        # ç§‘å­¦ç”¨èªã®ãƒã‚§ãƒƒã‚¯
        science_keywords = ['åŒ–å­¦', 'ç‰©ç†', 'ç”Ÿç‰©', 'æ•°å­¦', 'science', 'physics', 'chemistry', 'biology']
        has_science_content = df['question'].str.contains('|'.join(science_keywords), na=False, case=False).any()
        if not has_science_content:
            issues.append("âš ï¸ ç§‘å­¦é–¢é€£ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        
        # é¸æŠè‚¢ã®å­˜åœ¨ç¢ºèª
        distractor_cols = ['distractor1', 'distractor2', 'distractor3']
        available_distractors = [col for col in distractor_cols if col in df.columns]
        if available_distractors:
            st.info(f"ğŸ’¡ é¸æŠè‚¢ãƒ‡ãƒ¼ã‚¿ã‚ã‚Š: {len(available_distractors)}å€‹ã®èª¤ç­”é¸æŠè‚¢")
        
        # ã‚µãƒãƒ¼ãƒˆãƒ†ã‚­ã‚¹ãƒˆã®ç¢ºèª
        if 'support' in df.columns:
            support_non_empty = df['support'].notna().sum()
            st.info(f"ğŸ’¡ è£œè¶³èª¬æ˜ä»˜ããƒ‡ãƒ¼ã‚¿: {support_non_empty}/{len(df)} ({support_non_empty/len(df)*100:.1f}%)")
    
    return issues

def validate_legal_data_specific(df: pd.DataFrame) -> List[str]:
    """æ³•å¾‹QAãƒ‡ãƒ¼ã‚¿å›ºæœ‰ã®æ¤œè¨¼"""
    issues = []
    
    if 'question' in df.columns and 'answer' in df.columns:
        # æ³•å¾‹ç”¨èªã®ãƒã‚§ãƒƒã‚¯
        legal_keywords = ['æ³•å¾‹', 'æ¡æ–‡', 'åˆ¤ä¾‹', 'å¥‘ç´„', 'law', 'legal', 'contract', 'regulation']
        has_legal_content = df['question'].str.contains('|'.join(legal_keywords), na=False, case=False).any()
        if not has_legal_content:
            issues.append("âš ï¸ æ³•å¾‹é–¢é€£ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        
        # æ³•çš„å‚ç…§ã®ç¢ºèª
        has_references = df['answer'].str.contains('æ¡|æ³•|è¦å‰‡|åˆ¤ä¾‹', na=False).any()
        if has_references:
            ref_count = df['answer'].str.contains('æ¡|æ³•|è¦å‰‡|åˆ¤ä¾‹', na=False).sum()
            st.info(f"ğŸ’¡ æ³•çš„å‚ç…§ã‚’å«ã‚€ãƒ‡ãƒ¼ã‚¿: {ref_count}/{len(df)} ({ref_count/len(df)*100:.1f}%)")
        
        # å›ç­”ã®åˆ†é¡
        answer_lengths = df['answer'].str.len()
        categories = pd.cut(answer_lengths, bins=[0, 50, 100, 200, 500, float('inf')], 
                          labels=['è¶…çŸ­æ–‡', 'çŸ­æ–‡', 'ä¸­æ–‡', 'é•·æ–‡', 'è¶…é•·æ–‡'])
        st.info(f"ğŸ’¡ å›ç­”ã®é•·ã•åˆ†å¸ƒ: {categories.value_counts().to_dict()}")
    
    return issues

def validate_trivia_data_specific(df: pd.DataFrame) -> List[str]:
    """é›‘å­¦QAãƒ‡ãƒ¼ã‚¿å›ºæœ‰ã®æ¤œè¨¼"""
    issues = []
    
    if 'question' in df.columns and 'answer' in df.columns:
        # é›‘å­¦çš„ãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®ç¢ºèªï¼ˆå¹…åºƒã„ãƒˆãƒ”ãƒƒã‚¯ï¼‰
        trivia_keywords = ['èª°', 'ä½•', 'ã©ã“', 'ã„ã¤', 'ãªãœ', 'ã©ã®', 'who', 'what', 'where', 'when', 'why', 'which']
        has_trivia_content = df['question'].str.contains('|'.join(trivia_keywords), na=False, case=False).any()
        if not has_trivia_content:
            issues.append("âš ï¸ ä¸€èˆ¬çš„ãªè³ªå•å½¢å¼ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        
        # å›ç­”ã®ç°¡æ½”æ€§ãƒã‚§ãƒƒã‚¯
        avg_answer_length = df['answer'].str.len().mean()
        if avg_answer_length > 500:
            issues.append(f"âš ï¸ å›ç­”ãŒé•·ã™ãã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ï¼ˆå¹³å‡{avg_answer_length:.0f}æ–‡å­—ï¼‰")
        
        # ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ãƒšãƒ¼ã‚¸ã®å­˜åœ¨ç¢ºèª
        if 'entity_pages' in df.columns:
            entity_non_empty = df['entity_pages'].notna().sum()
            st.info(f"ğŸ’¡ ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ãƒšãƒ¼ã‚¸ä»˜ããƒ‡ãƒ¼ã‚¿: {entity_non_empty}/{len(df)} ({entity_non_empty/len(df)*100:.1f}%)")
        
        # æ¤œç´¢çµæœã®å­˜åœ¨ç¢ºèª
        if 'search_results' in df.columns:
            search_non_empty = df['search_results'].notna().sum()
            st.info(f"ğŸ’¡ æ¤œç´¢çµæœï¼ˆã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆï¼‰ä»˜ããƒ‡ãƒ¼ã‚¿: {search_non_empty}/{len(df)} ({search_non_empty/len(df)*100:.1f}%)")
        
        # è³ªå•ã®å¤šæ§˜æ€§ãƒã‚§ãƒƒã‚¯
        question_starts = df['question'].str[:10]  # æœ€åˆã®10æ–‡å­—
        unique_starts = question_starts.nunique()
        diversity_ratio = unique_starts / len(df) * 100
        if diversity_ratio < 50:
            st.warning(f"âš ï¸ è³ªå•ã®å¤šæ§˜æ€§ãŒä½ã„å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ï¼ˆ{diversity_ratio:.1f}%ï¼‰")
        else:
            st.info(f"ğŸ’¡ è³ªå•ã®å¤šæ§˜æ€§: {diversity_ratio:.1f}%")
    
    return issues

# ===================================================================
# ãƒ¡ã‚¤ãƒ³å‡¦ç†
# ===================================================================

def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†é–¢æ•°"""
    
    # ãƒšãƒ¼ã‚¸è¨­å®šï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚¿ã‚¤ãƒ—ã§åˆæœŸåŒ–ï¼‰
    setup_page_config("customer_support_faq")
    
    # ã‚µã‚¤ãƒ‰ãƒãƒ¼è¨­å®š
    with st.sidebar:
        setup_sidebar_header("customer_support_faq")
        
        # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚¿ã‚¤ãƒ—é¸æŠ
        st.subheader("ğŸ“Š ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚¿ã‚¤ãƒ—é¸æŠ")
        
        dataset_options = RAGConfig.get_all_datasets()
        dataset_labels = {
            dt: f"{RAGConfig.get_config(dt)['icon']} {RAGConfig.get_config(dt)['name']}"
            for dt in dataset_options
        }
        
        selected_dataset = st.selectbox(
            "å‡¦ç†ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚¿ã‚¤ãƒ—",
            options=dataset_options,
            format_func=lambda x: dataset_labels[x],
            help="å‡¦ç†ã—ãŸã„ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ã‚¿ã‚¤ãƒ—ã‚’é¸æŠã—ã¦ãã ã•ã„"
        )
        
        # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆè¨­å®šã‚’å–å¾—
        dataset_config = RAGConfig.get_config(selected_dataset)
        
        # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæƒ…å ±è¡¨ç¤º
        st.info(f"""
        **é¸æŠä¸­ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ:**
        - ã‚¿ã‚¤ãƒ—: {dataset_config['name']}
        - å¿…é ˆåˆ—: {', '.join(dataset_config['required_columns'])}
        """)
        
        # ãƒ¢ãƒ‡ãƒ«é¸æŠ
        st.divider()
        selected_model = select_model()
        show_model_info(selected_model)
        
        # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå›ºæœ‰ã®ã‚ªãƒ—ã‚·ãƒ§ãƒ³
        st.divider()
        st.subheader("âš™ï¸ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå›ºæœ‰è¨­å®š")
        
        dataset_specific_options = {}
        
        if selected_dataset == "customer_support_faq":
            dataset_specific_options['preserve_formatting'] = st.checkbox(
                "ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã‚’ä¿æŒ", 
                value=True,
                help="æ”¹è¡Œã‚„ç®‡æ¡æ›¸ããªã©ã®ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã‚’ä¿æŒã—ã¾ã™"
            )
            dataset_specific_options['normalize_questions'] = st.checkbox(
                "è³ªå•ã‚’æ­£è¦åŒ–",
                value=False,
                help="è³ªå•æ–‡ã‚’ç–‘å•å½¢ã«çµ±ä¸€ã—ã¾ã™"
            )
            
        elif selected_dataset == "medical_qa":
            dataset_specific_options['preserve_medical_terms'] = st.checkbox(
                "åŒ»å­¦ç”¨èªã‚’ä¿æŒ",
                value=True,
                help="åŒ»å­¦å°‚é–€ç”¨èªã‚’ãã®ã¾ã¾ä¿æŒã—ã¾ã™"
            )
            if 'Complex_CoT' in st.session_state.get('uploaded_columns', []):
                dataset_specific_options['include_cot'] = st.checkbox(
                    "Complex_CoTã‚’å«ã‚ã‚‹",
                    value=True,
                    help="æ¨è«–éç¨‹ï¼ˆComplex_CoTï¼‰ã‚’å«ã‚ã¾ã™"
                )
                
        elif selected_dataset == "sciq_qa":
            dataset_specific_options['include_distractors'] = st.checkbox(
                "èª¤ç­”é¸æŠè‚¢ã‚’å«ã‚ã‚‹",
                value=False,
                help="é¸æŠè‚¢å•é¡Œã®èª¤ç­”ã‚‚å«ã‚ã¾ã™"
            )
            dataset_specific_options['include_support'] = st.checkbox(
                "ã‚µãƒãƒ¼ãƒˆãƒ†ã‚­ã‚¹ãƒˆã‚’å«ã‚ã‚‹",
                value=True,
                help="è£œè¶³èª¬æ˜ãƒ†ã‚­ã‚¹ãƒˆã‚’å«ã‚ã¾ã™"
            )
            dataset_specific_options['preserve_scientific_notation'] = st.checkbox(
                "ç§‘å­¦çš„è¨˜æ³•ã‚’ä¿æŒ",
                value=True,
                help="æ•°å¼ã‚„åŒ–å­¦å¼ãªã©ã®è¨˜æ³•ã‚’ä¿æŒã—ã¾ã™"
            )
            
        elif selected_dataset == "legal_qa":
            dataset_specific_options['preserve_legal_terms'] = st.checkbox(
                "æ³•å¾‹ç”¨èªã‚’ä¿æŒ",
                value=True,
                help="æ³•å¾‹å°‚é–€ç”¨èªã‚’ãã®ã¾ã¾ä¿æŒã—ã¾ã™"
            )
            dataset_specific_options['preserve_references'] = st.checkbox(
                "æ¡æ–‡å‚ç…§ã‚’ä¿æŒ",
                value=True,
                help="æ³•ä»¤ã‚„åˆ¤ä¾‹ã¸ã®å‚ç…§ã‚’ä¿æŒã—ã¾ã™"
            )
            dataset_specific_options['normalize_case_names'] = st.checkbox(
                "äº‹ä»¶åã‚’æ­£è¦åŒ–",
                value=False,
                help="åˆ¤ä¾‹ã®äº‹ä»¶åã‚’çµ±ä¸€å½¢å¼ã«æ­£è¦åŒ–ã—ã¾ã™"
            )
            
        elif selected_dataset == "trivia_qa":
            dataset_specific_options['include_entity_pages'] = st.checkbox(
                "ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ãƒšãƒ¼ã‚¸ã‚’å«ã‚ã‚‹",
                value=True,
                help="Wikipediaãªã©ã®ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ãƒšãƒ¼ã‚¸æƒ…å ±ã‚’å«ã‚ã¾ã™"
            )
            dataset_specific_options['include_search_results'] = st.checkbox(
                "æ¤œç´¢çµæœã‚’å«ã‚ã‚‹",
                value=True,
                help="æ¤œç´¢çµæœï¼ˆã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆï¼‰ã‚’å«ã‚ã¾ã™"
            )
            dataset_specific_options['preserve_formatting'] = st.checkbox(
                "ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã‚’ä¿æŒ",
                value=True,
                help="ãƒ†ã‚­ã‚¹ãƒˆã®ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã‚’ä¿æŒã—ã¾ã™"
            )
    
    # ãƒ¡ã‚¤ãƒ³ã‚³ãƒ³ãƒ†ãƒ³ãƒ„
    setup_page_header(selected_dataset)
    
    # ä½¿ã„æ–¹ä¾‹ã‚’Expanderã§è¡¨ç¤º
    with st.expander("ğŸ“– **ä½¿ã„æ–¹ä¾‹**", expanded=False):
        st.markdown("""
        ### ğŸ¯ åŸºæœ¬çš„ãªä½¿ã„æ–¹
        
        1. **å·¦ãƒšã‚¤ãƒ³ã§è¨­å®š**
           - ğŸ“Š **ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚¿ã‚¤ãƒ—ã‚’é¸æŠ** (ã‚«ã‚¹ã‚¿ãƒãƒ¼ã‚µãƒãƒ¼ãƒˆã€åŒ»ç™‚QAã€ç§‘å­¦QAã€æ³•å¾‹QA)
           - ğŸ¤– ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠ
           - âš™ï¸ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå›ºæœ‰ã®è¨­å®šã‚’èª¿æ•´
        
        2. **å³ãƒšã‚¤ãƒ³ã§å‡¦ç†å®Ÿè¡Œ**
        
           **ğŸ“ ãƒ‡ãƒ¼ã‚¿ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰**
           - CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã€ã¾ãŸã¯
           - HuggingFaceã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’è‡ªå‹•ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
             - ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆåã‚’å…¥åŠ›ï¼ˆä¾‹: `sciq`ï¼‰
             - Splitåã¨ã‚µãƒ³ãƒ—ãƒ«æ•°ã‚’æŒ‡å®š
             - ã€ŒğŸ“¥ HuggingFaceã‹ã‚‰ãƒ­ãƒ¼ãƒ‰ã€ã‚’ã‚¯ãƒªãƒƒã‚¯
           
           **ğŸ” ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼**
           - ãƒ‡ãƒ¼ã‚¿ã®å“è³ªã‚’ãƒã‚§ãƒƒã‚¯
           - å¿…é ˆåˆ—ã®ç¢ºèª
           - ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå›ºæœ‰ã®æ¤œè¨¼çµæœã‚’ç¢ºèª
           
           **âš™ï¸ å‰å‡¦ç†å®Ÿè¡Œ**
           - çµåˆã™ã‚‹ã‚«ãƒ©ãƒ ã‚’é¸æŠ
           - ã‚»ãƒ‘ãƒ¬ãƒ¼ã‚¿ã‚’é¸æŠ
           - ã€ŒğŸš€ å‰å‡¦ç†ã‚’å®Ÿè¡Œã€ã‚’ã‚¯ãƒªãƒƒã‚¯
           
           **ğŸ“Š çµæœãƒ»ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰**
           - å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã‚’CSVã€TXTã€JSONå½¢å¼ã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
           - OUTPUTãƒ•ã‚©ãƒ«ãƒ€ã«ä¿å­˜
        
        ### ğŸ’¡ ãƒ’ãƒ³ãƒˆ
        - HuggingFaceã‹ã‚‰ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ãŸãƒ‡ãƒ¼ã‚¿ã¯è‡ªå‹•çš„ã«`datasets/`ãƒ•ã‚©ãƒ«ãƒ€ã«ä¿å­˜ã•ã‚Œã¾ã™
        - ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚‚åŒæ™‚ã«ä¿å­˜ã•ã‚Œã€å¾Œã§å‡¦ç†ã®å±¥æ­´ã‚’ç¢ºèªã§ãã¾ã™
        """)
    
    # ã‚¿ãƒ–è¨­å®š
    tab1, tab2, tab3, tab4 = st.tabs([
        "ğŸ“ ãƒ‡ãƒ¼ã‚¿ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰",
        "ğŸ” ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼",
        "âš™ï¸ å‰å‡¦ç†å®Ÿè¡Œ",
        "ğŸ“Š çµæœãƒ»ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰"
    ])
    
    # Tab 1: ãƒ‡ãƒ¼ã‚¿ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
    with tab1:
        st.header("ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰")
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
        uploaded_file = st.file_uploader(
            f"{dataset_config['name']}ã®CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’é¸æŠ",
            type=['csv'],
            help=f"å¿…é ˆåˆ—: {', '.join(dataset_config['required_columns'])}"
        )
        
        # ã¾ãŸã¯ã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆåã‚’å…¥åŠ›ã—ã¦HuggingFaceã‹ã‚‰è‡ªå‹•ãƒ­ãƒ¼ãƒ‰
        st.divider()
        st.subheader("ã¾ãŸã¯ã€HuggingFaceã‹ã‚‰è‡ªå‹•ãƒ­ãƒ¼ãƒ‰")
        
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆåã‚’è¨­å®š
        default_datasets = {
            "customer_support_faq": "MakTek/Customer_support_faqs_dataset",
            "medical_qa": "FreedomIntelligence/medical-o1-reasoning-SFT",
            "sciq_qa": "sciq",
            "legal_qa": "nguha/legalbench",
            "trivia_qa": "trivia_qa"  # TriviaQAç”¨ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¿½åŠ 
        }
        
        dataset_name = st.text_input(
            "HuggingFaceãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå",
            value=default_datasets.get(selected_dataset, ""),
            placeholder="ä¾‹: sciq"
        )
        
        col1, col2 = st.columns([1, 1])
        with col1:
            split_name = st.text_input("Splitå", value="train")
        with col2:
            sample_size = st.number_input("ã‚µãƒ³ãƒ—ãƒ«æ•°", min_value=10, value=1000)
        
        if st.button("ğŸ“¥ HuggingFaceã‹ã‚‰ãƒ­ãƒ¼ãƒ‰", type="primary"):
            # HuggingFaceã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ­ãƒ¼ãƒ‰
            try:
                from datasets import load_dataset as hf_load_dataset
                
                # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå›ºæœ‰ã®configè¨­å®š
                config_mapping = {
                    "medical_qa": "en",  # åŒ»ç™‚ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆç”¨
                    "legal_qa": "consumer_contracts_qa",  # æ³•å¾‹ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆç”¨
                    "trivia_qa": "rc"  # TriviaQAç”¨ (Reading Comprehension)
                }
                
                # configãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®æ±ºå®šï¼ˆnameãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¨ã—ã¦æ¸¡ã™ï¼‰
                config_param = config_mapping.get(selected_dataset, None)
                
                # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ãƒ­ãƒ¼ãƒ‰
                with st.spinner(f"HuggingFaceã‹ã‚‰{dataset_name}ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ä¸­..."):
                    if selected_dataset == "trivia_qa":
                        # TriviaQAç”¨ã®ç‰¹åˆ¥ãªå‡¦ç†
                        dataset = hf_load_dataset(dataset_name, config_param, split=split_name)
                        # ãƒ‡ãƒ¼ã‚¿ã‚’è¾æ›¸å½¢å¼ã‹ã‚‰DataFrameã«å¤‰æ›
                        data_list = []
                        for i, item in enumerate(dataset):
                            if i >= sample_size:
                                break
                            # TriviaQAã®ãƒ‡ãƒ¼ã‚¿æ§‹é€ ã«å¿œã˜ã¦å¿…è¦ãªãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’æŠ½å‡º
                            record = {
                                'question_id': item.get('question_id', ''),
                                'question': item.get('question', ''),
                                'answer': '',  # answerã¯è¾æ›¸å‹ãªã®ã§å¾Œã§å‡¦ç†
                                'entity_pages': '',  # entity_pagesã¯è¾æ›¸å‹ãªã®ã§å¾Œã§å‡¦ç†
                                'search_results': ''  # search_resultsã¯è¾æ›¸å‹ãªã®ã§å¾Œã§å‡¦ç†
                            }
                            
                            # answerãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã®å‡¦ç†
                            if 'answer' in item and item['answer']:
                                answer_data = item['answer']
                                if isinstance(answer_data, dict):
                                    # æ­£è¦åŒ–ã•ã‚ŒãŸå€¤ã‚’å„ªå…ˆ
                                    if 'normalized_value' in answer_data and answer_data['normalized_value']:
                                        record['answer'] = answer_data['normalized_value']
                                    elif 'value' in answer_data:
                                        record['answer'] = answer_data['value']
                                    elif 'aliases' in answer_data and answer_data['aliases']:
                                        record['answer'] = answer_data['aliases'][0] if isinstance(answer_data['aliases'], list) else str(answer_data['aliases'])
                                else:
                                    record['answer'] = str(answer_data)
                            
                            # entity_pagesãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã®å‡¦ç†
                            if 'entity_pages' in item and item['entity_pages']:
                                pages = item['entity_pages']
                                if isinstance(pages, dict):
                                    # è¾æ›¸ã®å€¤ã‹ã‚‰ã‚¿ã‚¤ãƒˆãƒ«ã‚’æŠ½å‡º
                                    titles = []
                                    for key, page in list(pages.items())[:3]:  # æœ€åˆã®3ã¤
                                        if isinstance(page, dict) and 'title' in page:
                                            titles.append(page['title'])
                                        elif isinstance(page, str):
                                            titles.append(page)
                                    record['entity_pages'] = ' | '.join(titles)
                                elif isinstance(pages, list):
                                    # ãƒªã‚¹ãƒˆã®å ´åˆ
                                    titles = []
                                    for page in pages[:3]:
                                        if isinstance(page, dict) and 'title' in page:
                                            titles.append(page['title'])
                                        elif isinstance(page, str):
                                            titles.append(page)
                                    record['entity_pages'] = ' | '.join(titles)
                            
                            # search_resultsãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã®å‡¦ç†
                            if 'search_results' in item and item['search_results']:
                                results = item['search_results']
                                if isinstance(results, dict):
                                    # è¾æ›¸ã®å€¤ã‹ã‚‰æ¤œç´¢çµæœã‚’æŠ½å‡º
                                    contexts = []
                                    for key, result in list(results.items())[:2]:  # æœ€åˆã®2ã¤
                                        if isinstance(result, dict):
                                            context = result.get('search_context', result.get('description', ''))
                                            if context and len(context) > 500:
                                                context = context[:500] + '...'
                                            if context:
                                                contexts.append(context)
                                        elif isinstance(result, str):
                                            context = result[:500] + '...' if len(result) > 500 else result
                                            contexts.append(context)
                                    record['search_results'] = ' '.join(contexts)
                                elif isinstance(results, list):
                                    # ãƒªã‚¹ãƒˆã®å ´åˆ
                                    contexts = []
                                    for result in results[:2]:
                                        if isinstance(result, dict):
                                            context = result.get('search_context', result.get('description', ''))
                                            if context and len(context) > 500:
                                                context = context[:500] + '...'
                                            if context:
                                                contexts.append(context)
                                        elif isinstance(result, str):
                                            context = result[:500] + '...' if len(result) > 500 else result
                                            contexts.append(context)
                                    record['search_results'] = ' '.join(contexts)
                            
                            data_list.append(record)
                        
                        df = pd.DataFrame(data_list)
                        
                    elif config_param:
                        dataset = hf_load_dataset(dataset_name, name=config_param, split=split_name)
                        df = dataset.to_pandas().head(sample_size) if hasattr(dataset, 'to_pandas') else pd.DataFrame(dataset[:sample_size])
                    else:
                        dataset = hf_load_dataset(dataset_name, split=split_name)
                        df = dataset.to_pandas().head(sample_size) if hasattr(dataset, 'to_pandas') else pd.DataFrame(dataset[:sample_size])
                
                # datasetsãƒ•ã‚©ãƒ«ãƒ€ã«ä¿å­˜
                if df is not None:
                    # datasetsãƒ•ã‚©ãƒ«ãƒ€ã‚’ä½œæˆ
                    datasets_dir = Path("datasets")
                    datasets_dir.mkdir(exist_ok=True)
                    
                    # ãƒ•ã‚¡ã‚¤ãƒ«åã‚’ç”Ÿæˆ
                    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                    safe_dataset_name = dataset_name.replace("/", "_").replace("-", "_")
                    csv_filename = f"{safe_dataset_name}_{split_name}_{sample_size}_{timestamp}.csv"
                    csv_path = datasets_dir / csv_filename
                    
                    # CSVãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ä¿å­˜
                    df.to_csv(csv_path, index=False)
                    st.info(f"ğŸ“‚ ãƒ‡ãƒ¼ã‚¿ã‚’datasets/{csv_filename}ã«ä¿å­˜ã—ã¾ã—ãŸ")
                    
                    # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚‚ä¿å­˜
                    metadata = {
                        'dataset_name': dataset_name,
                        'dataset_type': selected_dataset,
                        'split': split_name,
                        'sample_size': sample_size,
                        'actual_size': len(df),
                        'config': config_param,
                        'downloaded_at': datetime.now().isoformat(),
                        'columns': df.columns.tolist()
                    }
                    
                    metadata_filename = f"{safe_dataset_name}_{split_name}_{sample_size}_{timestamp}_metadata.json"
                    metadata_path = datasets_dir / metadata_filename
                    
                    with open(metadata_path, 'w', encoding='utf-8') as f:
                        json.dump(metadata, f, ensure_ascii=False, indent=2)
                    
            except Exception as e:
                st.error(f"ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ãƒ­ãƒ¼ãƒ‰ã«å¤±æ•—ã—ã¾ã—ãŸ: {str(e)}")
                df = None
            
            if df is not None:
                st.session_state['uploaded_data'] = df
                st.session_state['uploaded_columns'] = df.columns.tolist()
                st.success(f"âœ… {len(df)}ä»¶ã®ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ­ãƒ¼ãƒ‰ã—ã€datasets/ãƒ•ã‚©ãƒ«ãƒ€ã«ä¿å­˜ã—ã¾ã—ãŸ")
        
        # ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã®å‡¦ç†
        if uploaded_file is not None:
            df = pd.read_csv(uploaded_file)
            st.session_state['uploaded_data'] = df
            st.session_state['uploaded_columns'] = df.columns.tolist()
            st.success(f"âœ… ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¾ã—ãŸ: {uploaded_file.name}")
        
        # ãƒ‡ãƒ¼ã‚¿ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼
        if 'uploaded_data' in st.session_state:
            df = st.session_state['uploaded_data']
            st.subheader("ğŸ“‹ ãƒ‡ãƒ¼ã‚¿ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼")
            st.info(f"ãƒ‡ãƒ¼ã‚¿ä»¶æ•°: {len(df)}ä»¶ | ã‚«ãƒ©ãƒ æ•°: {len(df.columns)}åˆ—")
            st.dataframe(df.head(10), use_container_width=True)
            
            # ã‚«ãƒ©ãƒ æƒ…å ±
            with st.expander("ğŸ“Š ã‚«ãƒ©ãƒ è©³ç´°"):
                col_info = pd.DataFrame({
                    'ã‚«ãƒ©ãƒ å': df.columns,
                    'ãƒ‡ãƒ¼ã‚¿å‹': df.dtypes.astype(str),
                    'éNULLæ•°': df.count(),
                    'NULLæ•°': df.isnull().sum(),
                    'ãƒ¦ãƒ‹ãƒ¼ã‚¯æ•°': [df[col].nunique() for col in df.columns]
                })
                st.dataframe(col_info, use_container_width=True)
    
    # Tab 2: ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼
    with tab2:
        st.header("ãƒ‡ãƒ¼ã‚¿å“è³ªãƒã‚§ãƒƒã‚¯")
        
        if 'uploaded_data' not in st.session_state:
            st.warning("âš ï¸ ã¾ãšãƒ‡ãƒ¼ã‚¿ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„")
        else:
            df = st.session_state['uploaded_data']
            
            # å…±é€šæ¤œè¨¼
            st.subheader("ğŸ“‹ åŸºæœ¬æ¤œè¨¼")
            issues = validate_data(df, selected_dataset)
            
            # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå›ºæœ‰ã®æ¤œè¨¼
            st.subheader(f"ğŸ” {dataset_config['name']}å›ºæœ‰ã®æ¤œè¨¼")
            
            if selected_dataset == "customer_support_faq":
                specific_issues = validate_customer_support_data_specific(df)
            elif selected_dataset == "medical_qa":
                specific_issues = validate_medical_data_specific(df)
            elif selected_dataset == "sciq_qa":
                specific_issues = validate_sciq_data_specific(df)
            elif selected_dataset == "legal_qa":
                specific_issues = validate_legal_data_specific(df)
            elif selected_dataset == "trivia_qa":
                specific_issues = validate_trivia_data_specific(df)
            else:
                specific_issues = []
            
            issues.extend(specific_issues)
            
            # æ¤œè¨¼çµæœã®è¡¨ç¤º
            if issues:
                st.warning("ä»¥ä¸‹ã®å•é¡ŒãŒæ¤œå‡ºã•ã‚Œã¾ã—ãŸï¼š")
                for issue in issues:
                    st.write(f"â€¢ {issue}")
            else:
                st.success("âœ… ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼ã«åˆæ ¼ã—ã¾ã—ãŸï¼")
            
            # ãƒ‡ãƒ¼ã‚¿çµ±è¨ˆï¼ˆå‡¦ç†å‰å¾Œã®ãƒ‡ãƒ¼ã‚¿æ¯”è¼ƒç”¨ã«åŒã˜ãƒ‡ãƒ¼ã‚¿ã‚’æ¸¡ã™ï¼‰
            display_statistics(df, df, selected_dataset)
            
            # å¿…é ˆåˆ—ã®ã‚µãƒ³ãƒ—ãƒ«è¡¨ç¤º
            st.subheader("ğŸ“ å¿…é ˆåˆ—ã®ã‚µãƒ³ãƒ—ãƒ«")
            for col in dataset_config['required_columns']:
                if col in df.columns:
                    with st.expander(f"{col} ã®ã‚µãƒ³ãƒ—ãƒ«ï¼ˆæœ€åˆã®3ä»¶ï¼‰"):
                        for i, value in enumerate(df[col].head(3), 1):
                            st.text(f"[{i}] {value}")
    
    # Tab 3: å‰å‡¦ç†å®Ÿè¡Œ
    with tab3:
        st.header("RAGç”¨ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†")
        
        if 'uploaded_data' not in st.session_state:
            st.warning("âš ï¸ ã¾ãšãƒ‡ãƒ¼ã‚¿ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„")
        else:
            df = st.session_state['uploaded_data']
            
            # ã‚«ãƒ©ãƒ çµåˆã‚ªãƒ—ã‚·ãƒ§ãƒ³
            st.subheader("ğŸ“ ãƒ†ã‚­ã‚¹ãƒˆçµåˆè¨­å®š")
            
            # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå›ºæœ‰ã®ã‚«ãƒ©ãƒ çµåˆè¨­å®š
            if selected_dataset == "medical_qa":
                # å®Ÿéš›ã®ã‚«ãƒ©ãƒ åã‚’å¤§æ–‡å­—å°æ–‡å­—ã‚’ç„¡è¦–ã—ã¦æ¤œç´¢
                actual_columns = []
                medical_columns_map = {}
                for col in df.columns:
                    col_lower = col.lower()
                    if 'question' in col_lower:
                        actual_columns.append(col)
                        medical_columns_map['Question'] = col
                    elif 'complex_cot' in col_lower or 'cot' in col_lower:
                        actual_columns.append(col)
                        medical_columns_map['Complex_CoT'] = col
                    elif 'response' in col_lower:
                        actual_columns.append(col)
                        medical_columns_map['Response'] = col
                
                # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã®è¨­å®š
                if dataset_specific_options.get('include_cot', True) and 'Complex_CoT' in medical_columns_map:
                    default_cols = list(medical_columns_map.values())
                else:
                    default_cols = [v for k, v in medical_columns_map.items() if k != 'Complex_CoT']
                
                combine_columns = st.multiselect(
                    "çµåˆã™ã‚‹ã‚«ãƒ©ãƒ ã‚’é¸æŠ",
                    options=actual_columns,
                    default=default_cols if default_cols else actual_columns
                )
            elif selected_dataset == "sciq_qa":
                available_cols = ['question', 'correct_answer']
                if 'support' in df.columns and dataset_specific_options.get('include_support', True):
                    available_cols.append('support')
                if dataset_specific_options.get('include_distractors', False):
                    for col in ['distractor1', 'distractor2', 'distractor3']:
                        if col in df.columns:
                            available_cols.append(col)
                combine_columns = st.multiselect(
                    "çµåˆã™ã‚‹ã‚«ãƒ©ãƒ ã‚’é¸æŠ",
                    options=available_cols,
                    default=available_cols[:2]  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯ question ã¨ correct_answer
                )
            elif selected_dataset == "trivia_qa":
                # TriviaQAç”¨ã®ã‚«ãƒ©ãƒ è¨­å®š
                available_cols = []
                if 'question' in df.columns:
                    available_cols.append('question')
                if 'answer' in df.columns:
                    available_cols.append('answer')
                if 'entity_pages' in df.columns and dataset_specific_options.get('include_entity_pages', True):
                    available_cols.append('entity_pages')
                if 'search_results' in df.columns and dataset_specific_options.get('include_search_results', True):
                    available_cols.append('search_results')
                
                # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯å…¨ã‚«ãƒ©ãƒ ã‚’é¸æŠ
                default_cols = ['question', 'answer']
                if 'entity_pages' in df.columns:
                    default_cols.append('entity_pages')
                if 'search_results' in df.columns:
                    default_cols.append('search_results')
                    
                combine_columns = st.multiselect(
                    "çµåˆã™ã‚‹ã‚«ãƒ©ãƒ ã‚’é¸æŠ",
                    options=available_cols,
                    default=[col for col in default_cols if col in available_cols]
                )
            else:
                combine_columns = st.multiselect(
                    "çµåˆã™ã‚‹ã‚«ãƒ©ãƒ ã‚’é¸æŠ",
                    options=dataset_config['required_columns'],
                    default=dataset_config['required_columns']
                )
            
            # ã‚»ãƒ‘ãƒ¬ãƒ¼ã‚¿é¸æŠ
            separator_options = {
                "ã‚¹ãƒšãƒ¼ã‚¹": " ",
                "æ”¹è¡Œ": "\n",
                "ã‚¿ãƒ–": "\t",
                "ã‚«ã‚¹ã‚¿ãƒ ": ""
            }
            separator_type = st.radio("ã‚»ãƒ‘ãƒ¬ãƒ¼ã‚¿", options=list(separator_options.keys()), horizontal=True)
            
            if separator_type == "ã‚«ã‚¹ã‚¿ãƒ ":
                custom_separator = st.text_input("ã‚«ã‚¹ã‚¿ãƒ ã‚»ãƒ‘ãƒ¬ãƒ¼ã‚¿", value=" | ")
                separator = custom_separator
            else:
                separator = separator_options[separator_type]
            
            # å‡¦ç†å®Ÿè¡Œãƒœã‚¿ãƒ³
            if st.button("ğŸš€ å‰å‡¦ç†ã‚’å®Ÿè¡Œ", type="primary"):
                with st.spinner("å‡¦ç†ä¸­..."):
                    # ãƒ‡ãƒ¼ã‚¿å‡¦ç†
                    df_processed = process_rag_data(
                        df, 
                        selected_dataset,
                        combine_columns_option=True  # ã‚«ãƒ©ãƒ ã‚’çµåˆã™ã‚‹ã‚ªãƒ—ã‚·ãƒ§ãƒ³
                    )
                    
                    # Combined_Textã‚«ãƒ©ãƒ ãŒæ—¢ã«å­˜åœ¨ã™ã‚‹å ´åˆã¯ã€ã‚»ãƒ‘ãƒ¬ãƒ¼ã‚¿ã‚’é©ç”¨ã—ã¦ä¸Šæ›¸ã
                    # (process_rag_dataé–¢æ•°ã§Combined_TextãŒä½œæˆã•ã‚Œã¦ã„ã‚‹ãŸã‚)
                    if combine_columns and 'Combined_Text' in df_processed.columns:
                        # é¸æŠã•ã‚ŒãŸã‚«ãƒ©ãƒ ãŒå®Ÿéš›ã«å­˜åœ¨ã™ã‚‹ã‹ç¢ºèª
                        missing_cols = [col for col in combine_columns if col not in df.columns]
                        if missing_cols:
                            st.error(f"âŒ é¸æŠã•ã‚ŒãŸã‚«ãƒ©ãƒ ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {missing_cols}")
                            st.info(f"åˆ©ç”¨å¯èƒ½ãªã‚«ãƒ©ãƒ : {list(df.columns)}")
                        else:
                            # å…ƒã®ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã‹ã‚‰é¸æŠã•ã‚ŒãŸã‚«ãƒ©ãƒ ã‚’çµåˆ
                            df_processed['Combined_Text'] = df[combine_columns].apply(
                                lambda row: separator.join(row.dropna().astype(str)), axis=1
                            )
                    
                    # çµæœã‚’ä¿å­˜
                    st.session_state['processed_data'] = df_processed
                    st.session_state['processing_config'] = {
                        'dataset_type': selected_dataset,
                        'combine_columns': combine_columns,
                        'separator': separator,
                        'options': dataset_specific_options
                    }
                    
                    st.success("âœ… å‰å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸï¼")
            
            # å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã®ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼
            if 'processed_data' in st.session_state:
                df_processed = st.session_state['processed_data']
                
                st.subheader("ğŸ“‹ å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼")
                st.dataframe(df_processed.head(10), use_container_width=True)
                
                # ãƒˆãƒ¼ã‚¯ãƒ³ä½¿ç”¨é‡æ¨å®š
                st.subheader("ğŸ’° ãƒˆãƒ¼ã‚¯ãƒ³ä½¿ç”¨é‡æ¨å®š")
                estimate_token_usage(df_processed, selected_model)  # ã“ã®é–¢æ•°ã¯å†…éƒ¨ã§è¡¨ç¤ºã‚’è¡Œã†
    
    # Tab 4: çµæœãƒ»ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
    with tab4:
        st.header("å‡¦ç†çµæœã¨ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰")
        
        if 'processed_data' not in st.session_state:
            st.warning("âš ï¸ ã¾ãšå‰å‡¦ç†ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„")
        else:
            df_processed = st.session_state['processed_data']
            config = st.session_state['processing_config']
            
            # å‡¦ç†ã‚µãƒãƒªãƒ¼
            st.subheader("ğŸ“Š å‡¦ç†ã‚µãƒãƒªãƒ¼")
            col1, col2, col3 = st.columns(3)
            with col1:
                st.metric("å‡¦ç†ä»¶æ•°", f"{len(df_processed):,}ä»¶")
            with col2:
                st.metric("ã‚«ãƒ©ãƒ æ•°", len(df_processed.columns))
            with col3:
                st.metric("çµåˆã‚«ãƒ©ãƒ æ•°", len(config['combine_columns']))
            
            # ãƒ•ã‚¡ã‚¤ãƒ«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
            st.subheader("ğŸ“¥ ãƒ•ã‚¡ã‚¤ãƒ«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰")
            
            # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒ‡ãƒ¼ã‚¿ä½œæˆ
            csv_data, txt_data = create_download_data(
                df_processed,
                include_combined=True,
                dataset_type=config['dataset_type']
            )
            
            # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆ
            metadata = {
                'dataset_type': config['dataset_type'],
                'processed_at': datetime.now().isoformat(),
                'row_count': len(df_processed),
                'column_count': len(df_processed.columns),
                'config': config
            }
            
            col1, col2, col3 = st.columns(3)
            
            with col1:
                st.download_button(
                    label="ğŸ“„ CSVãƒ•ã‚¡ã‚¤ãƒ«",
                    data=csv_data,
                    file_name=f"preprocessed_{config['dataset_type']}.csv",
                    mime="text/csv"
                )
            
            with col2:
                st.download_button(
                    label="ğŸ“ ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«",
                    data=txt_data,
                    file_name=f"{config['dataset_type']}.txt",
                    mime="text/plain"
                )
            
            with col3:
                st.download_button(
                    label="ğŸ“‹ ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿(JSON)",
                    data=json.dumps(metadata, ensure_ascii=False, indent=2),
                    file_name=f"metadata_{config['dataset_type']}.json",
                    mime="application/json"
                )
            
            # OUTPUTãƒ•ã‚©ãƒ«ãƒ€ã¸ã®ä¿å­˜
            st.divider()
            if st.button("ğŸ’¾ OUTPUTãƒ•ã‚©ãƒ«ãƒ€ã«ä¿å­˜", type="primary"):
                output_dir = Path("OUTPUT")
                # CSVãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆ
                csv_buffer = io.StringIO()
                df_processed.to_csv(csv_buffer, index=False)
                csv_data = csv_buffer.getvalue()
                
                # ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆ
                text_data = None
                if 'Combined_Text' in df_processed.columns:
                    text_data = '\n'.join(df_processed['Combined_Text'].dropna().astype(str))
                
                saved_files = save_files_to_output(
                    df_processed,
                    config['dataset_type'],
                    csv_data,
                    text_data
                )
                
                if saved_files:
                    st.success("âœ… ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜ã—ã¾ã—ãŸï¼š")
                    for file in saved_files:
                        st.write(f"â€¢ {file}")
                else:
                    st.error("âŒ ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜ã«å¤±æ•—ã—ã¾ã—ãŸ")
            
            # ãƒ‡ãƒ¼ã‚¿ã‚µãƒ³ãƒ—ãƒ«è¡¨ç¤º
            st.divider()
            st.subheader("ğŸ“ å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã®ã‚µãƒ³ãƒ—ãƒ«")
            st.dataframe(df_processed.head(5), use_container_width=True)
    
    # ä½¿ç”¨æ–¹æ³•
    with st.expander("ğŸ“š ä½¿ç”¨æ–¹æ³•"):
        show_usage_instructions(selected_dataset)

if __name__ == "__main__":
    main()