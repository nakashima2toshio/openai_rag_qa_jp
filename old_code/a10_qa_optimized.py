#!/usr/bin/env python3
"""
Q&Aç”Ÿæˆã«æœ€é©åŒ–ã•ã‚ŒãŸã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡ºã‚·ã‚¹ãƒ†ãƒ 
preprocessed CSVãƒ•ã‚¡ã‚¤ãƒ«å¯¾å¿œç‰ˆï¼ˆa02_make_qa.pyå‚è€ƒï¼‰

ä½¿ç”¨æ–¹æ³•:
    # åŸºæœ¬ä½¿ç”¨ï¼ˆcc_newsãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼‰
    python a10_qa_optimized.py --dataset cc_news --output qa_output

    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæŒ‡å®š
    python a10_qa_optimized.py --dataset cc_news --max-docs 10

    # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæŒ‡å®š
    python a10_qa_optimized.py --dataset japanese_text --output qa_output

    # Q/Aæ•°ã‚’æ‰‹å‹•æŒ‡å®š
    python a10_qa_optimized.py --dataset wikipedia_ja --qa-count 10
"""

import os
import sys
import json
import argparse
import pandas as pd
from pathlib import Path
from typing import List, Dict, Optional
from datetime import datetime
import logging

from helper_rag_qa import QAOptimizedExtractor, QACountOptimizer

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# ==========================================
# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆè¨­å®šï¼ˆa02_make_qa.pyã‹ã‚‰å¼•ç”¨ï¼‰
# ==========================================

DATASET_CONFIGS = {
    "cc_news": {
        "name": "CC-Newsè‹±èªãƒ‹ãƒ¥ãƒ¼ã‚¹",
        "file": "OUTPUT/preprocessed_cc_news.csv",
        "text_column": "Combined_Text",
        "title_column": "title",
        "lang": "en",
    },
    "japanese_text": {
        "name": "æ—¥æœ¬èªWebãƒ†ã‚­ã‚¹ãƒˆ",
        "file": "OUTPUT/preprocessed_japanese_text.csv",
        "text_column": "Combined_Text",
        "title_column": None,
        "lang": "ja",
    },
    "wikipedia_ja": {
        "name": "Wikipediaæ—¥æœ¬èªç‰ˆ",
        "file": "OUTPUT/preprocessed_wikipedia_ja.csv",
        "text_column": "Combined_Text",
        "title_column": "title",
        "lang": "ja",
    }
}


# ==========================================
# ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿é–¢æ•°
# ==========================================

def load_preprocessed_data(dataset_type: str) -> pd.DataFrame:
    """preprocessedãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿
    Args:
        dataset_type: ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚¿ã‚¤ãƒ—
    Returns:
        èª­ã¿è¾¼ã‚“ã DataFrame
    """
    config = DATASET_CONFIGS.get(dataset_type)
    if not config:
        raise ValueError(f"æœªå¯¾å¿œã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ: {dataset_type}")

    file_path = config["file"]
    if not Path(file_path).exists():
        raise FileNotFoundError(f"ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {file_path}")

    logger.info(f"ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ä¸­: {file_path}")
    df = pd.read_csv(file_path)

    # å¿…è¦ãªã‚«ãƒ©ãƒ ã®ç¢ºèª
    text_col = config["text_column"]
    if text_col not in df.columns:
        raise ValueError(f"ãƒ†ã‚­ã‚¹ãƒˆã‚«ãƒ©ãƒ  '{text_col}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")

    # ç©ºã®ãƒ†ã‚­ã‚¹ãƒˆã‚’é™¤å¤–
    df = df[df[text_col].notna() & (df[text_col].str.strip() != '')]

    logger.info(f"èª­ã¿è¾¼ã¿å®Œäº†: {len(df)}ä»¶ã®ãƒ‡ãƒ¼ã‚¿")
    return df


# ==========================================
# ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡ºå‡¦ç†
# ==========================================

def extract_keywords_from_dataset(
    df: pd.DataFrame,
    dataset_type: str,
    max_docs: Optional[int] = None,
    qa_count: Optional[int] = None,
    use_progressive: bool = True,
    output_dir: str = "qa_keywords_output"
) -> Dict:
    """ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‹ã‚‰ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡ºã¨Q/Aãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆç”Ÿæˆ

    Args:
        df: ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ 
        dataset_type: ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚¿ã‚¤ãƒ—
        max_docs: å‡¦ç†ã™ã‚‹æœ€å¤§æ–‡æ›¸æ•°
        qa_count: ç”Ÿæˆã™ã‚‹Q/Aæ•°ï¼ˆNoneã®å ´åˆã¯è‡ªå‹•æ±ºå®šï¼‰
        use_progressive: æ®µéšçš„ç”Ÿæˆã‚’ä½¿ç”¨ã™ã‚‹ã‹
        output_dir: å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª

    Returns:
        æŠ½å‡ºçµæœã®è¾æ›¸
    """
    config = DATASET_CONFIGS[dataset_type]
    text_col = config["text_column"]
    title_col = config.get("title_column")

    # å‡¦ç†ã™ã‚‹æ–‡æ›¸æ•°ã‚’åˆ¶é™
    docs_to_process = df.head(max_docs) if max_docs else df

    logger.info(f"ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡ºé–‹å§‹: {len(docs_to_process)}ä»¶ã®æ–‡æ›¸")

    extractor = QAOptimizedExtractor()
    optimizer = QACountOptimizer()

    all_results = []
    total_keywords = 0
    total_relations = 0
    total_qa_templates = 0

    for idx, row in docs_to_process.iterrows():
        # ãƒ†ã‚­ã‚¹ãƒˆå–å¾—
        text = str(row[text_col]) if pd.notna(row[text_col]) else ""

        # ã‚¿ã‚¤ãƒˆãƒ«å–å¾—ï¼ˆã‚ã‚Œã°ï¼‰
        title = ""
        if title_col and title_col in row and pd.notna(row[title_col]):
            title = str(row[title_col])

        # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆIDä½œæˆ
        doc_id = f"{dataset_type}_{idx}"
        if title:
            doc_id += f"_{title[:30]}"

        logger.debug(f"å‡¦ç†ä¸­: {doc_id}")

        # Q/Aæ•°ã‚’è‡ªå‹•æ±ºå®šï¼ˆæœªæŒ‡å®šã®å ´åˆï¼‰
        if qa_count is None:
            qa_result = optimizer.calculate_optimal_qa_count(text, mode="auto")
            doc_qa_count = qa_result['optimal_count']
            logger.debug(f"  è‡ªå‹•æ±ºå®šQ/Aæ•°: {doc_qa_count}")
        else:
            doc_qa_count = qa_count

        # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡ºã¨Q/Aãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆç”Ÿæˆ
        result = extractor.extract_for_qa_generation(
            text,
            qa_count=doc_qa_count,
            mode="auto",
            use_progressive=use_progressive,
            return_details=True
        )

        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿è¿½åŠ 
        result['doc_id'] = doc_id
        result['doc_idx'] = idx
        result['title'] = title

        # çµ±è¨ˆæ›´æ–°
        total_keywords += result['metadata']['total_keywords_extracted']
        total_relations += result['metadata']['total_relations_found']
        total_qa_templates += len(result.get('suggested_qa_pairs', []))

        all_results.append(result)

    # çµæœã®ã‚µãƒãƒªãƒ¼ä½œæˆ
    summary = {
        "dataset_type": dataset_type,
        "dataset_name": config["name"],
        "documents_processed": len(docs_to_process),
        "total_keywords": total_keywords,
        "total_relations": total_relations,
        "total_qa_templates": total_qa_templates,
        "avg_keywords_per_doc": total_keywords / len(docs_to_process) if docs_to_process.shape[0] > 0 else 0,
        "avg_relations_per_doc": total_relations / len(docs_to_process) if docs_to_process.shape[0] > 0 else 0,
        "avg_qa_templates_per_doc": total_qa_templates / len(docs_to_process) if docs_to_process.shape[0] > 0 else 0,
        "extraction_timestamp": datetime.now().isoformat()
    }

    return {
        "summary": summary,
        "results": all_results
    }


# ==========================================
# çµæœä¿å­˜
# ==========================================

def save_results(
    extraction_results: Dict,
    dataset_type: str,
    output_dir: str = "qa_keywords_output"
) -> Dict[str, str]:
    """çµæœã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜

    Args:
        extraction_results: æŠ½å‡ºçµæœ
        dataset_type: ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚¿ã‚¤ãƒ—
        output_dir: å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª

    Returns:
        ä¿å­˜ã—ãŸãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
    """
    output_path = Path(output_dir)
    output_path.mkdir(exist_ok=True)

    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

    # 1. ã‚µãƒãƒªãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«
    summary_file = output_path / f"keyword_summary_{dataset_type}_{timestamp}.json"
    with open(summary_file, 'w', encoding='utf-8') as f:
        json.dump(extraction_results['summary'], f, ensure_ascii=False, indent=2)

    # 2. è©³ç´°çµæœãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆJSONï¼‰
    details_file = output_path / f"keyword_details_{dataset_type}_{timestamp}.json"
    with open(details_file, 'w', encoding='utf-8') as f:
        json.dump(extraction_results['results'], f, ensure_ascii=False, indent=2)

    # 3. ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒªã‚¹ãƒˆï¼ˆCSVï¼‰
    keywords_data = []
    for doc_result in extraction_results['results']:
        doc_id = doc_result['doc_id']
        for kw in doc_result.get('keywords', []):
            keywords_data.append({
                'doc_id': doc_id,
                'keyword': kw['keyword'],
                'difficulty': kw['difficulty'],
                'category': kw['category'],
                'frequency': kw['frequency'],
                'context': kw.get('best_context', '')[:100]  # æœ€åˆã®100æ–‡å­—
            })

    if keywords_data:
        keywords_df = pd.DataFrame(keywords_data)
        keywords_csv = output_path / f"keywords_{dataset_type}_{timestamp}.csv"
        keywords_df.to_csv(keywords_csv, index=False, encoding='utf-8')
    else:
        keywords_csv = None

    # 4. Q/Aãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆï¼ˆCSVï¼‰
    qa_data = []
    for doc_result in extraction_results['results']:
        doc_id = doc_result['doc_id']
        for qa in doc_result.get('suggested_qa_pairs', []):
            for q_template in qa.get('question_templates', []):
                qa_data.append({
                    'doc_id': doc_id,
                    'keyword': qa['keyword'],
                    'difficulty': qa['difficulty'],
                    'question_template': q_template,
                    'suggested_answer_length': qa.get('suggested_answer_length', 'medium')
                })

    if qa_data:
        qa_df = pd.DataFrame(qa_data)
        qa_csv = output_path / f"qa_templates_{dataset_type}_{timestamp}.csv"
        qa_df.to_csv(qa_csv, index=False, encoding='utf-8')
    else:
        qa_csv = None

    logger.info(f"çµæœã‚’ä¿å­˜ã—ã¾ã—ãŸ: {output_path}")

    return {
        "summary": str(summary_file),
        "details": str(details_file),
        "keywords_csv": str(keywords_csv) if keywords_csv else None,
        "qa_templates_csv": str(qa_csv) if qa_csv else None
    }


# ==========================================
# ãƒ¡ã‚¤ãƒ³å‡¦ç†
# ==========================================

def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    parser = argparse.ArgumentParser(
        description="Q&Aç”Ÿæˆæœ€é©åŒ–ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡ºã‚·ã‚¹ãƒ†ãƒ ï¼ˆpreprocessedå¯¾å¿œç‰ˆï¼‰"
    )
    parser.add_argument(
        "--dataset",
        type=str,
        choices=list(DATASET_CONFIGS.keys()),
        default="cc_news",
        help="å‡¦ç†ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ"
    )
    parser.add_argument(
        "--max-docs",
        type=int,
        default=None,
        help="å‡¦ç†ã™ã‚‹æœ€å¤§æ–‡æ›¸æ•°ï¼ˆãƒ†ã‚¹ãƒˆç”¨ï¼‰"
    )
    parser.add_argument(
        "--qa-count",
        type=int,
        default=None,
        help="æ–‡æ›¸ã‚ãŸã‚Šã®Q/Aæ•°ï¼ˆæœªæŒ‡å®šã®å ´åˆã¯è‡ªå‹•æ±ºå®šï¼‰"
    )
    parser.add_argument(
        "--output",
        type=str,
        default="qa_keywords_output",
        help="å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª"
    )
    parser.add_argument(
        "--no-progressive",
        action="store_true",
        help="æ®µéšçš„ç”Ÿæˆã‚’ç„¡åŠ¹åŒ–"
    )
    parser.add_argument(
        "--demo",
        action="store_true",
        help="ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãƒ¢ãƒ¼ãƒ‰ã§å®Ÿè¡Œ"
    )

    args = parser.parse_args()

    # ãƒ‡ãƒ¢ãƒ¢ãƒ¼ãƒ‰ã®å ´åˆã¯å…ƒã®å‡¦ç†ã‚’å®Ÿè¡Œ
    if args.demo:
        logger.info("ãƒ‡ãƒ¢ãƒ¢ãƒ¼ãƒ‰ã§å®Ÿè¡Œä¸­...")
        run_original_demo()
        return

    logger.info(f"""
    =====================================
    Q&Aæœ€é©åŒ–ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡ºé–‹å§‹
    =====================================
    ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ: {DATASET_CONFIGS[args.dataset]['name']}
    å‡ºåŠ›å…ˆ: {args.output}
    æœ€å¤§æ–‡æ›¸æ•°: {args.max_docs if args.max_docs else 'åˆ¶é™ãªã—'}
    Q/Aæ•°: {args.qa_count if args.qa_count else 'è‡ªå‹•æ±ºå®š'}
    æ®µéšçš„ç”Ÿæˆ: {'ç„¡åŠ¹' if args.no_progressive else 'æœ‰åŠ¹'}
    """)

    try:
        # 1. ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
        logger.info("\n[1/3] ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿...")
        df = load_preprocessed_data(args.dataset)

        # 2. ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡º
        logger.info("\n[2/3] ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡ºã¨Q/Aãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆç”Ÿæˆ...")
        extraction_results = extract_keywords_from_dataset(
            df,
            args.dataset,
            max_docs=args.max_docs,
            qa_count=args.qa_count,
            use_progressive=not args.no_progressive,
            output_dir=args.output
        )

        # 3. çµæœä¿å­˜
        logger.info("\n[3/3] çµæœä¿å­˜...")
        saved_files = save_results(extraction_results, args.dataset, args.output)

        # å®Œäº†ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
        summary = extraction_results['summary']
        logger.info(f"""
        =====================================
        å‡¦ç†å®Œäº†
        =====================================
        å‡¦ç†æ–‡æ›¸æ•°: {summary['documents_processed']}
        æŠ½å‡ºã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ç·æ•°: {summary['total_keywords']}
        é–¢ä¿‚æ€§ç·æ•°: {summary['total_relations']}
        Q/Aãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆç·æ•°: {summary['total_qa_templates']}

        å¹³å‡å€¤ï¼ˆæ–‡æ›¸ã‚ãŸã‚Šï¼‰:
        - ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: {summary['avg_keywords_per_doc']:.1f}å€‹
        - é–¢ä¿‚æ€§: {summary['avg_relations_per_doc']:.1f}å€‹
        - Q/Aãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ: {summary['avg_qa_templates_per_doc']:.1f}å€‹

        ä¿å­˜ãƒ•ã‚¡ã‚¤ãƒ«:
        - ã‚µãƒãƒªãƒ¼: {saved_files['summary']}
        - è©³ç´°: {saved_files['details']}
        - ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰CSV: {saved_files['keywords_csv']}
        - Q/Aãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆCSV: {saved_files['qa_templates_csv']}
        """)

        # OpenAI APIåˆ©ç”¨å›æ•°ã®èª¬æ˜
        print("\n" + "=" * 80)
        print("ğŸ“Š OpenAI APIåˆ©ç”¨å›æ•°ã«ã¤ã„ã¦")
        print("=" * 80)
        print(f"""
ã“ã®ãƒ„ãƒ¼ãƒ«ï¼ˆa10_qa_optimized.pyï¼‰ã¯ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆãƒ™ãƒ¼ã‚¹ã§Q/Aã‚’ç”Ÿæˆã™ã‚‹ãŸã‚ã€
OpenAI APIã‚’ä½¿ç”¨ã—ã¾ã›ã‚“ï¼ˆ0å›ï¼‰ã€‚

ãŸã ã—ã€ç”Ÿæˆã•ã‚ŒãŸãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‹ã‚‰å®Ÿéš›ã®Q/Aãƒšã‚¢ã‚’ç”Ÿæˆã™ã‚‹å ´åˆã¯ã€
å¾Œå‡¦ç†ã§LLMã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰ã€‚

ã€497è¨˜äº‹ï¼ˆpreprocessed_cc_news.csvå…¨ä½“ï¼‰ã‚’å‡¦ç†ã—ãŸå ´åˆã€‘
- ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡º: 0å›ï¼ˆãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹ï¼‰
- é–¢ä¿‚æ€§åˆ†æ: 0å›ï¼ˆãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒãƒ³ã‚°ï¼‰
- Q/Aãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆç”Ÿæˆ: 0å›ï¼ˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆï¼‰
- åˆè¨ˆ: 0å›

ã€LLMã¨ä½µç”¨ã™ã‚‹å ´åˆï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰ã€‘
å„è¨˜äº‹ã«å¯¾ã—ã¦LLMã§Q/Aã‚’ç”Ÿæˆã™ã‚‹å ´åˆ:
- 497è¨˜äº‹ Ã— 1å› = 497å›ã®APIå‘¼ã³å‡ºã—
- ã‚³ã‚¹ãƒˆè©¦ç®—: ç´„$0.15ï¼ˆgpt-5-miniä½¿ç”¨æ™‚ï¼‰
""")

    except Exception as e:
        logger.error(f"å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


def run_original_demo():
    """å…ƒã®ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³é–¢æ•°ã‚’å®Ÿè¡Œ"""
    from helper_rag_qa import QAOptimizedExtractor, QACountOptimizer

    # å…ƒã®ãƒ‡ãƒ¢ã‚³ãƒ¼ãƒ‰ã‚’å®Ÿè¡Œ
    print("=" * 80)
    print("Q&Aç”Ÿæˆæœ€é©åŒ–ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡ºã‚·ã‚¹ãƒ†ãƒ ï¼ˆãƒ‡ãƒ¢ï¼‰")
    print("=" * 80)

    test_text = """
    äººå·¥çŸ¥èƒ½ï¼ˆAIï¼‰ã¯ã€æ©Ÿæ¢°å­¦ç¿’ã¨æ·±å±¤å­¦ç¿’ã‚’åŸºç›¤ã¨ã—ã¦æ€¥é€Ÿã«ç™ºå±•ã—ã¦ã„ã¾ã™ã€‚
    ç‰¹ã«è‡ªç„¶è¨€èªå‡¦ç†ï¼ˆNLPï¼‰ã®åˆ†é‡ã§ã¯ã€ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ãƒ¢ãƒ‡ãƒ«ãŒé©å‘½çš„ãªæˆæœã‚’ä¸Šã’ã¾ã—ãŸã€‚
    BERTã‚„GPTãªã©ã®å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ã¯ã€æ–‡è„ˆç†è§£èƒ½åŠ›ã‚’å¤§å¹…ã«å‘ä¸Šã•ã›ã¦ã„ã¾ã™ã€‚
    """

    extractor = QAOptimizedExtractor()
    result = extractor.extract_for_qa_generation(
        test_text,
        qa_count=3,
        difficulty_distribution={'basic': 0.4, 'intermediate': 0.4, 'advanced': 0.2}
    )

    print(f"\nå…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆé•·: {len(test_text)}æ–‡å­—")
    print(f"æŠ½å‡ºã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ•°: {result['metadata']['total_keywords_extracted']}")
    print(f"é–¢ä¿‚æ€§æ•°: {result['metadata']['total_relations_found']}")

    print("\næŠ½å‡ºã•ã‚ŒãŸã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰:")
    for kw in result['keywords'][:5]:
        print(f"  â€¢ {kw['keyword']} ({kw['difficulty']})")

    print("\nç”Ÿæˆã•ã‚ŒãŸQ&Aãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ:")
    for qa in result['suggested_qa_pairs'][:3]:
        print(f"  â€¢ {qa['keyword']}: {qa['question_templates'][0] if qa['question_templates'] else 'N/A'}")


if __name__ == "__main__":
    main()