#!/usr/bin/env python3
"""
ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’èª­ã¿è¾¼ã¿ã€å…ˆé ­10ã‚»ãƒ³ãƒ†ãƒ³ã‚¹åˆ†ã®ãƒãƒ£ãƒ³ã‚¯ã¨Q/Aãƒšã‚¢ã‚’å‡ºåŠ›
python a03_rag_qa_coverage_file.py
"""

from helper_rag_qa import SemanticCoverage
import os
import re


def load_file(file_path: str, max_sentences: int = 10) -> str:
    """
    æŒ‡å®šã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰å…ˆé ­max_sentencesæ–‡ã‚’èª­ã¿è¾¼ã‚€

    Args:
        file_path: èª­ã¿è¾¼ã‚€ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
        max_sentences: èª­ã¿è¾¼ã‚€æœ€å¤§æ–‡æ•°

    Returns:
        èª­ã¿è¾¼ã‚“ã ãƒ†ã‚­ã‚¹ãƒˆ
    """
    with open(file_path, 'r', encoding='utf-8') as f:
        text = f.read()

    # æ—¥æœ¬èªæ–‡ã®åˆ†å‰²ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆSemanticCoverageã¨åŒã˜ãƒ­ã‚¸ãƒƒã‚¯ï¼‰
    sentence_pattern = r'[ã€‚ï¼.!?ï¼ï¼Ÿ\n]+'
    sentences = re.split(sentence_pattern, text)

    # ç©ºæ–‡å­—åˆ—ã‚’é™¤å¤–
    sentences = [s.strip() for s in sentences if s.strip()]

    # å…ˆé ­max_sentencesæ–‡ã‚’çµåˆ
    selected_sentences = sentences[:max_sentences]
    return 'ã€‚'.join(selected_sentences) + 'ã€‚' if selected_sentences else ''


def demonstrate_file_coverage(file_path: str, max_sentences: int = 10):
    """
    ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’èª­ã¿è¾¼ã¿ã€ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯ã‚«ãƒãƒ¬ãƒƒã‚¸åˆ†æã‚’å®Ÿæ–½

    Args:
        file_path: å‡¦ç†ã™ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
        max_sentences: èª­ã¿è¾¼ã‚€æœ€å¤§æ–‡æ•°
    """

    print("=" * 80)
    print(f"ãƒ•ã‚¡ã‚¤ãƒ«: {file_path}")
    print(f"èª­ã¿è¾¼ã¿æ–‡æ•°: å…ˆé ­{max_sentences}ã‚»ãƒ³ãƒ†ãƒ³ã‚¹")
    print("=" * 80)

    # ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿
    print("\nãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿ä¸­...")
    document_text = load_file(file_path, max_sentences=max_sentences)

    print(f"\nèª­ã¿è¾¼ã‚“ã ãƒ†ã‚­ã‚¹ãƒˆ ({len(document_text)}æ–‡å­—):")
    print("-" * 80)
    print(document_text)
    print("-" * 80)

    # SemanticCoverageã®åˆæœŸåŒ–
    print("\nSemanticCoverageã‚’åˆæœŸåŒ–ä¸­...")
    analyzer = SemanticCoverage(embedding_model="text-embedding-3-small")

    # æ–‡æ›¸ã‚’ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯ãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰²
    print("\næ–‡æ›¸ã‚’ãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰²ä¸­...")
    chunks = analyzer.create_semantic_chunks(document_text, verbose=False)

    print(f"\nâœ… {len(chunks)}å€‹ã®ãƒãƒ£ãƒ³ã‚¯ã‚’ä½œæˆã—ã¾ã—ãŸ\n")

    # ãƒãƒ£ãƒ³ã‚¯æƒ…å ±ã®å‡ºåŠ›
    print("=" * 80)
    print("ãƒãƒ£ãƒ³ã‚¯æƒ…å ±")
    print("=" * 80)

    for i, chunk in enumerate(chunks, 1):
        print(f"\nã€ãƒãƒ£ãƒ³ã‚¯ {i}ã€‘")
        print(f"  ID: {chunk['id']}")
        print(f"  æ–‡æ•°: {len(chunk['sentences'])}")
        print(f"  æ–‡ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹: {chunk['start_sentence_idx']} â†’ {chunk['end_sentence_idx']}")
        print(f"  ãƒ†ã‚­ã‚¹ãƒˆ:")
        print(f"    {chunk['text'][:200]}{'...' if len(chunk['text']) > 200 else ''}")

    # Q/Aãƒšã‚¢ã®ã‚µãƒ³ãƒ—ãƒ«ç”Ÿæˆï¼ˆæ‰‹å‹•ï¼‰
    print("\n\n" + "=" * 80)
    print("ã‚µãƒ³ãƒ—ãƒ«Q/Aãƒšã‚¢ï¼ˆæ‰‹å‹•ä½œæˆï¼‰")
    print("=" * 80)

    # ãƒãƒ£ãƒ³ã‚¯ã‹ã‚‰ç°¡æ˜“çš„ã«Q/Aãƒšã‚¢ã‚’ä½œæˆ
    qa_pairs = []
    for i, chunk in enumerate(chunks[:3], 1):  # æœ€åˆã®3ãƒãƒ£ãƒ³ã‚¯ã®ã¿
        # æœ€åˆã®æ–‡ã‚’ã‚‚ã¨ã«è³ªå•ã‚’ä½œæˆ
        first_sentence = chunk['sentences'][0] if chunk['sentences'] else chunk['text'][:50]

        qa = {
            "question": f"ãƒãƒ£ãƒ³ã‚¯{i}ã«ã¤ã„ã¦èª¬æ˜ã—ã¦ãã ã•ã„",
            "answer": first_sentence,
            "chunk_id": chunk['id'],
            "type": "manual"
        }
        qa_pairs.append(qa)

    print(f"\n{len(qa_pairs)}å€‹ã®Q/Aãƒšã‚¢ã‚’ä½œæˆã—ã¾ã—ãŸ:\n")

    for i, qa in enumerate(qa_pairs, 1):
        print(f"ã€Q/A {i}ã€‘")
        print(f"  è³ªå•: {qa['question']}")
        print(f"  å›ç­”: {qa['answer'][:100]}{'...' if len(qa['answer']) > 100 else ''}")
        print(f"  å¯¾å¿œãƒãƒ£ãƒ³ã‚¯: {qa['chunk_id']}")
        print()

    # ã‚«ãƒãƒ¬ãƒƒã‚¸æƒ…å ±
    print("=" * 80)
    print("ã‚«ãƒãƒ¬ãƒƒã‚¸ã‚µãƒãƒªãƒ¼")
    print("=" * 80)
    print(f"  ç·ãƒãƒ£ãƒ³ã‚¯æ•°: {len(chunks)}")
    print(f"  Q/Aãƒšã‚¢æ•°: {len(qa_pairs)}")
    print(f"  ã‚«ãƒãƒ¬ãƒƒã‚¸ç‡ï¼ˆæ¦‚ç®—ï¼‰: {len(qa_pairs) / len(chunks) * 100:.1f}%")
    print()


def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""

    # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
    default_file = "OUTPUT/wikipedia_ja.txt"

    # ç’°å¢ƒãƒã‚§ãƒƒã‚¯
    api_key = os.getenv('OPENAI_API_KEY')
    print(f"\nğŸ“‹ ç’°å¢ƒãƒã‚§ãƒƒã‚¯:")
    print(f"  OpenAI APIã‚­ãƒ¼: {'âœ… è¨­å®šæ¸ˆã¿' if api_key else 'âŒ æœªè¨­å®š'}")

    if not api_key:
        print("\nâš ï¸  OPENAI_API_KEYãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
        print("   åŸ‹ã‚è¾¼ã¿ç”Ÿæˆæ©Ÿèƒ½ã¯ä½¿ç”¨ã§ãã¾ã›ã‚“")
        print("   ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²ã®ã¿å®Ÿè¡Œã—ã¾ã™\n")

    # ãƒ•ã‚¡ã‚¤ãƒ«ã®å­˜åœ¨ç¢ºèª
    if not os.path.exists(default_file):
        print(f"\nâŒ ã‚¨ãƒ©ãƒ¼: ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {default_file}")
        print("\nOUTPUTãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®åˆ©ç”¨å¯èƒ½ãªãƒ•ã‚¡ã‚¤ãƒ«:")
        output_dir = "OUTPUT"
        if os.path.exists(output_dir):
            for file in os.listdir(output_dir):
                if file.endswith('.txt'):
                    print(f"  - {os.path.join(output_dir, file)}")
        return

    # ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯ã‚«ãƒãƒ¬ãƒƒã‚¸åˆ†æå®Ÿè¡Œ
    demonstrate_file_coverage(default_file, max_sentences=10)

    print("\n" + "=" * 80)
    print("å‡¦ç†å®Œäº†")
    print("=" * 80)


if __name__ == "__main__":
    main()