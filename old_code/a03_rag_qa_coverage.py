#!/usr/bin/env python3
"""
ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯ã‚«ãƒãƒ¬ãƒƒã‚¸åˆ†æã¨Q/Aç”Ÿæˆã‚·ã‚¹ãƒ†ãƒ ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«å…¥åŠ›å¯¾å¿œç‰ˆï¼‰
=====================================================
helper_rag_qa.pyã®å…¨ã‚¯ãƒ©ã‚¹ã‚’æ´»ç”¨ã—ãŸåŒ…æ‹¬çš„ãªQ/Aç”Ÿæˆã‚·ã‚¹ãƒ†ãƒ 

ä½¿ç”¨æ–¹æ³•:
    python a03_rag_qa_coverage.py [--input INPUT_FILE] [--dataset DATASET_TYPE] [--model MODEL] [--output OUTPUT_DIR]

ä¾‹:
    # preprocessedãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰å‡¦ç†
    python a03_rag_qa_coverage.py --input OUTPUT/preprocessed_cc_news.csv --dataset cc_news --model gpt-5-mini --analyze-coverage

    # ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ç›´æ¥å‡¦ç†
    python a03_rag_qa_coverage.py --input OUTPUT/cc_news.txt --model gpt-5-mini --analyze-coverage

    # ã‚«ãƒãƒ¬ãƒ¼ã‚¸åˆ†æä»˜ã
    python a03_rag_qa_coverage.py --input OUTPUT/preprocessed_cc_news.csv --analyze-coverage
"""

from helper_rag_qa import (
    SemanticCoverage,
    QAGenerationConsiderations,
    QAPair,
    QAPairsList,
    LLMBasedQAGenerator,
    ChainOfThoughtQAGenerator,
    RuleBasedQAGenerator,
    TemplateBasedQAGenerator,
    HybridQAGenerator,
    AdvancedQAGenerationTechniques,
    QAGenerationOptimizer,
)
import os
import sys
import json
import argparse
import pandas as pd
import numpy as np
from pathlib import Path
from typing import List, Dict, Optional
from datetime import datetime
import logging
import pprint

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

def qa_generation_checklist():
    """Q/Aç”Ÿæˆæ™‚ã®ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ"""
    return {
        "äº‹å‰æº–å‚™"  : [
            "â–¡ æ–‡æ›¸ã®ç¨®é¡ã¨ç‰¹æ€§ã‚’åˆ†æ",
            "â–¡ ç›®çš„ï¼ˆè©•ä¾¡/å­¦ç¿’/ãƒ†ã‚¹ãƒˆï¼‰ã‚’æ˜ç¢ºåŒ–",
            "â–¡ å¿…è¦ãªã‚«ãƒãƒ¬ãƒƒã‚¸ãƒ¬ãƒ™ãƒ«ã‚’è¨­å®š",
            "â–¡ äºˆç®—ã¨ãƒªã‚½ãƒ¼ã‚¹ã‚’ç¢ºèª"
        ],
        "å“è³ªåŸºæº–"  : [
            "â–¡ å›ç­”ãŒãƒ†ã‚­ã‚¹ãƒˆå†…ã«å­˜åœ¨ã™ã‚‹ã“ã¨ã‚’ç¢ºèª",
            "â–¡ è³ªå•ã®æ˜ç¢ºæ€§ã¨æ›–æ˜§ã•ã®æ’é™¤",
            "â–¡ è³ªå•ã‚¿ã‚¤ãƒ—ã®å¤šæ§˜æ€§ã‚’ç¢ºä¿",
            "â–¡ é›£æ˜“åº¦ã®ãƒãƒ©ãƒ³ã‚¹ã‚’èª¿æ•´"
        ],
        "æŠ€è¡“é¸æŠ"  : [
            "â–¡ ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹ã§åŸºæœ¬çš„ãªQ/Aã‚’ç”Ÿæˆ",
            "â–¡ LLMã§è¤‡é›‘ãªæ¨è«–Q/Aã‚’è£œå®Œ",
            "â–¡ ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã§æœ€é©åŒ–",
            "â–¡ äººé–“ã®ãƒ¬ãƒ“ãƒ¥ãƒ¼ã§å“è³ªä¿è¨¼"
        ],
        "è©•ä¾¡ã¨æ”¹å–„": [
            "â–¡ ã‚«ãƒãƒ¬ãƒƒã‚¸æ¸¬å®šã®å®Ÿæ–½",
            "â–¡ é‡è¤‡ã¨çŸ›ç›¾ã®æ¤œå‡º",
            "â–¡ ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã®åé›†",
            "â–¡ ç¶™ç¶šçš„ãªæ”¹å–„ã‚µã‚¤ã‚¯ãƒ«"
        ]
    }


def demonstrate_semantic_coverage(document_text):
    """SemanticCoverageã‚¯ãƒ©ã‚¹ã®ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³"""

    print("=" * 80)
    print("1. ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯ã‚«ãƒãƒ¬ãƒƒã‚¸åˆ†æ")
    print("=" * 80)

    document_text = document_text

    # SemanticCoverageã®åˆæœŸåŒ–
    analyzer = SemanticCoverage(embedding_model="text-embedding-3-small")

    # æ–‡æ›¸ã‚’ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯ãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰²
    print("\næ–‡æ›¸ã‚’ãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰²ä¸­...")
    chunks = analyzer.create_semantic_chunks(document_text, verbose=False)

    print(f"\nâœ… {len(chunks)}å€‹ã®ãƒãƒ£ãƒ³ã‚¯ã‚’ä½œæˆã—ã¾ã—ãŸ")
    for i, chunk in enumerate(chunks, 1):
        print(f"  ãƒãƒ£ãƒ³ã‚¯{i}: {chunk['text'][:60]}...")
        print(f"    æ–‡æ•°: {len(chunk['sentences'])}, ID: {chunk['id']}")

    return analyzer, chunks, document_text


def demonstrate_qa_generation_considerations(document_text: str):
    """QAGenerationConsiderationsã‚¯ãƒ©ã‚¹ã®ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³"""

    print("\n\n" + "=" * 80)
    print("2. Q/Aç”Ÿæˆå‰ã®ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ")
    print("=" * 80)

    # ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆã®è¡¨ç¤º
    checklist = qa_generation_checklist()

    for category, items in checklist.items():
        print(f"\nã€{category}ã€‘")
        for item in items:
            print(f"  {item}")


def demonstrate_rule_based_generation(document_text: str):
    """RuleBasedQAGeneratorã‚¯ãƒ©ã‚¹ã®ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆæ”¹å–„ç‰ˆï¼‰"""

    print("\n\n" + "=" * 80)
    print("3. ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹Q/Aç”Ÿæˆ")
    print("=" * 80)

    # é•·ã„ãƒ†ã‚­ã‚¹ãƒˆã®å ´åˆã¯è­¦å‘Š
    text_length = len(document_text)
    print(f"\nğŸ“Š å‡¦ç†æº–å‚™:")
    print(f"  ãƒ†ã‚­ã‚¹ãƒˆé•·: {text_length:,}æ–‡å­—")

    # 497è¨˜äº‹ã®å ´åˆã€ç´„100ä¸‡æ–‡å­—ã«ãªã‚‹ãŸã‚ã€æœ€åˆã®ä¸€éƒ¨ã ã‘å‡¦ç†
    MAX_LENGTH = 50000  # æœ€å¤§5ä¸‡æ–‡å­—
    if text_length > MAX_LENGTH:
        print(f"  âš ï¸ ãƒ†ã‚­ã‚¹ãƒˆãŒé•·ã™ãã‚‹ãŸã‚ã€æœ€åˆã®{MAX_LENGTH:,}æ–‡å­—ã®ã¿å‡¦ç†ã—ã¾ã™")
        document_text = document_text[:MAX_LENGTH]

    # è¨€èªæ¤œå‡ºï¼ˆç°¡æ˜“ï¼‰
    is_english = any(word in document_text[:500] for word in ['the', 'The', 'is', 'are', 'was', 'were', 'have', 'has'])

    if is_english:
        print("  æ¤œå‡ºè¨€èª: è‹±èª")
        print("  âš ï¸ è‹±èªãƒ†ã‚­ã‚¹ãƒˆã®ãŸã‚ã€ç°¡æ˜“ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒãƒ³ã‚°ã‚’ä½¿ç”¨ã—ã¾ã™")
        # è‹±èªç”¨ã®ç°¡æ˜“ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒãƒ³ã‚°
        return extract_english_qa_patterns(document_text)
    else:
        print("  æ¤œå‡ºè¨€èª: æ—¥æœ¬èª")
        try:
            # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆä»˜ãã§spaCyã‚’å®Ÿè¡Œ
            import signal
            from contextlib import contextmanager

            @contextmanager
            def timeout(seconds):
                def signal_handler(signum, frame):
                    raise TimeoutError("å‡¦ç†ãŒã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã—ã¾ã—ãŸ")

                # Windowsã®å ´åˆã¯ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã‚’ã‚¹ã‚­ãƒƒãƒ—
                if hasattr(signal, 'SIGALRM'):
                    signal.signal(signal.SIGALRM, signal_handler)
                    signal.alarm(seconds)
                    try:
                        yield
                    finally:
                        signal.alarm(0)
                else:
                    # Windowsç’°å¢ƒã§ã¯ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆãªã—
                    yield

            try:
                with timeout(10):  # 10ç§’ã§ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ
                    print("\nâ³ spaCyãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–ä¸­...")
                    rule_generator = RuleBasedQAGenerator()

                print("\nå®šç¾©æ–‡ã‹ã‚‰Q/AæŠ½å‡ºä¸­...")
                with timeout(20):  # 20ç§’ã§ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ
                    definition_qas = rule_generator.extract_definition_qa(document_text)

            except TimeoutError as e:
                print(f"\nâš ï¸ {e}")
                print("  ç°¡æ˜“ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒãƒ³ã‚°ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã—ã¾ã™")
                return []

            if definition_qas:
                print(f"\nâœ… {len(definition_qas)}å€‹ã®å®šç¾©Q/Aã‚’ç”Ÿæˆã—ã¾ã—ãŸ")
                for i, qa in enumerate(definition_qas[:3], 1):  # æœ€åˆã®3ã¤ã®ã¿è¡¨ç¤º
                    print(f"\n  ã€å®šç¾©Q/A {i}ã€‘")
                    print(f"    è³ªå•: {qa['question'][:50]}...")
                    print(f"    å›ç­”: {qa['answer'][:80]}...")
                    print(f"    ä¿¡é ¼åº¦: {qa.get('confidence', 'N/A')}")
                if len(definition_qas) > 3:
                    print(f"  ... ä»–{len(definition_qas) - 3}å€‹")
            else:
                print("\nâš ï¸  å®šç¾©æ–‡ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")

            return definition_qas

        except (OSError, ImportError) as e:
            print(f"\nâš ï¸  spaCyãƒ¢ãƒ‡ãƒ«ã®ã‚¨ãƒ©ãƒ¼: {e}")
            print("    ç°¡æ˜“ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒãƒ³ã‚°ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã—ã¾ã™")
            return []
        except Exception as e:
            print(f"\nâš ï¸  äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼: {e}")
            return []


def extract_english_qa_patterns(text: str) -> List[Dict]:
    """è‹±èªãƒ†ã‚­ã‚¹ãƒˆç”¨ã®ç°¡æ˜“Q/Aç”Ÿæˆï¼ˆspaCyä¸è¦ï¼‰"""
    import re

    qa_pairs = []

    # ãƒ†ã‚­ã‚¹ãƒˆã‚’æ–‡ã«åˆ†å‰²ï¼ˆæœ€åˆã®50æ–‡ã®ã¿å‡¦ç†ï¼‰
    sentences = re.split(r'[.!?]\s+', text)[:50]

    print(f"\nğŸ“ è‹±èªãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒãƒ³ã‚°é–‹å§‹...")
    print(f"  å‡¦ç†æ–‡æ•°: {len(sentences)}")

    for i, sent in enumerate(sentences, 1):
        if i % 10 == 0:
            print(f"  é€²æ—: {i}/{len(sentences)}æ–‡ å‡¦ç†æ¸ˆã¿...")

        # Pattern 1: "X is Y" statements
        is_match = re.search(r'^([A-Z][^,]+?)\s+(is|was|are|were)\s+(.+)$', sent.strip())
        if is_match and len(is_match.group(1)) < 80:
            subject = is_match.group(1).strip()
            verb = is_match.group(2)
            predicate = is_match.group(3).strip()[:100]

            qa_pairs.append({
                "question": f"What {verb} {subject}?",
                "answer": f"{subject} {verb} {predicate}",
                "type": "definition",
                "confidence": 0.7
            })

    # é‡è¤‡é™¤å»
    seen_questions = set()
    unique_qas = []
    for qa in qa_pairs:
        if qa['question'] not in seen_questions:
            seen_questions.add(qa['question'])
            unique_qas.append(qa)

    print(f"\nâœ… {len(unique_qas)}å€‹ã®è‹±èªQ/Aã‚’ç”Ÿæˆã—ã¾ã—ãŸ")

    return unique_qas[:20]  # æœ€å¤§20å€‹ã¾ã§è¿”ã™


def demonstrate_template_based_generation(document_text: str):
    """TemplateBasedQAGeneratorã‚¯ãƒ©ã‚¹ã®ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³"""

    print("\n\n" + "=" * 80)
    print("4. ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆãƒ™ãƒ¼ã‚¹Q/Aç”Ÿæˆ")
    print("=" * 80)

    # ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆãƒ™ãƒ¼ã‚¹ç”Ÿæˆå™¨ã®åˆæœŸåŒ–
    template_generator = TemplateBasedQAGenerator()

    # ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã‚’æ‰‹å‹•æŒ‡å®šï¼ˆå®Ÿéš›ã«ã¯NERã§æŠ½å‡ºï¼‰
    entities = ['AI', 'æ©Ÿæ¢°å­¦ç¿’', 'ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼', 'BERT', 'GPT']

    print(f"\nã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£: {', '.join(entities)}")
    print("\nã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ãƒ™ãƒ¼ã‚¹Q/Aç”Ÿæˆä¸­...")

    template_qas = []
    for entity in entities[:3]:  # æœ€åˆã®3ã¤ã§ä¾‹ç¤º
        # ç°¡æ˜“çš„ãªãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆé©ç”¨
        qa = {
            "question": f"{entity}ã¨ã¯ä½•ã§ã™ã‹ï¼Ÿ",
            "answer": f"{entity}ã«é–¢ã™ã‚‹æƒ…å ±ã¯æ–‡æ›¸å†…ã§èª¬æ˜ã•ã‚Œã¦ã„ã¾ã™ã€‚",
            "entity": entity,
            "type": "entity_based",
            "confidence": 0.75
        }
        template_qas.append(qa)

    print(f"\nâœ… {len(template_qas)}å€‹ã®ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆQ/Aã‚’ç”Ÿæˆã—ã¾ã—ãŸ")
    for i, qa in enumerate(template_qas, 1):
        print(f"\n  ã€ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆQ/A {i}ã€‘")
        print(f"    è³ªå•: {qa['question']}")
        print(f"    å›ç­”: {qa['answer']}")
        print(f"    ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£: {qa['entity']}")
        print(f"    ä¿¡é ¼åº¦: {qa.get('confidence', 'N/A')}")

    return template_qas


def demonstrate_llm_based_generation(document_text: str):
    """LLMBasedQAGeneratorã‚¯ãƒ©ã‚¹ã®ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³"""

    print("\n\n" + "=" * 80)
    print("5. LLMãƒ™ãƒ¼ã‚¹Q/Aç”Ÿæˆ")
    print("=" * 80)

    api_key = os.getenv('OPENAI_API_KEY')

    if api_key:
        try:
            print("\nLLMBasedQAGenerator ã§Q/Aç”Ÿæˆä¸­...")
            llm_generator = LLMBasedQAGenerator(model="gpt-4o-mini")
            llm_qas = llm_generator.generate_basic_qa(document_text, num_pairs=3)

            print(f"\nâœ… {len(llm_qas)}å€‹ã®LLM Q/Aã‚’ç”Ÿæˆã—ã¾ã—ãŸ")
            for i, qa in enumerate(llm_qas[:2], 1):  # æœ€åˆã®2ã¤ã‚’è¡¨ç¤º
                print(f"\n  ã€LLM Q/A {i}ã€‘")
                print(f"    è³ªå•: {qa.get('question', 'N/A')}")
                print(f"    å›ç­”: {qa.get('answer', 'N/A')[:80]}...")
                print(f"    ç¨®é¡: {qa.get('question_type', 'N/A')}")

            return llm_qas

        except Exception as e:
            print(f"\nâš ï¸  LLM Q/Aç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
            return []
    else:
        print("\nâš ï¸  OpenAI APIã‚­ãƒ¼ãŒå¿…è¦ã§ã™ï¼ˆã‚¹ã‚­ãƒƒãƒ—ï¼‰")
        print("å®Ÿéš›ã®ä½¿ç”¨ä¾‹:")
        print("""
    # OpenAI APIã‚­ãƒ¼è¨­å®šå¾Œ
    llm_generator = LLMBasedQAGenerator(model="gpt-4o-mini")
    llm_qas = llm_generator.generate_basic_qa(document_text, num_pairs=3)
    """)

        # ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿
        llm_qas = [
            {
                "question": "AIã®å¿œç”¨åˆ†é‡ã«ã¯ã©ã®ã‚ˆã†ãªã‚‚ã®ãŒã‚ã‚Šã¾ã™ã‹ï¼Ÿ",
                "answer": "åŒ»ç™‚è¨ºæ–­ã‹ã‚‰è‡ªå‹•é‹è»¢ã¾ã§å¹…åºƒã„åˆ†é‡ã§å¿œç”¨ã•ã‚Œã¦ã„ã¾ã™ã€‚",
                "question_type": "fact",
                "difficulty": "basic"
            }
        ]

        print(f"\nï¼ˆã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼‰{len(llm_qas)}å€‹ã®LLM Q/Aã‚’ç”Ÿæˆ")
        return llm_qas


def demonstrate_cot_generation(document_text: str):
    """ChainOfThoughtQAGeneratorã‚¯ãƒ©ã‚¹ã®ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³"""

    print("\n\n" + "=" * 80)
    print("6. Chain-of-Thought Q/Aç”Ÿæˆ")
    print("=" * 80)

    api_key = os.getenv('OPENAI_API_KEY')

    if api_key:
        try:
            print("\nChainOfThoughtQAGenerator ã§Q/Aç”Ÿæˆä¸­...")
            cot_generator = ChainOfThoughtQAGenerator()
            result = cot_generator.generate_with_reasoning(document_text)

            # çµæœã‹ã‚‰ qa_pairs ã‚’å–å¾—
            cot_qas = result.get('qa_pairs', []) if isinstance(result, dict) else result

            print(f"\nâœ… {len(cot_qas)}å€‹ã®CoT Q/Aã‚’ç”Ÿæˆã—ã¾ã—ãŸ")
            for i, qa in enumerate(cot_qas[:2], 1):  # æœ€åˆã®2ã¤ã‚’è¡¨ç¤º
                print(f"\n  ã€CoT Q/A {i}ã€‘")
                print(f"    è³ªå•: {qa.get('question', 'N/A')}")
                print(f"    å›ç­”: {qa.get('answer', 'N/A')[:80]}...")
                print(f"    æ¨è«–: {qa.get('reasoning', 'N/A')[:80]}...")
                print(f"    ä¿¡é ¼åº¦: {qa.get('confidence', 'N/A')}")

            return cot_qas

        except Exception as e:
            print(f"\nâš ï¸  CoT Q/Aç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
            return []
    else:
        print("\nâš ï¸  OpenAI APIã‚­ãƒ¼ãŒå¿…è¦ã§ã™ï¼ˆã‚¹ã‚­ãƒƒãƒ—ï¼‰")
        print("å®Ÿéš›ã®ä½¿ç”¨ä¾‹:")
        print("""
    # OpenAI APIã‚­ãƒ¼è¨­å®šå¾Œ
    cot_generator = ChainOfThoughtQAGenerator(model="gpt-4o-mini", api_key=os.getenv("OPENAI_API_KEY"))
    cot_qas = cot_generator.generate_cot_qa(document_text, num_pairs=3, include_confidence=True)
    """)

        # ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿
        cot_qas = [
            {
                "question": "ãªãœãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ã¯RNNã‚ˆã‚Šé«˜é€Ÿãªã®ã§ã™ã‹ï¼Ÿ",
                "answer": "ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³æ©Ÿæ§‹ã«ã‚ˆã‚Šä¸¦åˆ—å‡¦ç†ãŒå¯èƒ½ã ã‹ã‚‰ã§ã™ã€‚",
                "reasoning": "ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ã¯ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³æ©Ÿæ§‹ã‚’ä½¿ç”¨ â†’ é †æ¬¡å‡¦ç†ä¸è¦ â†’ ä¸¦åˆ—åŒ–å¯èƒ½",
                "confidence": 0.92,
                "question_type": "reason",
                "difficulty": "intermediate"
            }
        ]

        print(f"\nï¼ˆã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼‰{len(cot_qas)}å€‹ã®CoT Q/Aã‚’ç”Ÿæˆ")

        return cot_qas


def demonstrate_hybrid_generation(document_text: str, rule_qas: List[Dict], template_qas: List[Dict]):
    """HybridQAGeneratorã‚¯ãƒ©ã‚¹ã®ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³"""

    print("\n\n" + "=" * 80)
    print("7. ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰Q/Aç”Ÿæˆï¼ˆçµ±åˆï¼‰")
    print("=" * 80)

    # å…¨Q/Aã‚’çµ±åˆ
    all_qas = []
    all_qas.extend(rule_qas)
    all_qas.extend(template_qas)

    print(f"\nçµ±åˆçµæœ:")
    print(f"  - ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹: {len(rule_qas)}å€‹")
    print(f"  - ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆãƒ™ãƒ¼ã‚¹: {len(template_qas)}å€‹")
    print(f"  - åˆè¨ˆ: {len(all_qas)}å€‹")

    # ç°¡æ˜“çš„ãªé‡è¤‡é™¤å»
    unique_questions = {}
    for qa in all_qas:
        q = qa['question']
        if q not in unique_questions:
            unique_questions[q] = qa

    unique_qas = list(unique_questions.values())

    print(f"\né‡è¤‡é™¤å»å¾Œ: {len(unique_qas)}å€‹")

    # çµ±åˆã•ã‚ŒãŸQ/Aãƒšã‚¢ã®è¡¨ç¤º
    print(f"\nã€çµ±åˆQ/Aãƒšã‚¢ï¼ˆæœ€åˆã®3å€‹ï¼‰ã€‘")
    for i, qa in enumerate(unique_qas[:3], 1):
        print(f"\n  ã€çµ±åˆQ/A {i}ã€‘")
        print(f"    è³ªå•: {qa.get('question', 'N/A')}")
        print(f"    å›ç­”: {qa.get('answer', 'N/A')[:80]}{'...' if len(qa.get('answer', '')) > 80 else ''}")
        print(f"    ã‚¿ã‚¤ãƒ—: {qa.get('type', 'N/A')}")
        print(f"    ä¿¡é ¼åº¦: {qa.get('confidence', 'N/A')}")

    return unique_qas


def demonstrate_advanced_techniques(document_text: str):
    """AdvancedQAGenerationTechniquesã‚¯ãƒ©ã‚¹ã®ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³"""

    print("\n\n" + "=" * 80)
    print("8. é«˜åº¦ãªQ/Aç”ŸæˆæŠ€è¡“")
    print("=" * 80)

    api_key = os.getenv('OPENAI_API_KEY')

    if api_key:
        try:
            print("\nAdvancedQAGenerationTechniques ã§Q/Aç”Ÿæˆä¸­...")
            advanced_gen = AdvancedQAGenerationTechniques()

            # æ•µå¯¾çš„Q/Aç”Ÿæˆï¼ˆæ—¢å­˜ã®Q/Aãƒšã‚¢ãŒå¿…è¦ãªãŸã‚ã€ç°¡æ˜“çš„ãªã‚µãƒ³ãƒ—ãƒ«ã‚’ä½œæˆï¼‰
            print("\næ•µå¯¾çš„Q/Aç”Ÿæˆä¸­...")
            sample_qa = [{"question": "RAGã¨ã¯ä½•ã§ã™ã‹ï¼Ÿ", "answer": "æ¤œç´¢æ‹¡å¼µç”Ÿæˆã§ã™"}]
            adversarial_qas = advanced_gen.generate_adversarial_qa(document_text, existing_qa=sample_qa)

            print(f"\nâœ… {len(adversarial_qas)}å€‹ã®é«˜åº¦ãªQ/Aã‚’ç”Ÿæˆã—ã¾ã—ãŸ")
            for i, qa in enumerate(adversarial_qas[:2], 1):
                print(f"\n  ã€é«˜åº¦ãªQ/A {i}ã€‘")
                print(f"    è³ªå•: {qa.get('question', 'N/A')}")
                print(f"    å›ç­”: {qa.get('answer', 'N/A')[:80]}...")
                print(f"    ã‚¿ã‚¤ãƒ—: {qa.get('type', 'N/A')}")

            return adversarial_qas

        except Exception as e:
            print(f"\nâš ï¸  é«˜åº¦ãªQ/Aç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
            return []
    else:
        print("\nâš ï¸  OpenAI APIã‚­ãƒ¼ãŒå¿…è¦ã§ã™ï¼ˆã‚¹ã‚­ãƒƒãƒ—ï¼‰")
        print("å®Ÿéš›ã®ä½¿ç”¨ä¾‹:")
        print("""
    # OpenAI APIã‚­ãƒ¼è¨­å®šå¾Œ
    advanced_gen = AdvancedQAGenerationTechniques(model="gpt-4o-mini", api_key=os.getenv("OPENAI_API_KEY"))

    # æ•µå¯¾çš„Q/Aç”Ÿæˆ
    adversarial_qas = advanced_gen.generate_adversarial_qa(document_text, num_pairs=3)

    # ãƒãƒ«ãƒãƒ›ãƒƒãƒ—æ¨è«–Q/Aç”Ÿæˆ
    multihop_qas = advanced_gen.generate_multihop_qa(document_text, chunks, num_pairs=2)

    # åäº‹å®Ÿçš„Q/Aç”Ÿæˆ
    counterfactual_qas = advanced_gen.generate_counterfactual_qa(document_text, num_pairs=2)
    """)

        # ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿
        advanced_qas = [
            {
                "question": "ã‚‚ã—ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ãŒé–‹ç™ºã•ã‚Œã¦ã„ãªã‹ã£ãŸã‚‰ã€NLPã¯ã©ã†ãªã£ã¦ã„ã¾ã—ãŸã‹ï¼Ÿ",
                "answer": "RNNãƒ™ãƒ¼ã‚¹ã®ãƒ¢ãƒ‡ãƒ«ãŒä¸»æµã®ã¾ã¾ã€å‡¦ç†é€Ÿåº¦ã¨ç²¾åº¦ã®ä¸¡ç«‹ãŒå›°é›£ã ã£ãŸã§ã—ã‚‡ã†ã€‚",
                "type": "counterfactual",
                "difficulty": "advanced"
            }
        ]

        print(f"\nï¼ˆã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼‰{len(advanced_qas)}å€‹ã®é«˜åº¦ãªQ/Aã‚’ç”Ÿæˆ")

        return advanced_qas


def demonstrate_coverage_optimization(analyzer, chunks, document_text: str, all_qas: List[Dict]):
    """QAGenerationOptimizerã‚¯ãƒ©ã‚¹ã®ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³"""

    print("\n\n" + "=" * 80)
    print("9. ã‚«ãƒãƒ¬ãƒƒã‚¸æœ€é©åŒ–")
    print("=" * 80)

    api_key = os.getenv('OPENAI_API_KEY')

    if api_key and analyzer.has_api_key:
        try:
            print("\nã‚«ãƒãƒ¬ãƒƒã‚¸åˆ†æã‚’å®Ÿè¡Œä¸­...")

            # æ–‡æ›¸ã®åŸ‹ã‚è¾¼ã¿ã‚’ç”Ÿæˆ
            doc_embeddings = analyzer.generate_embeddings(chunks)

            # Q/Aãƒšã‚¢ã®åŸ‹ã‚è¾¼ã¿ã‚’ç”Ÿæˆ
            qa_texts = [qa.get('question', '') + ' ' + qa.get('answer', '') for qa in all_qas if qa.get('question') and qa.get('answer')]

            if not qa_texts:
                print("\nâš ï¸  æœ‰åŠ¹ãªQ/Aãƒšã‚¢ãŒã‚ã‚Šã¾ã›ã‚“")
                return

            # å„Q/Aã®åŸ‹ã‚è¾¼ã¿ã‚’ç”Ÿæˆ
            qa_embeddings = []
            for qa_text in qa_texts:
                emb = analyzer.generate_embedding(qa_text)
                qa_embeddings.append(emb)

            # ã‚«ãƒãƒ¬ãƒƒã‚¸ã‚’è¨ˆç®—ï¼ˆé–¾å€¤0.7ä»¥ä¸Šã§ã€Œã‚«ãƒãƒ¼ã•ã‚Œã¦ã„ã‚‹ã€ã¨åˆ¤å®šï¼‰
            threshold = 0.7
            covered_chunks = set()

            for qa_emb in qa_embeddings:
                for i, doc_emb in enumerate(doc_embeddings):
                    similarity = analyzer.cosine_similarity(doc_emb, qa_emb)
                    if similarity >= threshold:
                        covered_chunks.add(i)

            coverage_rate = len(covered_chunks) / len(chunks) if len(chunks) > 0 else 0

            print(f"\nâœ… ã‚«ãƒãƒ¬ãƒƒã‚¸åˆ†æå®Œäº†")
            print(f"  ç·ãƒãƒ£ãƒ³ã‚¯æ•°: {len(chunks)}")
            print(f"  ã‚«ãƒãƒ¼ã•ã‚ŒãŸãƒãƒ£ãƒ³ã‚¯: {len(covered_chunks)}")
            print(f"  ã‚«ãƒãƒ¬ãƒƒã‚¸ç‡: {coverage_rate:.1%}")
            print(f"  ç·Q/Aæ•°: {len(all_qas)}")

            if coverage_rate < 0.8:
                uncovered_count = len(chunks) - len(covered_chunks)
                print(f"\nğŸ’¡ æ¨å¥¨: ã‚«ãƒãƒ¼ã•ã‚Œã¦ã„ãªã„ãƒãƒ£ãƒ³ã‚¯ãŒ{uncovered_count}å€‹ã‚ã‚Šã¾ã™")
                print(f"   è¿½åŠ ã§{uncovered_count * 2}å€‹ç¨‹åº¦ã®Q/Aãƒšã‚¢ç”Ÿæˆã‚’æ¨å¥¨ã—ã¾ã™")

        except Exception as e:
            print(f"\nâš ï¸  ã‚«ãƒãƒ¬ãƒƒã‚¸åˆ†æä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
            print("    ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãƒ¢ãƒ¼ãƒ‰ã«åˆ‡ã‚Šæ›¿ãˆã¾ã™")
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã¨ã—ã¦ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³çµæœã‚’è¡¨ç¤º
            print(f"\nï¼ˆã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼‰ã‚«ãƒãƒ¬ãƒƒã‚¸æœ€é©åŒ–çµæœ:")
            print(f"  åˆæœŸã‚«ãƒãƒ¬ãƒƒã‚¸: 65.0%")
            print(f"  æœ€çµ‚ã‚«ãƒãƒ¬ãƒƒã‚¸: 95.0%")
            print(f"  æ”¹å–„åº¦: +30.0%")
            print(f"  æ–°è¦ç”ŸæˆQ/Aæ•°: 8å€‹")
            print(f"  ç·Q/Aæ•°: {len(all_qas) + 8}å€‹")
    else:
        print("\nâš ï¸  OpenAI APIã‚­ãƒ¼ãŒå¿…è¦ã§ã™ï¼ˆã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãƒ¢ãƒ¼ãƒ‰ï¼‰")
        print("\nå®Ÿéš›ã®ä½¿ç”¨ä¾‹:")
        print("""
    # OpenAI APIã‚­ãƒ¼è¨­å®šå¾Œ
    optimizer = QAGenerationOptimizer(analyzer=analyzer, generator=hybrid_gen)

    optimized_result = optimizer.optimize_coverage(
        document_text=document_text,
        existing_qa_pairs=all_qas,
        target_coverage=0.95,
        max_iterations=5
    )

    print(f"åˆæœŸã‚«ãƒãƒ¬ãƒƒã‚¸: {optimized_result['initial_coverage']:.2%}")
    print(f"æœ€çµ‚ã‚«ãƒãƒ¬ãƒƒã‚¸: {optimized_result['coverage_rate']:.2%}")
    print(f"æ”¹å–„åº¦: +{optimized_result['improvement']:.2%}")
    """)

        # ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³çµæœ
        print(f"\nï¼ˆã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼‰ã‚«ãƒãƒ¬ãƒƒã‚¸æœ€é©åŒ–çµæœ:")
        print(f"  åˆæœŸã‚«ãƒãƒ¬ãƒƒã‚¸: 65.0%")
        print(f"  æœ€çµ‚ã‚«ãƒãƒ¬ãƒƒã‚¸: 95.0%")
        print(f"  æ”¹å–„åº¦: +30.0%")
        print(f"  æ–°è¦ç”ŸæˆQ/Aæ•°: 8å€‹")
        print(f"  ç·Q/Aæ•°: {len(all_qas) + 8}å€‹")


def export_results(all_qas: List[Dict], output_file: str = "a03_qa_results.json"):
    """ç”Ÿæˆã•ã‚ŒãŸQ/Aãƒšã‚¢ã‚’ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ"""

    print("\n\n" + "=" * 80)
    print("10. çµæœã®ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ")
    print("=" * 80)

    export_data = {
        "total_qa_pairs": len(all_qas),
        "generation_methods": {
            "rule_based": len([qa for qa in all_qas if qa.get('type') == 'definition' or qa.get('type') == 'terminology']),
            "template_based": len([qa for qa in all_qas if qa.get('type') == 'entity_based']),
            "llm_based": len([qa for qa in all_qas if qa.get('question_type') in ['fact', 'reason']]),
        },
        "qa_pairs": all_qas
    }

    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(export_data, f, ensure_ascii=False, indent=2)

    print(f"\nâœ… Q/Aç”Ÿæˆçµæœã‚’ {output_file} ã«ä¿å­˜ã—ã¾ã—ãŸ")
    print(f"ç·Q/Aæ•°: {export_data['total_qa_pairs']}")
    print(f"ç”Ÿæˆæ‰‹æ³•åˆ¥:")
    for method, count in export_data['generation_methods'].items():
        print(f"  - {method}: {count}å€‹")

def print_doc():
    print("""
        1. document_text (str)
          - å‹: str
          - å†…å®¹: RAGã‚·ã‚¹ãƒ†ãƒ ã«é–¢ã™ã‚‹æ—¥æœ¬èªãƒ†ã‚­ã‚¹ãƒˆï¼ˆ233æ–‡å­—ã€5æ–‡ï¼‰
          - å½¹å‰²: å…¨ã¦ã®Q/Aç”Ÿæˆå‡¦ç†ã®å…¥åŠ›ã¨ãªã‚‹å…ƒãƒ†ã‚­ã‚¹ãƒˆ
            """)
    print("document_text =:", document_text)

    print("""
        2. chunks (List[Dict])

          - å‹: List[Dict[str, Any]]
          - è¦ç´ æ•°: 1å€‹ï¼ˆã“ã®ã‚µãƒ³ãƒ—ãƒ«ã§ã¯å…¨æ–‡ãŒ1ãƒãƒ£ãƒ³ã‚¯ã«åã¾ã‚‹ï¼‰
          - å„ãƒãƒ£ãƒ³ã‚¯ã®æ§‹é€ :
          {
              "id": str,                    # "chunk_0"
              "text": str,                  # ãƒãƒ£ãƒ³ã‚¯å…¨ä½“ã®ãƒ†ã‚­ã‚¹ãƒˆ
              "sentences": List[str],       # å€‹åˆ¥ã®æ–‡ã®ãƒªã‚¹ãƒˆ
              "start_sentence_idx": int,    # é–‹å§‹æ–‡ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ (0)
              "end_sentence_idx": int       # çµ‚äº†æ–‡ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ (4)
          }
          - å½¹å‰²: æ–‡æ›¸ã‚’æ„å‘³çš„ã«åˆ†å‰²ã—ãŸå˜ä½ã€‚ã‚«ãƒãƒ¬ãƒƒã‚¸åˆ†æã§ä½¿ç”¨

        3. analyzer (SemanticCoverage)

          - å‹: SemanticCoverage (helper_rag_qa.SemanticCoverage)
          - ä¸»è¦å±æ€§:
            - embedding_model: "text-embedding-3-small"
            - has_api_key: True
            - client: OpenAI()
          - ä¸»è¦ãƒ¡ã‚½ãƒƒãƒ‰:
            - create_semantic_chunks(text) â†’ List[Dict]
            - generate_embeddings(chunks) â†’ List[np.ndarray]
            - generate_embedding(text) â†’ np.ndarray
            - cosine_similarity(vec1, vec2) â†’ float
          - å½¹å‰²: ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯åˆ†æã®ä¸­æ ¸ã€‚ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²ã€åŸ‹ã‚è¾¼ã¿ç”Ÿæˆã€é¡ä¼¼åº¦è¨ˆç®—ã‚’å®Ÿè¡Œ

          ã“ã®3ã¤ã®å¤‰æ•°ãŒé€£æºã—ã¦ã€æ–‡æ›¸ã®ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯ã‚«ãƒãƒ¬ãƒƒã‚¸åˆ†æã‚’å®Ÿç¾ã—ã¦ã„ã¾ã™ã€‚
            """)

    print(""" -----------------------------------------------------------
    
    ã€1. create_semantic_chunks(document: str, verbose: bool = True) â†’ List[Dict]ã€‘

      èª¬æ˜: æ–‡æ›¸ã‚’æ„å‘³çš„ã«åŒºåˆ‡ã‚‰ã‚ŒãŸãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰²

      å‡¦ç†æ‰‹é †:
        1. æ–‡å˜ä½ã§åˆ†å‰²ï¼ˆ_split_into_sentences()ï¼‰
           - æ—¥æœ¬èª: ã€‚ï¼.!? ã§åˆ†å‰²
           - è‹±èª: . ! ? ã§åˆ†å‰²

        2. ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’è¨ˆç®—ã—ãªãŒã‚‰ãƒãƒ£ãƒ³ã‚¯æ§‹ç¯‰
           - max_tokens = 200 ãƒˆãƒ¼ã‚¯ãƒ³/ãƒãƒ£ãƒ³ã‚¯
           - æ–‡ã®é€”ä¸­ã§ã¯åˆ†å‰²ã—ãªã„ï¼ˆæ„å‘³ã®æ–­çµ¶ã‚’é˜²ãï¼‰

        3. ãƒˆãƒ”ãƒƒã‚¯ã®é€£ç¶šæ€§ã‚’è€ƒæ…®ã—ãŸèª¿æ•´
           - _adjust_chunks_for_topic_continuity()ã§æœ€é©åŒ–

      æˆ»ã‚Šå€¤: List[Dict]
        å„ãƒãƒ£ãƒ³ã‚¯: {
          "id": "chunk_0",
          "text": "ãƒãƒ£ãƒ³ã‚¯å…¨ä½“ã®ãƒ†ã‚­ã‚¹ãƒˆ",
          "sentences": ["æ–‡1", "æ–‡2", ...],
          "start_sentence_idx": 0,
          "end_sentence_idx": 2
        }

    ã€2. generate_embeddings(chunks: List[Dict]) â†’ List[np.ndarray]ã€‘

      èª¬æ˜: è¤‡æ•°ãƒãƒ£ãƒ³ã‚¯ã®åŸ‹ã‚è¾¼ã¿ã‚’ãƒãƒƒãƒç”Ÿæˆ

      å‡¦ç†æ‰‹é †:
        1. å„ãƒãƒ£ãƒ³ã‚¯ã®textãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’æŠ½å‡º
        2. OpenAI Embeddings APIã«ä¸€æ‹¬é€ä¿¡
           client.embeddings.create(
             input=[chunk["text"] for chunk in chunks],
             model=self.embedding_model
           )
        3. è¿”å´ã•ã‚ŒãŸãƒ™ã‚¯ãƒˆãƒ«ã‚’L2æ­£è¦åŒ–

      æˆ»ã‚Šå€¤: List[np.ndarray]
        å„ãƒ™ã‚¯ãƒˆãƒ«: 1536æ¬¡å…ƒã®numpyé…åˆ—

    ã€3. generate_embedding(text: str) â†’ np.ndarrayã€‘

      èª¬æ˜: å˜ä¸€ãƒ†ã‚­ã‚¹ãƒˆã®åŸ‹ã‚è¾¼ã¿ç”Ÿæˆ

      å‡¦ç†æ‰‹é †:
        1. ãƒ†ã‚­ã‚¹ãƒˆã‚’OpenAI APIã«é€ä¿¡
        2. åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«å–å¾—
        3. L2æ­£è¦åŒ–ã—ã¦è¿”å´

      æˆ»ã‚Šå€¤: np.ndarray (1536æ¬¡å…ƒ)

      ç”¨é€”: Q/Aãƒšã‚¢ã®åŸ‹ã‚è¾¼ã¿ç”Ÿæˆ

    ã€4. cosine_similarity(vec1: np.ndarray, vec2: np.ndarray) â†’ floatã€‘

      èª¬æ˜: 2ã¤ã®ãƒ™ã‚¯ãƒˆãƒ«é–“ã®ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ã‚’è¨ˆç®—

      è¨ˆç®—å¼:
        similarity = np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))

      æˆ»ã‚Šå€¤: float (0.0ã€œ1.0)
        - 1.0: å®Œå…¨ã«ä¸€è‡´
        - 0.7ä»¥ä¸Š: é«˜ã„é¡ä¼¼æ€§ï¼ˆã‚«ãƒãƒ¬ãƒƒã‚¸åˆ¤å®šã®é–¾å€¤ï¼‰
        - 0.0: å…¨ãé–¢é€£æ€§ãªã—

      ç”¨é€”: Q/Aãƒšã‚¢ã¨æ–‡æ›¸ãƒãƒ£ãƒ³ã‚¯ã®é–¢é€£æ€§è©•ä¾¡
        """)

# ==========================================
# æ–°è¦è¿½åŠ : ãƒ•ã‚¡ã‚¤ãƒ«å…¥åŠ›å‡¦ç†ã¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆè¨­å®š
# ==========================================

DATASET_CONFIGS = {
    "cc_news": {
        "name": "CC-Newsè‹±èªãƒ‹ãƒ¥ãƒ¼ã‚¹",
        "text_column": "Combined_Text",
        "title_column": "title",
        "lang": "en"
    },
    "japanese_text": {
        "name": "æ—¥æœ¬èªWebãƒ†ã‚­ã‚¹ãƒˆ",
        "text_column": "Combined_Text",
        "title_column": None,
        "lang": "ja"
    },
    "wikipedia_ja": {
        "name": "Wikipediaæ—¥æœ¬èªç‰ˆ",
        "text_column": "Combined_Text",
        "title_column": "title",
        "lang": "ja"
    }
}


def load_input_data(input_file: str, dataset_type: Optional[str] = None, max_docs: Optional[int] = None) -> str:
    """
    å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿
    Args:
        input_file: å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ï¼ˆCSVã€TXTã€JSONå¯¾å¿œï¼‰
        dataset_type: ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚¿ã‚¤ãƒ—ï¼ˆCSVã®å ´åˆã«ä½¿ç”¨ï¼‰
        max_docs: å‡¦ç†ã™ã‚‹æœ€å¤§æ–‡æ›¸æ•°
    Returns:
        å‡¦ç†å¯¾è±¡ãƒ†ã‚­ã‚¹ãƒˆï¼ˆçµåˆæ¸ˆã¿ï¼‰
    """
    file_path = Path(input_file)
    if not file_path.exists():
        raise FileNotFoundError(f"å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {input_file}")

    logger.info(f"ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ä¸­: {input_file}")

    # ãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼ã«å¿œã˜ã¦å‡¦ç†
    if file_path.suffix.lower() == '.csv':
        # CSVãƒ•ã‚¡ã‚¤ãƒ«ã®å ´åˆ
        df = pd.read_csv(file_path)

        # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆè¨­å®šã‚’é©ç”¨
        if dataset_type and dataset_type in DATASET_CONFIGS:
            config = DATASET_CONFIGS[dataset_type]
            text_col = config["text_column"]

            if text_col not in df.columns:
                # "text"ã‚«ãƒ©ãƒ ãŒã‚ã‚‹å ´åˆã¯ãã‚Œã‚’ä½¿ç”¨
                if "text" in df.columns:
                    text_col = "text"
                else:
                    raise ValueError(f"ãƒ†ã‚­ã‚¹ãƒˆã‚«ãƒ©ãƒ  '{text_col}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")

            # æ–‡æ›¸æ•°åˆ¶é™
            if max_docs:
                df = df.head(max_docs)

            # ãƒ†ã‚­ã‚¹ãƒˆã‚’çµåˆ
            texts = df[text_col].dropna().tolist()
            combined_text = "\n\n".join([str(t) for t in texts])

            logger.info(f"èª­ã¿è¾¼ã¿å®Œäº†: {len(texts)}ä»¶ã®æ–‡æ›¸")

        else:
            # dataset_typeæœªæŒ‡å®šã®å ´åˆã€æœ€åˆã®ãƒ†ã‚­ã‚¹ãƒˆã‚«ãƒ©ãƒ ã‚’ä½¿ç”¨
            text_cols = [col for col in df.columns if 'text' in col.lower() or 'content' in col.lower()]
            if text_cols:
                text_col = text_cols[0]
            else:
                # ãƒ†ã‚­ã‚¹ãƒˆã‚‰ã—ã„ã‚«ãƒ©ãƒ ãŒãªã„å ´åˆã¯å…¨ã‚«ãƒ©ãƒ ã‚’çµåˆ
                text_col = df.columns[0]

            if max_docs:
                df = df.head(max_docs)

            texts = df[text_col].dropna().tolist()
            combined_text = "\n\n".join([str(t) for t in texts])

    elif file_path.suffix.lower() == '.json':
        # JSONãƒ•ã‚¡ã‚¤ãƒ«ã®å ´åˆ
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)

        if isinstance(data, list):
            # ãƒªã‚¹ãƒˆå½¢å¼ã®å ´åˆ
            if max_docs:
                data = data[:max_docs]

            # textãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’æ¢ã™
            texts = []
            for item in data:
                if isinstance(item, dict):
                    if 'text' in item:
                        texts.append(item['text'])
                    elif 'content' in item:
                        texts.append(item['content'])
                    else:
                        # æœ€åˆã®æ–‡å­—åˆ—ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’ä½¿ç”¨
                        for v in item.values():
                            if isinstance(v, str):
                                texts.append(v)
                                break
                else:
                    texts.append(str(item))

            combined_text = "\n\n".join(texts)

        else:
            combined_text = str(data)

    else:
        # ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦æ‰±ã†
        with open(file_path, 'r', encoding='utf-8') as f:
            combined_text = f.read()

        # max_docsãŒæŒ‡å®šã•ã‚Œã¦ã„ã‚‹å ´åˆã€æ®µè½ã§åŒºåˆ‡ã‚‹
        if max_docs:
            paragraphs = combined_text.split('\n\n')
            paragraphs = paragraphs[:max_docs]
            combined_text = '\n\n'.join(paragraphs)

    return combined_text


def save_results(
    qa_pairs: List[Dict],
    coverage_results: Optional[Dict] = None,
    dataset_type: str = "custom",
    output_dir: str = "qa_output"
) -> Dict[str, str]:
    """
    çµæœã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ï¼ˆa02_make_qa.pyã¨åŒã˜å½¢å¼ï¼‰
    """
    output_path = Path(output_dir)
    output_path.mkdir(exist_ok=True)

    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

    # Q/Aãƒšã‚¢ã‚’ä¿å­˜ï¼ˆJSONï¼‰
    qa_file = output_path / f"qa_pairs_{dataset_type}_{timestamp}.json"
    with open(qa_file, 'w', encoding='utf-8') as f:
        json.dump(qa_pairs, f, ensure_ascii=False, indent=2)

    # Q/Aãƒšã‚¢ã‚’ä¿å­˜ï¼ˆCSVï¼‰
    qa_csv_file = output_path / f"qa_pairs_{dataset_type}_{timestamp}.csv"
    qa_df = pd.DataFrame(qa_pairs)
    qa_df.to_csv(qa_csv_file, index=False, encoding='utf-8')

    saved_files = {
        "qa_json": str(qa_file),
        "qa_csv": str(qa_csv_file)
    }

    # ã‚«ãƒãƒ¬ãƒƒã‚¸åˆ†æçµæœãŒã‚ã‚‹å ´åˆã¯ä¿å­˜
    if coverage_results:
        coverage_file = output_path / f"coverage_{dataset_type}_{timestamp}.json"
        with open(coverage_file, 'w', encoding='utf-8') as f:
            json.dump(coverage_results, f, ensure_ascii=False, indent=2)
        saved_files["coverage"] = str(coverage_file)

    # ã‚µãƒãƒªãƒ¼æƒ…å ±ã‚’ä¿å­˜
    summary = {
        "dataset_type": dataset_type,
        "generated_at": timestamp,
        "total_qa_pairs": len(qa_pairs),
        "files": saved_files
    }

    if coverage_results:
        summary["coverage_rate"] = coverage_results.get('coverage_rate', 0)

    summary_file = output_path / f"summary_{dataset_type}_{timestamp}.json"
    with open(summary_file, 'w', encoding='utf-8') as f:
        json.dump(summary, f, ensure_ascii=False, indent=2)

    saved_files["summary"] = str(summary_file)

    logger.info(f"çµæœã‚’ä¿å­˜ã—ã¾ã—ãŸ: {output_path}")

    return saved_files


def generate_qa_for_chunk(chunk_text: str, num_qa: int = 2) -> List[Dict]:
    """
    å˜ä¸€ãƒãƒ£ãƒ³ã‚¯ã«å¯¾ã—ã¦Q/Aã‚’ç”Ÿæˆï¼ˆãƒãƒ£ãƒ³ã‚¯ãƒ™ãƒ¼ã‚¹æ”¹è‰¯ç‰ˆï¼‰
    """
    qas = []

    # è‹±èª/æ—¥æœ¬èªã®ç°¡æ˜“åˆ¤å®š
    is_english = any(word in chunk_text[:100] for word in ['the', 'The', 'is', 'are', 'was'])

    if is_english:
        # è‹±èªç”¨ã®åŸºæœ¬çš„ãªãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒãƒ³ã‚°
        sentences = chunk_text.split('. ')[:5]  # æœ€åˆã®5æ–‡
        for i, sent in enumerate(sentences[:num_qa]):
            if len(sent) > 20:
                # Whatå‹ã®è³ªå•
                qa = {
                    'question': f"What is described in this passage about: {sent[:30]}...?",
                    'answer': sent,
                    'type': 'factual'
                }
                qas.append(qa)

                # Why/Howå‹ã®è³ªå•ï¼ˆå¯èƒ½ãªã‚‰ï¼‰
                if 'because' in sent.lower() or 'due to' in sent.lower():
                    qa = {
                        'question': f"Why does this occur: {sent[:30]}...?",
                        'answer': sent,
                        'type': 'reasoning'
                    }
                    qas.append(qa)
    else:
        # æ—¥æœ¬èªç”¨ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒãƒ³ã‚°
        sentences = chunk_text.split('ã€‚')[:5]
        for i, sent in enumerate(sentences[:num_qa]):
            if len(sent) > 10:
                qa = {
                    'question': f"{sent[:20]}...ã«ã¤ã„ã¦èª¬æ˜ã—ã¦ãã ã•ã„",
                    'answer': sent,
                    'type': 'factual'
                }
                qas.append(qa)

    return qas[:num_qa]  # æŒ‡å®šã•ã‚ŒãŸæ•°ã ã‘è¿”ã™


def process_with_methods(
    document_text: str,
    methods: List[str],
    model: str = "gpt-4o-mini"
) -> tuple:
    """
    æŒ‡å®šã•ã‚ŒãŸæ‰‹æ³•ã§Q/Aç”Ÿæˆã‚’å®Ÿè¡Œï¼ˆæ”¹è‰¯ç‰ˆï¼šãƒãƒ£ãƒ³ã‚¯ãƒ™ãƒ¼ã‚¹å‡¦ç†ï¼‰
    Args:
        document_text: å‡¦ç†å¯¾è±¡ãƒ†ã‚­ã‚¹ãƒˆ
        methods: ä½¿ç”¨ã™ã‚‹æ‰‹æ³•ã®ãƒªã‚¹ãƒˆ
        model: ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«
    Returns:
        (ç”Ÿæˆã•ã‚ŒãŸQ/Aãƒšã‚¢ã®ãƒªã‚¹ãƒˆ, analyzer, chunks)
    """
    all_qas = []

    # SemanticCoverageåˆæœŸåŒ–ï¼ˆãƒãƒ£ãƒ³ã‚¯ä½œæˆç”¨ï¼‰
    analyzer = SemanticCoverage(embedding_model="text-embedding-3-small")
    chunks = analyzer.create_semantic_chunks(document_text, verbose=False)
    logger.info(f"ãƒãƒ£ãƒ³ã‚¯ä½œæˆå®Œäº†: {len(chunks)}å€‹")

    # ã‚«ãƒãƒ¬ãƒƒã‚¸ç›®æ¨™ã‚’é”æˆã™ã‚‹ãŸã‚ã®æˆ¦ç•¥
    total_chunks = len(chunks)
    target_coverage = 0.8  # 80%ã‚«ãƒãƒ¬ãƒƒã‚¸ç›®æ¨™

    # å¿…è¦ãªQ/Aæ•°ã‚’æ¨å®šï¼ˆ1ã¤ã®Q/AãŒå¹³å‡2-3ãƒãƒ£ãƒ³ã‚¯ã‚’ã‚«ãƒãƒ¼ã™ã‚‹ã¨ä»®å®šï¼‰
    avg_coverage_per_qa = 2.5
    target_qa_count = int((total_chunks * target_coverage) / avg_coverage_per_qa)
    logger.info(f"ç›®æ¨™Q/Aæ•°: ç´„{target_qa_count}å€‹ï¼ˆ80%ã‚«ãƒãƒ¬ãƒƒã‚¸é”æˆç”¨ï¼‰")

    # ãƒãƒ£ãƒ³ã‚¯æ•°ãŒå¤šã„å ´åˆã¯ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
    if total_chunks > 100:
        # å‡ç­‰ã«ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼ˆæœ€å¤§200ãƒãƒ£ãƒ³ã‚¯ï¼‰
        sample_size = min(200, total_chunks)
        step = max(1, total_chunks // sample_size)
        sampled_chunks = chunks[::step][:sample_size]
        logger.info(f"å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã®ãŸã‚ã€{len(sampled_chunks)}ãƒãƒ£ãƒ³ã‚¯ã‚’ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°")
    else:
        sampled_chunks = chunks

    # å„æ‰‹æ³•ã§ãƒãƒ£ãƒ³ã‚¯ãƒ™ãƒ¼ã‚¹ã®Q/Aç”Ÿæˆ
    qa_per_method = max(2, target_qa_count // len(methods))
    chunks_per_method = min(len(sampled_chunks), qa_per_method // 2)

    # æŒ‡å®šã•ã‚ŒãŸæ‰‹æ³•ã§å‡¦ç†
    if "rule" in methods:
        logger.info(f"ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹Q/Aç”Ÿæˆä¸­...ï¼ˆ{chunks_per_method}ãƒãƒ£ãƒ³ã‚¯å‡¦ç†ï¼‰")
        for i, chunk in enumerate(sampled_chunks[:chunks_per_method]):
            chunk_qas = generate_qa_for_chunk(chunk['text'], num_qa=2)
            all_qas.extend(chunk_qas)
            if (i + 1) % 50 == 0:
                logger.info(f"  é€²æ—: {i+1}/{chunks_per_method}ãƒãƒ£ãƒ³ã‚¯")

    if "template" in methods:
        logger.info(f"ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆãƒ™ãƒ¼ã‚¹Q/Aç”Ÿæˆä¸­...ï¼ˆ{chunks_per_method}ãƒãƒ£ãƒ³ã‚¯å‡¦ç†ï¼‰")
        # ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆãƒ™ãƒ¼ã‚¹ã§è¿½åŠ Q/Aç”Ÿæˆ
        template_generator = TemplateBasedQAGenerator()
        for i, chunk in enumerate(sampled_chunks[:chunks_per_method]):
            try:
                # è¤‡æ•°ã®ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’é©ç”¨
                templates = [
                    "What is the main topic discussed in this text?",
                    "What are the key points mentioned?",
                    "What information is provided about",
                    "According to the passage, what"
                ]

                for template in templates[:2]:  # å„ãƒãƒ£ãƒ³ã‚¯ã«2ã¤ã®ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ
                    qa = {
                        'question': f"{template} {chunk['text'][:30]}...?",
                        'answer': chunk['text'][:200],
                        'type': 'template_based'
                    }
                    all_qas.append(qa)

            except Exception as e:
                logger.debug(f"ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")

            if (i + 1) % 50 == 0:
                logger.info(f"  é€²æ—: {i+1}/{chunks_per_method}ãƒãƒ£ãƒ³ã‚¯")

    if "llm" in methods:
        logger.info(f"LLMãƒ™ãƒ¼ã‚¹Q/Aç”Ÿæˆä¸­...ï¼ˆã‚³ã‚¹ãƒˆåˆ¶ç´„ã«ã‚ˆã‚Š{min(10, chunks_per_method)}ãƒãƒ£ãƒ³ã‚¯ï¼‰")
        # LLMã¯é«˜ã‚³ã‚¹ãƒˆãªã®ã§åˆ¶é™
        for chunk in sampled_chunks[:min(10, chunks_per_method)]:
            try:
                llm_qas = demonstrate_llm_based_generation(chunk['text'])
                all_qas.extend(llm_qas)
            except Exception as e:
                logger.debug(f"LLMç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")

    if "cot" in methods:
        logger.info("Chain-of-Thought Q/Aç”Ÿæˆä¸­...")
        # CoTã¯å°‘æ•°ã®é«˜å“è³ªQ/A
        for chunk in sampled_chunks[:min(5, chunks_per_method)]:
            try:
                cot_qas = demonstrate_cot_generation(chunk['text'])
                all_qas.extend(cot_qas)
            except Exception as e:
                logger.debug(f"CoTç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")

    if "advanced" in methods:
        logger.info("é«˜åº¦ãªQ/Aç”Ÿæˆä¸­...")
        try:
            advanced_qas = demonstrate_advanced_techniques(document_text[:5000])
            all_qas.extend(advanced_qas)
        except Exception as e:
            logger.debug(f"é«˜åº¦ãªç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")

    # é‡è¤‡é™¤å»
    unique_questions = {}
    for qa in all_qas:
        q = qa.get('question', '')
        if q and q not in unique_questions:
            unique_questions[q] = qa

    unique_qas = list(unique_questions.values())

    # Q/Aæ•°ãŒç›®æ¨™ã«é”ã—ãªã„å ´åˆã¯è¿½åŠ ç”Ÿæˆ
    if len(unique_qas) < target_qa_count * 0.5:
        logger.info(f"Q/Aæ•°ãŒä¸è¶³ï¼ˆ{len(unique_qas)}å€‹ï¼‰ã€‚è¿½åŠ ç”Ÿæˆä¸­...")
        additional_needed = target_qa_count - len(unique_qas)

        # æœªå‡¦ç†ã®ãƒãƒ£ãƒ³ã‚¯ã‹ã‚‰è¿½åŠ ç”Ÿæˆ
        unprocessed_start = chunks_per_method * len(methods)
        for i, chunk in enumerate(chunks[unprocessed_start:unprocessed_start + additional_needed // 2]):
            chunk_qas = generate_qa_for_chunk(chunk['text'], num_qa=2)
            unique_qas.extend(chunk_qas)
            if (i + 1) % 50 == 0:
                logger.info(f"  è¿½åŠ ç”Ÿæˆé€²æ—: {i+1}ãƒãƒ£ãƒ³ã‚¯")

    logger.info(f"Q/Aç”Ÿæˆå®Œäº†: {len(unique_qas)}å€‹ï¼ˆé‡è¤‡é™¤å»å¾Œï¼‰")

    return unique_qas, analyzer, chunks


def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°ï¼ˆã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¯¾å¿œç‰ˆï¼‰"""

    # ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°ã®ãƒ‘ãƒ¼ã‚µãƒ¼è¨­å®š
    parser = argparse.ArgumentParser(
        description="ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯ã‚«ãƒãƒ¬ãƒƒã‚¸åˆ†æã¨Q/Aç”Ÿæˆã‚·ã‚¹ãƒ†ãƒ ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«å…¥åŠ›å¯¾å¿œç‰ˆï¼‰"
    )

    # å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«é–¢é€£
    parser.add_argument(
        "--input",
        type=str,
        help="å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ï¼ˆCSVã€TXTã€JSONå¯¾å¿œï¼‰"
    )
    parser.add_argument(
        "--dataset",
        type=str,
        choices=list(DATASET_CONFIGS.keys()),
        help="ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚¿ã‚¤ãƒ—ï¼ˆcc_news, japanese_text, wikipedia_jaï¼‰"
    )
    parser.add_argument(
        "--max-docs",
        type=int,
        default=None,
        help="å‡¦ç†ã™ã‚‹æœ€å¤§æ–‡æ›¸æ•°"
    )

    # Q/Aç”Ÿæˆæ‰‹æ³•
    parser.add_argument(
        "--methods",
        type=str,
        nargs='+',
        choices=['rule', 'template', 'llm', 'cot', 'advanced', 'all'],
        default=['rule', 'template'],
        help="ä½¿ç”¨ã™ã‚‹Q/Aç”Ÿæˆæ‰‹æ³•"
    )

    # ãƒ¢ãƒ‡ãƒ«è¨­å®š
    parser.add_argument(
        "--model",
        type=str,
        default="gpt-4o-mini",
        help="ä½¿ç”¨ã™ã‚‹OpenAIãƒ¢ãƒ‡ãƒ«"
    )

    # å‡ºåŠ›è¨­å®š
    parser.add_argument(
        "--output",
        type=str,
        default="qa_output",
        help="å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª"
    )

    # åˆ†æã‚ªãƒ—ã‚·ãƒ§ãƒ³
    parser.add_argument(
        "--analyze-coverage",
        action="store_true",
        help="ã‚«ãƒãƒ¬ãƒƒã‚¸åˆ†æã‚’å®Ÿè¡Œ"
    )

    parser.add_argument(
        "--demo",
        action="store_true",
        help="ãƒ‡ãƒ¢ãƒ¢ãƒ¼ãƒ‰ï¼ˆã‚µãƒ³ãƒ—ãƒ«ãƒ†ã‚­ã‚¹ãƒˆã§å®Ÿè¡Œï¼‰"
    )

    args = parser.parse_args()

    # ãƒ˜ãƒƒãƒ€ãƒ¼è¡¨ç¤º
    print("\n" + "=" * 80)
    print("ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯ã‚«ãƒãƒ¬ãƒƒã‚¸åˆ†æã¨Q/Aç”Ÿæˆã‚·ã‚¹ãƒ†ãƒ ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«å…¥åŠ›å¯¾å¿œç‰ˆï¼‰")
    print("=" * 80)

    # ç’°å¢ƒãƒã‚§ãƒƒã‚¯
    api_key = os.getenv('OPENAI_API_KEY')
    print(f"\nğŸ“‹ ç’°å¢ƒãƒã‚§ãƒƒã‚¯:")
    print(f"  OpenAI APIã‚­ãƒ¼: {'âœ… è¨­å®šæ¸ˆã¿' if api_key else 'âŒ æœªè¨­å®š'}")
    if api_key:
        print(f"  å‹•ä½œãƒ¢ãƒ¼ãƒ‰: ãƒ•ãƒ«æ©Ÿèƒ½")
    else:
        print(f"  å‹•ä½œãƒ¢ãƒ¼ãƒ‰: ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆãƒ«ãƒ¼ãƒ«/ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã®ã¿ï¼‰")

    # ãƒ‡ãƒ¢ãƒ¢ãƒ¼ãƒ‰ã¾ãŸã¯ãƒ•ã‚¡ã‚¤ãƒ«å…¥åŠ›ãƒ¢ãƒ¼ãƒ‰
    if args.demo or not args.input:
        # ãƒ‡ãƒ¢ãƒ¢ãƒ¼ãƒ‰: ã‚µãƒ³ãƒ—ãƒ«ãƒ†ã‚­ã‚¹ãƒˆã§å®Ÿè¡Œ
        print("\nğŸ“ ãƒ‡ãƒ¢ãƒ¢ãƒ¼ãƒ‰: ã‚µãƒ³ãƒ—ãƒ«ãƒ†ã‚­ã‚¹ãƒˆã§å®Ÿè¡Œ")
        document_text = """
        RAGã‚·ã‚¹ãƒ†ãƒ ã¯ã€Retrieval-Augmented Generationã®ç•¥ã§ã€æ¤œç´¢æ‹¡å¼µç”Ÿæˆã¨å‘¼ã°ã‚Œã¾ã™ã€‚
        ã“ã®ã‚·ã‚¹ãƒ†ãƒ ã¯ã€å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ã¨æƒ…å ±æ¤œç´¢ã‚’çµ„ã¿åˆã‚ã›ãŸæŠ€è¡“ã§ã™ã€‚
        RAGã®ä¸»ãªåˆ©ç‚¹ã¯ã€å¤–éƒ¨çŸ¥è­˜ãƒ™ãƒ¼ã‚¹ã‹ã‚‰é–¢é€£æƒ…å ±ã‚’å–å¾—ã—ã€ã‚ˆã‚Šæ­£ç¢ºãªå›ç­”ã‚’ç”Ÿæˆã§ãã‚‹ã“ã¨ã§ã™ã€‚
        Qdrantã¯ãƒ™ã‚¯ãƒˆãƒ«ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã§ã‚ã‚Šã€é«˜é€Ÿãªé¡ä¼¼åº¦æ¤œç´¢ã‚’å®Ÿç¾ã—ã¾ã™ã€‚
        OpenAIã®text-embedding-3-smallãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ã¦ã€ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒ™ã‚¯ãƒˆãƒ«è¡¨ç¾ã«å¤‰æ›ã—ã¾ã™ã€‚
        """
        dataset_type = "demo"

    else:
        # ãƒ•ã‚¡ã‚¤ãƒ«å…¥åŠ›ãƒ¢ãƒ¼ãƒ‰
        print(f"\nğŸ“ å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«: {args.input}")
        print(f"  ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ: {args.dataset if args.dataset else 'è‡ªå‹•æ¤œå‡º'}")
        print(f"  æœ€å¤§æ–‡æ›¸æ•°: {args.max_docs if args.max_docs else 'åˆ¶é™ãªã—'}")

        try:
            # ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
            document_text = load_input_data(
                args.input,
                args.dataset,
                args.max_docs
            )
            dataset_type = args.dataset if args.dataset else "custom"

        except Exception as e:
            logger.error(f"ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            sys.exit(1)

    # Q/Aç”Ÿæˆæ‰‹æ³•ã®æ±ºå®š
    methods = args.methods
    if 'all' in methods:
        methods = ['rule', 'template', 'llm', 'cot', 'advanced']

    print(f"\nğŸ› ï¸  ä½¿ç”¨ã™ã‚‹æ‰‹æ³•: {', '.join(methods)}")
    print(f"  ãƒ¢ãƒ‡ãƒ«: {args.model}")
    print(f"  å‡ºåŠ›å…ˆ: {args.output}")
    print(f"  ã‚«ãƒãƒ¬ãƒƒã‚¸åˆ†æ: {'å®Ÿè¡Œ' if args.analyze_coverage else 'ã‚¹ã‚­ãƒƒãƒ—'}")

    print("\n" + "=" * 80)
    print("å‡¦ç†é–‹å§‹")
    print("=" * 80)

    try:
        # Q/Aç”Ÿæˆå‡¦ç†
        qa_pairs, analyzer, chunks = process_with_methods(
            document_text,
            methods,
            args.model
        )

        # ã‚«ãƒãƒ¬ãƒƒã‚¸åˆ†æï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
        coverage_results = None
        if args.analyze_coverage and qa_pairs:
            print("\n" + "=" * 80)
            print("ã‚«ãƒãƒ¬ãƒƒã‚¸åˆ†æ")
            print("=" * 80)

            try:
                # ã‚«ãƒãƒ¬ãƒƒã‚¸è¨ˆç®—
                coverage_results = {}

                # åŸ‹ã‚è¾¼ã¿ç”Ÿæˆã¨ã‚«ãƒãƒ¬ãƒƒã‚¸è¨ˆç®—
                if api_key and analyzer.has_api_key:
                    doc_embeddings = analyzer.generate_embeddings(chunks)

                    qa_texts = [
                        qa.get('question', '') + ' ' + qa.get('answer', '')
                        for qa in qa_pairs
                        if qa.get('question') and qa.get('answer')
                    ]

                    if qa_texts:
                        qa_embeddings = []
                        for qa_text in qa_texts:
                            emb = analyzer.generate_embedding(qa_text)
                            qa_embeddings.append(emb)

                        # ã‚«ãƒãƒ¬ãƒƒã‚¸è¨ˆç®—
                        threshold = 0.7
                        covered_chunks = set()

                        for qa_emb in qa_embeddings:
                            for i, doc_emb in enumerate(doc_embeddings):
                                similarity = analyzer.cosine_similarity(doc_emb, qa_emb)
                                if similarity >= threshold:
                                    covered_chunks.add(i)

                        coverage_rate = len(covered_chunks) / len(chunks) if chunks else 0

                        coverage_results = {
                            "coverage_rate": coverage_rate,
                            "covered_chunks": len(covered_chunks),
                            "total_chunks": len(chunks),
                            "threshold": threshold
                        }

                        print(f"\nğŸ“Š ã‚«ãƒãƒ¬ãƒƒã‚¸åˆ†æçµæœ:")
                        print(f"  ã‚«ãƒãƒ¬ãƒƒã‚¸ç‡: {coverage_rate:.1%}")
                        print(f"  ã‚«ãƒãƒ¼æ¸ˆã¿ãƒãƒ£ãƒ³ã‚¯: {len(covered_chunks)}/{len(chunks)}")
                        print(f"  é–¾å€¤: {threshold}")

            except Exception as e:
                logger.warning(f"ã‚«ãƒãƒ¬ãƒƒã‚¸åˆ†æä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")

        # çµæœä¿å­˜
        saved_files = save_results(
            qa_pairs,
            coverage_results,
            dataset_type,
            args.output
        )

        # å®Œäº†ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
        print("\n" + "=" * 80)
        print("å‡¦ç†å®Œäº†")
        print("=" * 80)
        print(f"\nâœ… ç”Ÿæˆã•ã‚ŒãŸQ/Aãƒšã‚¢æ•°: {len(qa_pairs)}")
        print(f"âœ… ä¿å­˜ãƒ•ã‚¡ã‚¤ãƒ«:")
        for file_type, file_path in saved_files.items():
            print(f"  - {file_type}: {file_path}")

        # çµ±è¨ˆæƒ…å ±
        if qa_pairs:
            print(f"\nğŸ“Š Q/Aãƒšã‚¢çµ±è¨ˆ:")
            type_counts = {}
            for qa in qa_pairs:
                qa_type = qa.get('type', qa.get('question_type', 'unknown'))
                type_counts[qa_type] = type_counts.get(qa_type, 0) + 1

            for qa_type, count in sorted(type_counts.items()):
                print(f"  - {qa_type}: {count}ä»¶")

    except Exception as e:
        logger.error(f"å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()

