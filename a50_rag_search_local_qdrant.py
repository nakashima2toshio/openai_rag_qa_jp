#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
a50_rag_search_local_qdrant.py â€” Qdrant RAGæ¤œç´¢ç”¨Streamlit UI
------------------------------------------------------------------------------
æ©Ÿèƒ½æ¦‚è¦:
  - è¤‡æ•°ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³å¯¾å¿œï¼ˆproduct_embeddings, qa_corpus, qa_cc_news_*ç­‰ï¼‰
  - ãƒ‰ãƒ¡ã‚¤ãƒ³åˆ¥æ¤œç´¢ï¼ˆcustomer, medical, sciq, legal, triviaï¼‰
  - Named Vectorsåˆ‡æ›¿ï¼ˆada-002, 3-smallç­‰ï¼‰
  - å‹•çš„ãªåŸ‹ã‚è¾¼ã¿æ¬¡å…ƒå¯¾å¿œï¼ˆ384æ¬¡å…ƒã€1536æ¬¡å…ƒï¼‰
  - ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ é¡ä¼¼åº¦ã‚¹ã‚³ã‚¢è¡¨ç¤º
  - OpenAI GPT-4o-miniã«ã‚ˆã‚‹æ—¥æœ¬èªå›ç­”ç”Ÿæˆ

å¯¾å¿œãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ:
  - ã‚«ã‚¹ã‚¿ãƒãƒ¼ã‚µãƒãƒ¼ãƒˆãƒ»FAQ (customer)
  - åŒ»ç™‚QAãƒ‡ãƒ¼ã‚¿ (medical)
  - ç§‘å­¦ãƒ»æŠ€è¡“QA (sciq)
  - æ³•å¾‹ãƒ»åˆ¤ä¾‹QA (legal)
  - TriviaQAï¼ˆãƒˆãƒªãƒ“ã‚¢QAï¼‰ (trivia)
  - CC News Q&A (qa_cc_news_a02_llm, qa_cc_news_a03_rule, qa_cc_news_a10_hybrid)

èµ·å‹•: streamlit run a50_rag_search_local_qdrant.py --server.port=8504
"""
import os
from typing import Dict, Any, List, Optional

import pandas as pd
import streamlit as st

try:
    import yaml
except Exception:
    yaml = None

from qdrant_client import QdrantClient
from qdrant_client.http import models
from openai import OpenAI

# ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šã®å®šç¾©ï¼ˆconfig.ymlãŒå­˜åœ¨ã—ãªã„å ´åˆã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
DEFAULTS = {
    "rag": {"collection": "product_embeddings"},  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³
    "embeddings": {
        "primary": {"provider": "openai", "model": "text-embedding-3-small", "dims": 1536},
        "ada-002": {"provider": "openai", "model": "text-embedding-ada-002", "dims": 1536},
        "3-small": {"provider": "openai", "model": "text-embedding-3-small", "dims": 1536},
    },
    "qdrant": {"url": "http://localhost:6333"},  # Qdrantã‚µãƒ¼ãƒãƒ¼ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚¢ãƒ‰ãƒ¬ã‚¹
}

# ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³å›ºæœ‰ã®åŸ‹ã‚è¾¼ã¿è¨­å®šï¼ˆãƒ¢ãƒ‡ãƒ«ã¨æ¬¡å…ƒæ•°ã‚’æŒ‡å®šï¼‰
COLLECTION_EMBEDDINGS = {
    "product_embeddings": {"model": "text-embedding-3-small", "dims": 384},  # è£½å“æƒ…å ±ç”¨ï¼š384æ¬¡å…ƒã§é«˜é€Ÿå‡¦ç†
    "qa_corpus": {"model": "text-embedding-3-small", "dims": 1536},  # Q&Aã‚³ãƒ¼ãƒ‘ã‚¹ç”¨ï¼š1536æ¬¡å…ƒã§é«˜ç²¾åº¦
    "qa_cc_news_a02_llm": {"model": "text-embedding-3-small", "dims": 1536},  # CC News LLMç”Ÿæˆæ–¹å¼
    "qa_cc_news_a03_rule": {"model": "text-embedding-3-small", "dims": 1536},  # CC News ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹ç”Ÿæˆæ–¹å¼
    "qa_cc_news_a10_hybrid": {"model": "text-embedding-3-small", "dims": 1536},  # CC News ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ç”Ÿæˆæ–¹å¼
}

def load_config(path="config.yml") -> Dict[str, Any]:
    """
    è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿ã€DEFAULTSã¨ãƒãƒ¼ã‚¸ã™ã‚‹

    Args:
        path: è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯config.ymlï¼‰

    Returns:
        ãƒãƒ¼ã‚¸ã•ã‚ŒãŸè¨­å®šè¾æ›¸
    """
    cfg = {}
    if yaml and os.path.exists(path):
        with open(path, "r", encoding="utf-8") as f:
            cfg = yaml.safe_load(f) or {}
    full = DEFAULTS.copy()
    # æµ…ã„ãƒãƒ¼ã‚¸ï¼ˆç¬¬1éšå±¤ã®è¾æ›¸ã¯æ›´æ–°ã€ãã‚Œä»¥å¤–ã¯ä¸Šæ›¸ãï¼‰
    for k, v in (cfg or {}).items():
        if isinstance(v, dict) and isinstance(full.get(k), dict):
            full[k].update(v)
        else:
            full[k] = v
    return full

def embed_query(text: str, model: str, dims: Optional[int] = None) -> List[float]:
    """
    ã‚¯ã‚¨ãƒªãƒ†ã‚­ã‚¹ãƒˆã‚’åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ã«å¤‰æ›

    Args:
        text: åŸ‹ã‚è¾¼ã‚€ãƒ†ã‚­ã‚¹ãƒˆ
        model: ä½¿ç”¨ã™ã‚‹åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ï¼ˆä¾‹ï¼štext-embedding-3-smallï¼‰
        dims: ãƒ™ã‚¯ãƒˆãƒ«ã®æ¬¡å…ƒæ•°ï¼ˆtext-embedding-3ç³»ãƒ¢ãƒ‡ãƒ«ã§ã®ã¿æœ‰åŠ¹ï¼‰

    Returns:
        åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ï¼ˆfloaté…åˆ—ï¼‰
    """
    client = OpenAI()
    # text-embedding-3ç³»ãƒ¢ãƒ‡ãƒ«ã¯æ¬¡å…ƒæ•°ã®æŒ‡å®šã‚’ã‚µãƒãƒ¼ãƒˆï¼ˆ384æ¬¡å…ƒã§é«˜é€ŸåŒ–ã€1536æ¬¡å…ƒã§é«˜ç²¾åº¦ï¼‰
    if dims and "text-embedding-3" in model:
        return client.embeddings.create(model=model, input=[text], dimensions=dims).data[0].embedding
    else:
        return client.embeddings.create(model=model, input=[text]).data[0].embedding

st.set_page_config(page_title="Qdrant RAG UI", page_icon="ğŸ”", layout="wide")
st.title("ğŸ” Qdrant RAG UI (domain filter / named vectors)")

cfg = load_config("config.yml")
default_collection = cfg.get("rag", {}).get("collection", "product_embeddings")
embeddings_cfg: Dict[str, Dict[str, Any]] = cfg.get("embeddings", {})
qdrant_url = (cfg.get("qdrant", {}) or {}).get("url", "http://localhost:6333")

# Fetch available collections from Qdrant
available_collections = []
try:
    temp_client = QdrantClient(url=qdrant_url)
    collections_response = temp_client.get_collections()
    available_collections = [col.name for col in collections_response.collections]
    # Sort collections with default_collection first
    if default_collection in available_collections:
        available_collections.remove(default_collection)
        available_collections.insert(0, default_collection)
except Exception:
    available_collections = [default_collection]  # Fallback to default if can't connect

# Sample questions for each domain
SAMPLE_QUESTIONS = {
    "customer": [
        "è¿”é‡‘ã¯å¯èƒ½ã§ã™ã‹ï¼Ÿ",
        "é…é€ã«ã¯ã©ã®ãã‚‰ã„æ™‚é–“ãŒã‹ã‹ã‚Šã¾ã™ã‹ï¼Ÿ",
        "ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã‚’ä½œæˆã™ã‚‹ã«ã¯ã©ã†ã™ã‚Œã°ã‚ˆã„ã§ã™ã‹ï¼Ÿ"
    ],
    "medical": [
        "å‰¯ä½œç”¨ã¯ã‚ã‚Šã¾ã™ã‹ï¼Ÿ",
        "å¿ƒæˆ¿ç´°å‹•ã®ç—‡çŠ¶ã«ã¤ã„ã¦æ•™ãˆã¦ãã ã•ã„",
        "ç³–å°¿ç—…ã®ç®¡ç†æ–¹æ³•ã¯ä½•ã§ã™ã‹ï¼Ÿ"
    ],
    "legal": [
        "Googleã¯ç§ãŒä½œæˆã—ãŸã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã«åŸºã¥ã„ã¦æ–°ã—ã„ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ä½œæˆã™ã‚‹ã“ã¨ãŒè¨±å¯ã•ã‚Œã¦ã„ã¾ã™ã‹ï¼Ÿ",
        "ãƒ¦ãƒ¼ã‚¶ãƒ¼ç”Ÿæˆã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®ä¿®æ­£ã‹ã‚‰ãªã‚‹æ´¾ç”Ÿä½œå“ã‚’ä½œæˆã™ã‚‹ã“ã¨ã¯Googleã®æ³•çš„æ¨©åˆ©å†…ã§ã™ã‹ï¼Ÿ",
        "Googleã¯å¸¸ã«ç§ã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’Googleã‚¢ã‚«ã‚¦ãƒ³ãƒˆã‹ã‚‰è»¢é€ã™ã‚‹ã“ã¨ã‚’è¨±å¯ã—ã¾ã™ã‹ï¼Ÿ"
    ],
    "sciq": [
        "ãƒãƒ¼ã‚ºã‚„ãƒ¨ãƒ¼ã‚°ãƒ«ãƒˆãªã©ã®é£Ÿå“ã®èª¿è£½ã«ä¸€èˆ¬çš„ã«ä½¿ç”¨ã•ã‚Œã‚‹ç”Ÿç‰©ã®ã‚¿ã‚¤ãƒ—ã¯ä½•ã§ã™ã‹ï¼Ÿ",
        "æ”¾å°„æ€§å´©å£Šã®æœ€ã‚‚å±é™ºæ€§ã®ä½ã„ã‚¿ã‚¤ãƒ—ã¯ä½•ã§ã™ã‹ï¼Ÿ",
        "ç‰©è³ªãŒé…¸ç´ ã¨è¿…é€Ÿã«åå¿œã™ã‚‹ã¨ãã«èµ·ã“ã‚‹åå¿œã®ç¨®é¡ã¯ä½•ã§ã™ã‹ï¼Ÿ"
    ],
    "trivia": [
        "æ—¥æœ¬ã§ä¸€ç•ªé«˜ã„å±±ã¯ä½•ã§ã™ã‹ï¼Ÿ",
        "ã‚¢ãƒ¡ãƒªã‚«ã®åˆä»£å¤§çµ±é ˜ã¯èª°ã§ã™ã‹ï¼Ÿ",
        "å¤ªé™½ç³»ã§æœ€ã‚‚å¤§ããªæƒ‘æ˜Ÿã¯ä½•ã§ã™ã‹ï¼Ÿ",
        "æ±äº¬ã‚ªãƒªãƒ³ãƒ”ãƒƒã‚¯ã¯ä½•å¹´ã«é–‹å‚¬ã•ã‚Œã¾ã—ãŸã‹ï¼Ÿ",
        "ä¸–ç•Œã§æœ€ã‚‚é•·ã„å·ã¯ä½•ã§ã™ã‹ï¼Ÿ"
    ],
    "cc_news": [
        "What are the main topics covered in recent news?",
        "Tell me about technology advancements",
        "What political events are being discussed?",
        "Are there any major scientific discoveries?",
        "What business trends are emerging?"
    ]
}

with st.sidebar:
    st.header("Settings")
    
    # Collection selector
    if available_collections:
        # Default collection is already at index 0 due to sorting
        collection = st.selectbox("Collection", options=available_collections, index=0)
    else:
        collection = st.text_input("Collection", value=default_collection)
    
    # Show collection info
    if collection in COLLECTION_EMBEDDINGS:
        col_info = COLLECTION_EMBEDDINGS[collection]
        st.info(f"ğŸ“Š Collection '{collection}' uses {col_info['model']} with {col_info['dims']} dimensions")
    
    vec_name = st.selectbox("Using vector (named)", options=list(embeddings_cfg.keys()))
    model_for_using = embeddings_cfg[vec_name]["model"]
    
    # Check if collection supports domain filtering
    supports_domain = collection in ["qa_corpus"]  # Add collections that support domain filtering
    
    # Show domain selector based on collection support
    if supports_domain:
        domain = st.selectbox("Domain", options=["ALL", "customer", "medical", "legal", "sciq", "trivia"])
    else:
        # For collections without domain field, force ALL
        st.info(f"â„¹ï¸ Collection '{collection}' doesn't support domain filtering. Using ALL.")
        domain = "ALL"
    
    topk = st.slider("TopK", min_value=1, max_value=20, value=5, step=1)
    qdrant_url_input = st.text_input("Qdrant URL", value=qdrant_url)
    debug_mode = st.checkbox("ğŸ› Debug Mode", value=False)
    
    # Sample questions section
    st.markdown("---")
    st.subheader("ğŸ’¡ è³ªå•ä¾‹")

    # Check if this is a CC News collection
    is_cc_news = collection.startswith("qa_cc_news_")

    if is_cc_news:
        # Show CC News sample questions
        st.write("**CC News ã‚µãƒ³ãƒ—ãƒ«æ¤œç´¢:**")
        collection_label = ""
        if "a02" in collection:
            collection_label = " (LLMç”Ÿæˆ)"
        elif "a03" in collection:
            collection_label = " (ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹)"
        elif "a10" in collection:
            collection_label = " (ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰)"
        st.caption(f"Collection: {collection}{collection_label}")

        for i, q in enumerate(SAMPLE_QUESTIONS.get("cc_news", []), 1):
            if st.button(f"{i}. {q[:40]}...", key=f"sample_cc_news_{i}"):
                st.session_state['selected_query'] = q
    elif domain != "ALL":
        st.write(f"**{domain.upper()}ãƒ‰ãƒ¡ã‚¤ãƒ³ã®è³ªå•ä¾‹:**")
        for i, question in enumerate(SAMPLE_QUESTIONS.get(domain, []), 1):
            if st.button(f"{i}. {question[:30]}...", key=f"sample_{domain}_{i}"):
                st.session_state['selected_query'] = question
    else:
        # Show two examples for each domain when ALL is selected
        st.write("**ALLãƒ‰ãƒ¡ã‚¤ãƒ³ã®è³ªå•ä¾‹ï¼ˆå„ãƒ‰ãƒ¡ã‚¤ãƒ³2ä»¶ï¼‰**")
        for dom in ["customer", "medical", "legal", "sciq", "trivia"]:
            st.caption(f"{dom.upper()} ãƒ‰ãƒ¡ã‚¤ãƒ³")
            examples = SAMPLE_QUESTIONS.get(dom, [])[:2]
            for i, q in enumerate(examples, 1):
                if st.button(f"{dom} {i}. {q[:30]}...", key=f"sample_all_{dom}_{i}"):
                    st.session_state['selected_query'] = q

        # Additionally show product_embeddings samples if that collection is selected
        if collection == "product_embeddings":
            st.markdown("---")
            st.write("**Product Embeddings ã‚µãƒ³ãƒ—ãƒ«æ¤œç´¢:**")
            sample_queries = [
                "è£½å“ã®ç‰¹å¾´",
                "ä¾¡æ ¼ã«ã¤ã„ã¦",
                "ä½¿ã„æ–¹ã‚’æ•™ãˆã¦",
                "ã‚µãƒãƒ¼ãƒˆæƒ…å ±"
            ]
            for i, q in enumerate(sample_queries, 1):
                if st.button(f"{i}. {q}", key=f"sample_product_{i}"):
                    st.session_state['selected_query'] = q

# Initialize session state for query
if 'selected_query' not in st.session_state:
    st.session_state['selected_query'] = "è¿”é‡‘ã¯å¯èƒ½ã§ã™ã‹ï¼Ÿ"

st.code("""
  - collectionã€Œqa_corpusã€ã¯5ç¨®é¡ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼ˆcustomer, medical, legal, sciq, triviaï¼‰ã«å¯¾å¿œ
  - ã“ã“ã§ãƒ‰ãƒ¡ã‚¤ãƒ³ã‚’é¸æŠã™ã‚‹ã¨ãã®ãƒ‰ãƒ¡ã‚¤ãƒ³ã«ç‰¹åŒ–ã—ãŸæƒ…å ±ãŒå–ã‚Šå‡ºã›ã¾ã™ã€‚
  - collectionã€Œqa_corpusã€ã®Domain=ALLã¯5ã¤ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®çµ±åˆç‰ˆã§ã™ã€‚
  - CC News collections: 3ã¤ã®ç”Ÿæˆæ‰‹æ³•ã§æ¯”è¼ƒå¯èƒ½
    - qa_cc_news_a02_llm: LLMç”Ÿæˆæ–¹å¼ï¼ˆa02_qa_pairs_cc_news.csvï¼‰
    - qa_cc_news_a03_rule: ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹ç”Ÿæˆæ–¹å¼ï¼ˆa03_qa_pairs_cc_news.csvï¼‰
    - qa_cc_news_a10_hybrid: ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ç”Ÿæˆæ–¹å¼ï¼ˆa10_qa_pairs_cc_news.csvï¼‰
  - OpenAIã®embeddingãƒ¢ãƒ‡ãƒ«ãŒå¤šè¨€èªå¯¾å¿œã®ãŸã‚ã€æ—¥æœ¬èªè³ªå•ã¨è‹±èªãƒ‡ãƒ¼ã‚¿ãŒåŒã˜ãƒ™ã‚¯ãƒˆãƒ«ç©ºé–“ã§æ¯”è¼ƒå¯èƒ½
  - ä¾‹ã°ã€æ—¥æœ¬èªã€Œè¿”é‡‘ã¯å¯èƒ½ã§ã™ã‹ï¼Ÿã€ã¨è‹±èªã€ŒCan I get a refund?ã€ã®é¡ä¼¼åº¦ãŒ0.4957ã¨é«˜ã„å€¤ã‚’ç¤ºã—ã¦ã„ã‚‹
  - ã“ã®å¤šè¨€èªembeddingæ©Ÿèƒ½ã«ã‚ˆã‚Šã€ç¿»è¨³ãªã—ã§æ—¥è‹±é–“ã®æ„å‘³çš„æ¤œç´¢ãŒå®Ÿç¾ã•ã‚Œã¦ã„ã‚‹ã€‚
  - å·¦ãƒšã‚¤ãƒ³ã§ã€å€‹åˆ¥domainã‚’é¸æŠã™ã‚‹ã¨è³ªå•ãƒ»å€™è£œãŒè¡¨ç¤ºã•ã‚Œã¾ã™ã€‚
  - å®Ÿç”¨çš„ãªé–¾å€¤ã®ç›®å®‰ï¼ˆScore:ï¼‰
  - 0.8ä»¥ä¸Š: éå¸¸ã«é–¢é€£æ€§ãŒé«˜ã„ï¼ˆã»ã¼ä¸€è‡´ï¼‰
  - 0.6-0.8: é–¢é€£æ€§ãŒã‚ã‚‹ï¼ˆæœ‰ç”¨ãªçµæœï¼‰
  - 0.4-0.6: éƒ¨åˆ†çš„ã«é–¢é€£ï¼ˆå‚è€ƒç¨‹åº¦ï¼‰
  - 0.4æœªæº€: é–¢é€£æ€§ãŒä½ã„ï¼ˆãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°æ¨å¥¨ï¼‰
""")
query = st.text_input("Enter your query", value=st.session_state['selected_query'])
do_search = st.button("Search")

if do_search and query.strip():
    try:
        # Use the updated URL from input if provided
        current_qdrant_url = qdrant_url_input if 'qdrant_url_input' in locals() else qdrant_url
        client = QdrantClient(url=current_qdrant_url)
        # Test connection
        try:
            client.get_collections()
        except Exception as conn_err:
            st.error(f"âŒ Qdrantã‚µãƒ¼ãƒãƒ¼ã«æ¥ç¶šã§ãã¾ã›ã‚“: {current_qdrant_url}")
            st.error("ä»¥ä¸‹ã‚’ç¢ºèªã—ã¦ãã ã•ã„:")
            st.error("1. Qdrantã‚µãƒ¼ãƒãƒ¼ãŒèµ·å‹•ã—ã¦ã„ã‚‹ã‹ç¢ºèª: `docker ps` ã¾ãŸã¯ `qdrant` ã‚³ãƒãƒ³ãƒ‰")
            st.error("2. URLãŒæ­£ã—ã„ã‹ç¢ºèª (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: http://localhost:6333)")
            st.error(f"ã‚¨ãƒ©ãƒ¼è©³ç´°: {str(conn_err)}")
            st.stop()
        
        # ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã«å¯¾å¿œã—ãŸåŸ‹ã‚è¾¼ã¿è¨­å®šã‚’å–å¾—ï¼ˆå„ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã¯ç•°ãªã‚‹æ¬¡å…ƒæ•°ã‚’æŒã¤å¯èƒ½æ€§ãŒã‚ã‚‹ï¼‰
        collection_config = COLLECTION_EMBEDDINGS.get(collection, {"model": model_for_using, "dims": None})
        embedding_model = collection_config["model"]
        embedding_dims = collection_config.get("dims")

        # ãƒ‡ãƒãƒƒã‚°ãƒ¢ãƒ¼ãƒ‰ï¼šä½¿ç”¨ã™ã‚‹åŸ‹ã‚è¾¼ã¿è¨­å®šã‚’è¡¨ç¤º
        if debug_mode:
            st.info(f"ğŸ” Using model: {embedding_model} with dims: {embedding_dims}")
        
        # ã‚¯ã‚¨ãƒªãƒ†ã‚­ã‚¹ãƒˆã‚’åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ã«å¤‰æ›ï¼ˆã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã”ã¨ã®æ¬¡å…ƒæ•°ã«å¯¾å¿œï¼‰
        try:
            qvec = embed_query(query, embedding_model, embedding_dims)
            if debug_mode:
                st.success(f"âœ… Generated embedding with {len(qvec)} dimensions")
        except Exception as embed_err:
            st.error(f"âŒ Embedding generation failed: {str(embed_err)}")
            st.error(f"Model: {embedding_model}, Requested dims: {embedding_dims}")
            st.stop()

        # ãƒ‰ãƒ¡ã‚¤ãƒ³ãƒ•ã‚£ãƒ«ã‚¿ã®è¨­å®šï¼ˆqa_corpusã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã®ã¿å¯¾å¿œï¼‰
        qfilter = None
        if domain != "ALL":
            qfilter = models.Filter(must=[models.FieldCondition(key="domain", match=models.MatchValue(value=domain))])

        # Qdrantã§ãƒ™ã‚¯ãƒˆãƒ«é¡ä¼¼æ¤œç´¢ã‚’å®Ÿè¡Œï¼ˆdeprecationè­¦å‘Šã‚’æŠ‘åˆ¶ï¼‰
        import warnings
        with warnings.catch_warnings():
            warnings.simplefilter("ignore", DeprecationWarning)
            hits = client.search(
                collection_name=collection,
                query_vector=qvec,
                limit=topk,
                query_filter=qfilter
            )
        rows = []
        for h in hits:
            # Debug: Show the actual payload structure
            if debug_mode:
                st.write(f"Debug - Payload keys: {h.payload.keys() if h.payload else 'No payload'}")
                st.write(f"Debug - Full payload: {h.payload}")
            
            # Try different field names that might be used
            row_data = {
                "score": h.score,
                "domain": h.payload.get("domain") if h.payload else None,
                "question": h.payload.get("question") or h.payload.get("text") or h.payload.get("content") if h.payload else None,
                "answer": h.payload.get("answer") or h.payload.get("response") or h.payload.get("metadata") if h.payload else None,
                "source": h.payload.get("source") or h.payload.get("file") if h.payload else None,
            }
            
            # If still no question/answer, try to extract from any text field
            if not row_data["question"] and h.payload:
                # Look for any text-like field
                for key in h.payload.keys():
                    if isinstance(h.payload[key], str) and len(h.payload[key]) > 10:
                        row_data["question"] = h.payload[key][:200]  # Limit to 200 chars
                        break
            
            rows.append(row_data)
        
        st.subheader("Results")
        st.dataframe(pd.DataFrame(rows))
        
        # Display the highest score result
        if rows:
            best_result = max(rows, key=lambda x: x["score"])
            st.subheader("ğŸ† Highest Score Result")
            st.write(f"**Score:** {best_result['score']:.4f}")
            st.write(f"**Question:** {best_result['question']}")
            st.write(f"**Answer:** {best_result['answer']}")

            # Ask OpenAI again using the result + original query (Japanese output)
            st.subheader("ğŸ§  OpenAI å¿œç­”ï¼ˆæ—¥æœ¬èªï¼‰")
            try:
                br_q = best_result.get("question") or ""
                br_a = best_result.get("answer") or ""
                br_score = best_result.get("score") or 0.0

                qa_prompt_jp = (
                    "ä»¥ä¸‹ã®æ¤œç´¢çµæœï¼ˆã‚¹ã‚³ã‚¢ãƒ»è³ªå•ãƒ»å›ç­”ï¼‰ã¨ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å…ƒã®è³ªå•ã‚’è¸ã¾ãˆã¦ã€" \
                    "æ—¥æœ¬èªã§ç°¡æ½”ã‹ã¤æ­£ç¢ºã«å›ç­”ã—ã¦ãã ã•ã„ã€‚å¿…è¦ã«å¿œã˜ã¦ç®‡æ¡æ›¸ãã‚’ç”¨ã„ã¦ãã ã•ã„ã€‚\n\n"
                    f"ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å…ƒã®è³ªå•ï¼ˆqueryï¼‰:\n{query}\n\n"
                    f"æ¤œç´¢çµæœã®ã‚¹ã‚³ã‚¢: {br_score:.4f}\n"
                    f"æ¤œç´¢çµæœã®è³ªå•: {br_q}\n"
                    f"æ¤œç´¢çµæœã®å›ç­”: {br_a}\n"
                )

                st.markdown("**è³ªå•ï¼ˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼‰**")
                st.code(qa_prompt_jp)

                with st.spinner("OpenAIã«å•ã„åˆã‚ã›ä¸­..."):
                    oai_client = OpenAI()
                    oai_resp = oai_client.responses.create(
                        model="gpt-4o-mini",
                        input=qa_prompt_jp
                    )
                    generated_answer = getattr(oai_resp, "output_text", None) or ""

                st.markdown("**å›ç­”ï¼ˆæ—¥æœ¬èªï¼‰**")
                if generated_answer.strip():
                    st.write(generated_answer)
                else:
                    st.info("å¿œç­”ãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚")
            except Exception as gen_err:
                st.error(f"OpenAIå¿œç­”ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸ: {str(gen_err)}")
    except ConnectionRefusedError:
        st.error(f"âŒ Qdrantã‚µãƒ¼ãƒãƒ¼ã¸ã®æ¥ç¶šãŒæ‹’å¦ã•ã‚Œã¾ã—ãŸ: {qdrant_url}")
        st.error("Qdrantã‚µãƒ¼ãƒãƒ¼ãŒèµ·å‹•ã—ã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¦ãã ã•ã„:")
        st.code("cd docker-compose && docker-compose up -d", language="bash")
    except Exception as e:
        if "Connection refused" in str(e):
            st.error(f"âŒ Qdrantã‚µãƒ¼ãƒãƒ¼ã«æ¥ç¶šã§ãã¾ã›ã‚“: {current_qdrant_url}")
            st.error("Qdrantã‚µãƒ¼ãƒãƒ¼ãŒèµ·å‹•ã—ã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¦ãã ã•ã„:")
            st.code("cd docker-compose && docker-compose up -d", language="bash")
        elif "collection" in str(e).lower() and "not found" in str(e).lower():
            st.error(f"âŒ ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ '{collection}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            st.error("å…ˆã« a42_qdrant_registration.py ã‚’å®Ÿè¡Œã—ã¦ãƒ‡ãƒ¼ã‚¿ã‚’ç™»éŒ²ã—ã¦ãã ã•ã„:")
            st.code("python a42_qdrant_registration.py", language="bash")
        else:
            st.error(f"âŒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
            st.error("ã‚¨ãƒ©ãƒ¼ã®è©³ç´°:")
            st.exception(e)
