# a011_make_rag_data_customer.py
# ã‚«ã‚¹ã‚¿ãƒãƒ¼ã‚µãƒãƒ¼ãƒˆFAQãƒ‡ãƒ¼ã‚¿ã®RAGå‰å‡¦ç†ï¼ˆãƒ¢ãƒ‡ãƒ«é¸æŠæ©Ÿèƒ½ä»˜ããƒ»ç‹¬ç«‹ç‰ˆï¼‰
# streamlit run a011_make_rag_data_customer.py --server.port=8501

import streamlit as st
import pandas as pd
import re
import io
import logging
import json
import os
from typing import List, Dict, Any, Optional, Tuple
from pathlib import Path
from datetime import datetime
from functools import wraps

# åŸºæœ¬ãƒ­ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


# ==================================================
# è¨­å®šç®¡ç†ï¼ˆç‹¬ç«‹å®Ÿè£…ï¼‰
# ==================================================
class AppConfig:
    """ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³è¨­å®šï¼ˆç‹¬ç«‹å®Ÿè£…ï¼‰"""

    # åˆ©ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«
    AVAILABLE_MODELS = [
        "gpt-4o",
        "gpt-4o-mini",
        "gpt-4o-audio-preview",
        "gpt-4o-mini-audio-preview",
        "gpt-4.1",
        "gpt-4.1-mini",
        "o1",
        "o1-mini",
        "o3",
        "o3-mini",
        "o4",
        "o4-mini"
    ]

    DEFAULT_MODEL = "gpt-4o-mini"

    # ãƒ¢ãƒ‡ãƒ«æ–™é‡‘ï¼ˆ1000ãƒˆãƒ¼ã‚¯ãƒ³ã‚ãŸã‚Šã®ãƒ‰ãƒ«ï¼‰
    MODEL_PRICING = {
        "gpt-4o"                   : {"input": 0.005, "output": 0.015},
        "gpt-4o-mini"              : {"input": 0.00015, "output": 0.0006},
        "gpt-4o-audio-preview"     : {"input": 0.01, "output": 0.02},
        "gpt-4o-mini-audio-preview": {"input": 0.00025, "output": 0.001},
        "gpt-4.1"                  : {"input": 0.0025, "output": 0.01},
        "gpt-4.1-mini"             : {"input": 0.0001, "output": 0.0004},
        "o1"                       : {"input": 0.015, "output": 0.06},
        "o1-mini"                  : {"input": 0.003, "output": 0.012},
        "o3"                       : {"input": 0.03, "output": 0.12},
        "o3-mini"                  : {"input": 0.006, "output": 0.024},
        "o4"                       : {"input": 0.05, "output": 0.20},
        "o4-mini"                  : {"input": 0.01, "output": 0.04},
    }

    # ãƒ¢ãƒ‡ãƒ«åˆ¶é™
    MODEL_LIMITS = {
        "gpt-4o"      : {"max_tokens": 128000, "max_output": 4096},
        "gpt-4o-mini" : {"max_tokens": 128000, "max_output": 4096},
        "gpt-4.1"     : {"max_tokens": 128000, "max_output": 4096},
        "gpt-4.1-mini": {"max_tokens": 128000, "max_output": 4096},
        "o1"          : {"max_tokens": 128000, "max_output": 32768},
        "o1-mini"     : {"max_tokens": 128000, "max_output": 65536},
        "o3"          : {"max_tokens": 200000, "max_output": 100000},
        "o3-mini"     : {"max_tokens": 200000, "max_output": 100000},
        "o4"          : {"max_tokens": 256000, "max_output": 128000},
        "o4-mini"     : {"max_tokens": 256000, "max_output": 128000},
    }

    @classmethod
    def get_model_limits(cls, model: str) -> Dict[str, int]:
        """ãƒ¢ãƒ‡ãƒ«ã®åˆ¶é™ã‚’å–å¾—"""
        return cls.MODEL_LIMITS.get(model, {"max_tokens": 128000, "max_output": 4096})

    @classmethod
    def get_model_pricing(cls, model: str) -> Dict[str, float]:
        """ãƒ¢ãƒ‡ãƒ«ã®æ–™é‡‘ã‚’å–å¾—"""
        return cls.MODEL_PRICING.get(model, {"input": 0.00015, "output": 0.0006})


# ==================================================
# RAGè¨­å®šï¼ˆç‹¬ç«‹å®Ÿè£…ï¼‰
# ==================================================
class RAGConfig:
    """RAGãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†ã®è¨­å®š"""

    DATASET_CONFIGS = {
        "customer_support_faq": {
            "name"            : "ã‚«ã‚¹ã‚¿ãƒãƒ¼ã‚µãƒãƒ¼ãƒˆãƒ»FAQ",
            "icon"            : "ğŸ’¬",
            "required_columns": ["question", "answer"],
            "description"     : "ã‚«ã‚¹ã‚¿ãƒãƒ¼ã‚µãƒãƒ¼ãƒˆFAQãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ",
            "combine_template": "{question} {answer}"
        }
    }

    @classmethod
    def get_config(cls, dataset_type: str) -> Dict[str, Any]:
        """ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆè¨­å®šã®å–å¾—"""
        return cls.DATASET_CONFIGS.get(dataset_type, {
            "name"            : "æœªçŸ¥ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ",
            "icon"            : "â“",
            "required_columns": [],
            "description"     : "æœªçŸ¥ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ",
            "combine_template": "{}"
        })


# ==================================================
# ãƒˆãƒ¼ã‚¯ãƒ³ç®¡ç†ï¼ˆç‹¬ç«‹å®Ÿè£…ï¼‰
# ==================================================
class TokenManager:
    """ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã®ç®¡ç†ï¼ˆç°¡æ˜“ç‰ˆï¼‰"""

    @staticmethod
    def count_tokens(text: str, model: str = None) -> int:
        """ãƒ†ã‚­ã‚¹ãƒˆã®ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆï¼ˆç°¡æ˜“æ¨å®šï¼‰"""
        if not text:
            return 0
        # ç°¡æ˜“æ¨å®š: 1æ–‡å­— = 0.5ãƒˆãƒ¼ã‚¯ãƒ³ï¼ˆæ—¥æœ¬èªï¼‰ã€1å˜èª = 1ãƒˆãƒ¼ã‚¯ãƒ³ï¼ˆè‹±èªï¼‰
        japanese_chars = len([c for c in text if ord(c) > 127])
        english_chars = len(text) - japanese_chars
        return int(japanese_chars * 0.5 + english_chars * 0.25)

    @staticmethod
    def estimate_cost(input_tokens: int, output_tokens: int, model: str) -> float:
        """APIä½¿ç”¨ã‚³ã‚¹ãƒˆã®æ¨å®š"""
        pricing = AppConfig.get_model_pricing(model)
        input_cost = (input_tokens / 1000) * pricing["input"]
        output_cost = (output_tokens / 1000) * pricing["output"]
        return input_cost + output_cost


# ==================================================
# UIé–¢æ•°ï¼ˆç‹¬ç«‹å®Ÿè£…ï¼‰
# ==================================================
def select_model(key: str = "model_selection") -> str:
    """ãƒ¢ãƒ‡ãƒ«é¸æŠUI"""
    models = AppConfig.AVAILABLE_MODELS
    default_model = AppConfig.DEFAULT_MODEL

    try:
        default_index = models.index(default_model)
    except ValueError:
        default_index = 0

    selected = st.sidebar.selectbox(
        "ğŸ¤– ãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠ",
        models,
        index=default_index,
        key=key,
        help="åˆ©ç”¨ã™ã‚‹OpenAIãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠã—ã¦ãã ã•ã„"
    )

    return selected


def show_model_info(selected_model: str) -> None:
    """é¸æŠã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã®æƒ…å ±ã‚’è¡¨ç¤º"""
    try:
        limits = AppConfig.get_model_limits(selected_model)
        pricing = AppConfig.get_model_pricing(selected_model)

        with st.sidebar.expander("ğŸ“Š é¸æŠãƒ¢ãƒ‡ãƒ«æƒ…å ±", expanded=False):
            # åŸºæœ¬æƒ…å ±
            col1, col2 = st.columns(2)
            with col1:
                st.write("**æœ€å¤§å…¥åŠ›**")
                st.write(f"{limits['max_tokens']:,}")
            with col2:
                st.write("**æœ€å¤§å‡ºåŠ›**")
                st.write(f"{limits['max_output']:,}")

            # æ–™é‡‘æƒ…å ±
            st.write("**æ–™é‡‘ï¼ˆ1000ãƒˆãƒ¼ã‚¯ãƒ³ï¼‰**")
            st.write(f"- å…¥åŠ›: ${pricing['input']:.5f}")
            st.write(f"- å‡ºåŠ›: ${pricing['output']:.5f}")

            # ãƒ¢ãƒ‡ãƒ«ç‰¹æ€§
            if selected_model.startswith("o"):
                st.info("ğŸ§  æ¨è«–ç‰¹åŒ–ãƒ¢ãƒ‡ãƒ«")
                st.write("é«˜åº¦ãªæ¨è«–ã‚¿ã‚¹ã‚¯ã«æœ€é©åŒ–")
            elif "audio" in selected_model:
                st.info("ğŸµ éŸ³å£°å¯¾å¿œãƒ¢ãƒ‡ãƒ«")
                st.write("éŸ³å£°å…¥åŠ›ãƒ»å‡ºåŠ›ã«å¯¾å¿œ")
            elif "gpt-4o" in selected_model:
                st.info("ğŸ‘ï¸ ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ãƒ¢ãƒ‡ãƒ«")
                st.write("ãƒ†ã‚­ã‚¹ãƒˆãƒ»ç”»åƒã®ç†è§£ãŒå¯èƒ½")
            else:
                st.info("ğŸ’¬ æ¨™æº–å¯¾è©±ãƒ¢ãƒ‡ãƒ«")
                st.write("ä¸€èˆ¬çš„ãªå¯¾è©±ãƒ»ãƒ†ã‚­ã‚¹ãƒˆå‡¦ç†")

            # RAGç”¨é€”ã§ã®æ¨å¥¨åº¦
            st.write("**RAGç”¨é€”æ¨å¥¨åº¦**")
            if selected_model in ["gpt-4o-mini", "gpt-4.1-mini"]:
                st.success("âœ… æœ€é©ï¼ˆã‚³ã‚¹ãƒˆåŠ¹ç‡è‰¯å¥½ï¼‰")
            elif selected_model in ["gpt-4o", "gpt-4.1"]:
                st.info("ğŸ’¡ é«˜å“è³ªï¼ˆã‚³ã‚¹ãƒˆé«˜ï¼‰")
            elif selected_model.startswith("o"):
                st.warning("âš ï¸ æ¨è«–ç‰¹åŒ–ï¼ˆRAGç”¨é€”ã«ã¯éå‰°ï¼‰")
            else:
                st.info("ğŸ’¬ æ¨™æº–çš„ãªæ€§èƒ½")

    except Exception as e:
        logger.error(f"ãƒ¢ãƒ‡ãƒ«æƒ…å ±è¡¨ç¤ºã‚¨ãƒ©ãƒ¼: {e}")
        st.sidebar.error("ãƒ¢ãƒ‡ãƒ«æƒ…å ±ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ")


# ==================================================
# ãƒ‡ã‚³ãƒ¬ãƒ¼ã‚¿ï¼ˆç‹¬ç«‹å®Ÿè£…ï¼‰
# ==================================================
def error_handler(func):
    """ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ãƒ‡ã‚³ãƒ¬ãƒ¼ã‚¿"""

    @wraps(func)
    def wrapper(*args, **kwargs):
        try:
            return func(*args, **kwargs)
        except Exception as e:
            logger.error(f"Error in {func.__name__}: {str(e)}")
            st.error(f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
            return None

    return wrapper


# ==================================================
# ãƒ‡ãƒ¼ã‚¿å‡¦ç†é–¢æ•°ï¼ˆç‹¬ç«‹å®Ÿè£…ï¼‰
# ==================================================
def clean_text(text: str) -> str:
    """ãƒ†ã‚­ã‚¹ãƒˆã®ã‚¯ãƒ¬ãƒ³ã‚¸ãƒ³ã‚°å‡¦ç†"""
    if pd.isna(text) or text == "":
        return ""

    # æ”¹è¡Œã‚’ç©ºç™½ã«ç½®æ›
    text = str(text).replace('\n', ' ').replace('\r', ' ')

    # é€£ç¶šã—ãŸç©ºç™½ã‚’1ã¤ã®ç©ºç™½ã«ã¾ã¨ã‚ã‚‹
    text = re.sub(r'\s+', ' ', text)

    # å…ˆé ­ãƒ»æœ«å°¾ã®ç©ºç™½ã‚’é™¤å»
    text = text.strip()

    # å¼•ç”¨ç¬¦ã®æ­£è¦åŒ–
    text = text.replace('"', '"').replace('"', '"')
    text = text.replace(''', "'").replace(''', "'")

    return text


def combine_columns(row: pd.Series, dataset_type: str = "customer_support_faq") -> str:
    """è¤‡æ•°åˆ—ã‚’çµåˆã—ã¦1ã¤ã®ãƒ†ã‚­ã‚¹ãƒˆã«ã™ã‚‹"""
    config_data = RAGConfig.get_config(dataset_type)
    required_columns = config_data["required_columns"]

    # å„åˆ—ã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡ºãƒ»ã‚¯ãƒ¬ãƒ³ã‚¸ãƒ³ã‚°
    cleaned_values = {}
    for col in required_columns:
        value = row.get(col, '')
        cleaned_values[col.lower()] = clean_text(str(value))

    # çµåˆ
    combined = " ".join(cleaned_values.values())
    return combined.strip()


def validate_data(df: pd.DataFrame, dataset_type: str = None) -> List[str]:
    """ãƒ‡ãƒ¼ã‚¿ã®æ¤œè¨¼"""
    issues = []

    # åŸºæœ¬çµ±è¨ˆ
    issues.append(f"ç·è¡Œæ•°: {len(df)}")
    issues.append(f"ç·åˆ—æ•°: {len(df.columns)}")

    # å¿…é ˆåˆ—ã®ç¢ºèª
    if dataset_type:
        config_data = RAGConfig.get_config(dataset_type)
        required_columns = config_data["required_columns"]

        missing_columns = [col for col in required_columns if col not in df.columns]
        if missing_columns:
            issues.append(f"âš ï¸ å¿…é ˆåˆ—ãŒä¸è¶³: {missing_columns}")
        else:
            issues.append(f"âœ… å¿…é ˆåˆ—ç¢ºèªæ¸ˆã¿: {required_columns}")

    # å„åˆ—ã®ç©ºå€¤ç¢ºèª
    for col in df.columns:
        empty_count = df[col].isna().sum() + (df[col] == '').sum()
        if empty_count > 0:
            percentage = (empty_count / len(df)) * 100
            issues.append(f"{col}åˆ—: ç©ºå€¤ {empty_count}å€‹ ({percentage:.1f}%)")

    # é‡è¤‡è¡Œã®ç¢ºèª
    duplicate_count = df.duplicated().sum()
    if duplicate_count > 0:
        issues.append(f"âš ï¸ é‡è¤‡è¡Œ: {duplicate_count}å€‹")
    else:
        issues.append("âœ… é‡è¤‡è¡Œãªã—")

    return issues


def load_dataset(uploaded_file, dataset_type: str = None) -> Tuple[pd.DataFrame, List[str]]:
    """ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®èª­ã¿è¾¼ã¿ã¨åŸºæœ¬æ¤œè¨¼"""
    try:
        # CSVãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿
        df = pd.read_csv(uploaded_file)

        # åŸºæœ¬æ¤œè¨¼
        validation_results = validate_data(df, dataset_type)

        logger.info(f"ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆèª­ã¿è¾¼ã¿å®Œäº†: {len(df)}è¡Œ, {len(df.columns)}åˆ—")
        return df, validation_results

    except Exception as e:
        logger.error(f"ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
        raise


def process_rag_data(df: pd.DataFrame, dataset_type: str, combine_columns_option: bool = True) -> pd.DataFrame:
    """RAGãƒ‡ãƒ¼ã‚¿ã®å‰å‡¦ç†ã‚’å®Ÿè¡Œ"""
    # åŸºæœ¬çš„ãªå‰å‡¦ç†
    df_processed = df.copy()

    # é‡è¤‡è¡Œã®é™¤å»
    initial_rows = len(df_processed)
    df_processed = df_processed.drop_duplicates()
    duplicates_removed = initial_rows - len(df_processed)

    # ç©ºè¡Œã®é™¤å»
    df_processed = df_processed.dropna(how='all')
    empty_rows_removed = initial_rows - duplicates_removed - len(df_processed)

    # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®ãƒªã‚»ãƒƒãƒˆ
    df_processed = df_processed.reset_index(drop=True)

    logger.info(f"å‰å‡¦ç†å®Œäº†: é‡è¤‡é™¤å»={duplicates_removed}è¡Œ, ç©ºè¡Œé™¤å»={empty_rows_removed}è¡Œ")

    # å„åˆ—ã®ã‚¯ãƒ¬ãƒ³ã‚¸ãƒ³ã‚°
    config_data = RAGConfig.get_config(dataset_type)
    required_columns = config_data["required_columns"]

    for col in required_columns:
        if col in df_processed.columns:
            df_processed[col] = df_processed[col].apply(clean_text)

    # åˆ—ã®çµåˆï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
    if combine_columns_option:
        df_processed['Combined_Text'] = df_processed.apply(
            lambda row: combine_columns(row, dataset_type),
            axis=1
        )

    return df_processed


def create_download_data(df: pd.DataFrame, include_combined: bool = True, dataset_type: str = None) -> Tuple[
    str, Optional[str]]:
    """ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ç”¨ãƒ‡ãƒ¼ã‚¿ã®ä½œæˆ"""
    try:
        # CSVãƒ‡ãƒ¼ã‚¿ã®ä½œæˆ
        csv_buffer = io.StringIO()
        df.to_csv(csv_buffer, index=False, encoding='utf-8')
        csv_data = csv_buffer.getvalue()

        # çµåˆãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®ä½œæˆ
        text_data = None
        if include_combined and 'Combined_Text' in df.columns:
            text_data = df['Combined_Text'].to_string(index=False)

        return csv_data, text_data

    except Exception as e:
        logger.error(f"create_download_data ã‚¨ãƒ©ãƒ¼: {e}")
        raise


def display_statistics(df_original: pd.DataFrame, df_processed: pd.DataFrame, dataset_type: str = None) -> None:
    """å‡¦ç†å‰å¾Œã®çµ±è¨ˆæƒ…å ±ã‚’è¡¨ç¤º"""
    st.subheader("ğŸ“Š çµ±è¨ˆæƒ…å ±")

    col1, col2, col3 = st.columns(3)

    with col1:
        st.metric("å…ƒã®è¡Œæ•°", len(df_original))
    with col2:
        st.metric("å‡¦ç†å¾Œã®è¡Œæ•°", len(df_processed))
    with col3:
        removed_rows = len(df_original) - len(df_processed)
        st.metric("é™¤å»ã•ã‚ŒãŸè¡Œæ•°", removed_rows)

    # çµåˆãƒ†ã‚­ã‚¹ãƒˆã®åˆ†æ
    if 'Combined_Text' in df_processed.columns:
        st.subheader("ğŸ“ çµåˆå¾Œãƒ†ã‚­ã‚¹ãƒˆåˆ†æ")
        text_lengths = df_processed['Combined_Text'].str.len()

        col1, col2, col3 = st.columns(3)
        with col1:
            st.metric("å¹³å‡æ–‡å­—æ•°", f"{text_lengths.mean():.0f}")
        with col2:
            st.metric("æœ€å¤§æ–‡å­—æ•°", text_lengths.max())
        with col3:
            st.metric("æœ€å°æ–‡å­—æ•°", text_lengths.min())


def estimate_token_usage(df_processed: pd.DataFrame, selected_model: str) -> None:
    """å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã®ãƒˆãƒ¼ã‚¯ãƒ³ä½¿ç”¨é‡æ¨å®š"""
    try:
        if 'Combined_Text' in df_processed.columns:
            # ã‚µãƒ³ãƒ—ãƒ«ãƒ†ã‚­ã‚¹ãƒˆã§ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’æ¨å®š
            sample_texts = df_processed['Combined_Text'].head(10).tolist()
            total_chars = df_processed['Combined_Text'].str.len().sum()

            if sample_texts:
                sample_text = " ".join(sample_texts)
                sample_tokens = TokenManager.count_tokens(sample_text, selected_model)
                sample_chars = len(sample_text)

                if sample_chars > 0:
                    # å…¨ä½“ã®ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’æ¨å®š
                    estimated_total_tokens = int((total_chars / sample_chars) * sample_tokens)

                    with st.expander("ğŸ”¢ ãƒˆãƒ¼ã‚¯ãƒ³ä½¿ç”¨é‡æ¨å®š", expanded=False):
                        col1, col2, col3 = st.columns(3)
                        with col1:
                            st.metric("æ¨å®šç·ãƒˆãƒ¼ã‚¯ãƒ³æ•°", f"{estimated_total_tokens:,}")
                        with col2:
                            avg_tokens_per_record = estimated_total_tokens / len(df_processed)
                            st.metric("å¹³å‡ãƒˆãƒ¼ã‚¯ãƒ³/ãƒ¬ã‚³ãƒ¼ãƒ‰", f"{avg_tokens_per_record:.0f}")
                        with col3:
                            # embeddingç”¨ã®ã‚³ã‚¹ãƒˆæ¨å®šï¼ˆå‚è€ƒå€¤ï¼‰
                            embedding_cost = (estimated_total_tokens / 1000) * 0.0001
                            st.metric("æ¨å®šembeddingè²»ç”¨", f"${embedding_cost:.4f}")

                        st.info(f"ğŸ’¡ é¸æŠãƒ¢ãƒ‡ãƒ«ã€Œ{selected_model}ã€ã§ã®æ¨å®šå€¤")

    except Exception as e:
        logger.error(f"ãƒˆãƒ¼ã‚¯ãƒ³ä½¿ç”¨é‡æ¨å®šã‚¨ãƒ©ãƒ¼: {e}")
        st.error("ãƒˆãƒ¼ã‚¯ãƒ³ä½¿ç”¨é‡ã®æ¨å®šã«å¤±æ•—ã—ã¾ã—ãŸ")


# ==================================================
# ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜é–¢æ•°
# ==================================================
def create_output_directory() -> Path:
    """OUTPUTãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ä½œæˆ"""
    try:
        output_dir = Path("OUTPUT")
        output_dir.mkdir(exist_ok=True)

        # æ›¸ãè¾¼ã¿æ¨©é™ã®ãƒ†ã‚¹ãƒˆ
        test_file = output_dir / ".test_write"
        try:
            test_file.write_text("test", encoding='utf-8')
            if test_file.exists():
                test_file.unlink()
                logger.info("æ›¸ãè¾¼ã¿æ¨©é™ãƒ†ã‚¹ãƒˆ: æˆåŠŸ")
        except Exception as e:
            raise PermissionError(f"æ›¸ãè¾¼ã¿æ¨©é™ãƒ†ã‚¹ãƒˆã«å¤±æ•—: {e}")

        logger.info(f"OUTPUTãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæº–å‚™å®Œäº†: {output_dir.absolute()}")
        return output_dir

    except Exception as e:
        logger.error(f"ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆã‚¨ãƒ©ãƒ¼: {e}")
        raise


def save_files_to_output(df_processed, dataset_type: str, csv_data: str, text_data: str = None) -> Dict[str, str]:
    """å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã‚’OUTPUTãƒ•ã‚©ãƒ«ãƒ€ã«ä¿å­˜"""
    try:
        output_dir = create_output_directory()
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        saved_files = {}

        # CSVãƒ•ã‚¡ã‚¤ãƒ«ã®ä¿å­˜
        csv_filename = f"preprocessed_{dataset_type}_{len(df_processed)}rows_{timestamp}.csv"
        csv_path = output_dir / csv_filename

        with open(csv_path, 'w', encoding='utf-8') as f:
            f.write(csv_data)

        if csv_path.exists():
            saved_files['csv'] = str(csv_path)
            logger.info(f"CSVãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜å®Œäº†: {csv_path}")

        # ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã®ä¿å­˜
        if text_data and len(text_data.strip()) > 0:
            txt_filename = f"{dataset_type}.txt"
            txt_path = output_dir / txt_filename

            with open(txt_path, 'w', encoding='utf-8') as f:
                f.write(text_data)

            if txt_path.exists():
                saved_files['txt'] = str(txt_path)
                logger.info(f"ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜å®Œäº†: {txt_path}")

        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ã®ä¿å­˜
        metadata = {
            "dataset_type"        : dataset_type,
            "processed_rows"      : len(df_processed),
            "processing_timestamp": timestamp,
            "created_at"          : datetime.now().isoformat(),
            "files_created"       : list(saved_files.keys())
        }

        metadata_filename = f"metadata_{dataset_type}_{timestamp}.json"
        metadata_path = output_dir / metadata_filename

        with open(metadata_path, 'w', encoding='utf-8') as f:
            json.dump(metadata, f, indent=2, ensure_ascii=False)

        if metadata_path.exists():
            saved_files['metadata'] = str(metadata_path)
            logger.info(f"ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜å®Œäº†: {metadata_path}")

        return saved_files

    except Exception as e:
        logger.error(f"ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
        raise


# ==================================================
# ã‚«ã‚¹ã‚¿ãƒãƒ¼ã‚µãƒãƒ¼ãƒˆFAQç‰¹æœ‰ã®å‡¦ç†é–¢æ•°
# ==================================================
def validate_customer_support_data_specific(df) -> List[str]:
    """ã‚«ã‚¹ã‚¿ãƒãƒ¼ã‚µãƒãƒ¼ãƒˆFAQãƒ‡ãƒ¼ã‚¿ç‰¹æœ‰ã®æ¤œè¨¼"""
    support_issues = []

    # ã‚µãƒãƒ¼ãƒˆé–¢é€£ç”¨èªã®å­˜åœ¨ç¢ºèª
    support_keywords = [
        'å•é¡Œ', 'è§£æ±º', 'ãƒˆãƒ©ãƒ–ãƒ«', 'ã‚¨ãƒ©ãƒ¼', 'ã‚µãƒãƒ¼ãƒˆ', 'ãƒ˜ãƒ«ãƒ—', 'å¯¾å¿œ',
        'problem', 'issue', 'error', 'help', 'support', 'solution', 'troubleshoot'
    ]

    if 'question' in df.columns:
        questions_with_support_terms = 0
        for _, row in df.iterrows():
            question_text = str(row.get('question', '')).lower()
            if any(keyword in question_text for keyword in support_keywords):
                questions_with_support_terms += 1

        support_ratio = (questions_with_support_terms / len(df)) * 100
        support_issues.append(f"ã‚µãƒãƒ¼ãƒˆé–¢é€£ç”¨èªã‚’å«ã‚€è³ªå•: {questions_with_support_terms}ä»¶ ({support_ratio:.1f}%)")

    # å›ç­”ã®é•·ã•åˆ†æ
    if 'answer' in df.columns:
        answer_lengths = df['answer'].astype(str).str.len()
        avg_answer_length = answer_lengths.mean()
        if avg_answer_length < 50:
            support_issues.append(f"âš ï¸ å¹³å‡å›ç­”é•·ãŒçŸ­ã„å¯èƒ½æ€§: {avg_answer_length:.0f}æ–‡å­—")
        else:
            support_issues.append(f"âœ… é©åˆ‡ãªå›ç­”é•·: å¹³å‡{avg_answer_length:.0f}æ–‡å­—")

    return support_issues


def show_usage_instructions(dataset_type: str = "customer_support_faq") -> None:
    """ä½¿ç”¨æ–¹æ³•ã®èª¬æ˜ã‚’è¡¨ç¤º"""
    st.markdown("---")
    st.subheader("ğŸ“– ä½¿ç”¨æ–¹æ³•")
    st.markdown(f"""
    1. **ãƒ¢ãƒ‡ãƒ«é¸æŠ**: ã‚µã‚¤ãƒ‰ãƒãƒ¼ã§RAGç”¨é€”ã«é©ã—ãŸãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠ
    2. **CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰**: question, answer åˆ—ã‚’å«ã‚€CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’é¸æŠ
    3. **å‰å‡¦ç†ã‚’å®Ÿè¡Œ**: ä»¥ä¸‹ã®å‡¦ç†ãŒè‡ªå‹•ã§å®Ÿè¡Œã•ã‚Œã¾ã™ï¼š
       - æ”¹è¡Œã®é™¤å»
       - é€£ç¶šã—ãŸç©ºç™½ã®çµ±ä¸€
       - é‡è¤‡è¡Œã®é™¤å»
       - ç©ºè¡Œã®é™¤å»
       - å¼•ç”¨ç¬¦ã®æ­£è¦åŒ–
    4. **è¤‡æ•°åˆ—çµåˆ**: Vector Store/RAGç”¨ã«æœ€é©åŒ–ã•ã‚ŒãŸè‡ªç„¶ãªæ–‡ç« ã¨ã—ã¦çµåˆ
    5. **ãƒˆãƒ¼ã‚¯ãƒ³ä½¿ç”¨é‡ç¢ºèª**: é¸æŠãƒ¢ãƒ‡ãƒ«ã§ã®ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã¨ã‚³ã‚¹ãƒˆã‚’æ¨å®š
    6. **ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰**: å‰å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã‚’CSVå½¢å¼ã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰

    **Vector Storeç”¨æœ€é©åŒ–:**
    - è‡ªç„¶ãªæ–‡ç« ã¨ã—ã¦çµåˆï¼ˆãƒ©ãƒ™ãƒ«æ–‡å­—åˆ—ãªã—ï¼‰
    - OpenAI embeddingãƒ¢ãƒ‡ãƒ«ã«æœ€é©åŒ–
    - æ¤œç´¢æ€§èƒ½ãŒå‘ä¸Š
    """)


# ==================================================
# ãƒ¡ã‚¤ãƒ³å‡¦ç†é–¢æ•°
# ==================================================
@error_handler
def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†é–¢æ•°"""

    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚¿ã‚¤ãƒ—ã®è¨­å®š
    DATASET_TYPE = "customer_support_faq"

    # ãƒšãƒ¼ã‚¸è¨­å®š
    try:
        st.set_page_config(
            page_title="ã‚«ã‚¹ã‚¿ãƒãƒ¼ã‚µãƒãƒ¼ãƒˆFAQãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†",
            page_icon="ğŸ’¬",
            layout="wide"
        )
    except st.errors.StreamlitAPIException:
        pass

    st.title("ğŸ’¬ ã‚«ã‚¹ã‚¿ãƒãƒ¼ã‚µãƒãƒ¼ãƒˆFAQãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†ã‚¢ãƒ—ãƒª")
    st.markdown("---")

    # =================================================
    # ãƒ¢ãƒ‡ãƒ«é¸æŠæ©Ÿèƒ½
    # =================================================
    st.sidebar.title("ğŸ’¬ ã‚«ã‚¹ã‚¿ãƒãƒ¼ã‚µãƒãƒ¼ãƒˆFAQ")
    st.sidebar.markdown("---")

    # ãƒ¢ãƒ‡ãƒ«é¸æŠ
    selected_model = select_model(key="rag_model_selection")

    # é¸æŠã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«æƒ…å ±ã‚’è¡¨ç¤º
    show_model_info(selected_model)

    st.sidebar.markdown("---")
    # =================================================

    # ã‚µã‚¤ãƒ‰ãƒãƒ¼è¨­å®š
    st.sidebar.header("å‰å‡¦ç†è¨­å®š")
    combine_columns_option = st.sidebar.checkbox(
        "è¤‡æ•°åˆ—ã‚’çµåˆã™ã‚‹ï¼ˆVector Storeç”¨ï¼‰",
        value=True,
        help="è¤‡æ•°åˆ—ã‚’çµåˆã—ã¦RAGç”¨ãƒ†ã‚­ã‚¹ãƒˆã‚’ä½œæˆ"
    )
    show_validation = st.sidebar.checkbox(
        "ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼ã‚’è¡¨ç¤º",
        value=True,
        help="ãƒ‡ãƒ¼ã‚¿ã®å“è³ªæ¤œè¨¼çµæœã‚’è¡¨ç¤º"
    )

    # ã‚«ã‚¹ã‚¿ãƒãƒ¼ã‚µãƒãƒ¼ãƒˆãƒ‡ãƒ¼ã‚¿ç‰¹æœ‰ã®è¨­å®š
    with st.sidebar.expander("ğŸ’¬ ã‚µãƒãƒ¼ãƒˆãƒ‡ãƒ¼ã‚¿è¨­å®š", expanded=False):
        preserve_formatting = st.checkbox(
            "æ›¸å¼ã‚’ä¿è­·",
            value=True,
            help="å›ç­”å†…ã®é‡è¦ãªæ›¸å¼ã‚’ä¿è­·"
        )
        normalize_questions = st.checkbox(
            "è³ªå•ã‚’æ­£è¦åŒ–",
            value=True,
            help="è³ªå•æ–‡ã®è¡¨è¨˜ã‚†ã‚Œã‚’çµ±ä¸€"
        )

    # ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
    st.subheader("ğŸ“ ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰")

    # é¸æŠã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«æƒ…å ±ã‚’è¡¨ç¤ºï¼ˆãƒ¡ã‚¤ãƒ³ã‚¨ãƒªã‚¢ï¼‰
    with st.expander("ğŸ“Š é¸æŠä¸­ã®ãƒ¢ãƒ‡ãƒ«æƒ…å ±", expanded=False):
        col1, col2 = st.columns(2)
        with col1:
            st.info(f"ğŸ¤– é¸æŠãƒ¢ãƒ‡ãƒ«: **{selected_model}**")
        with col2:
            limits = AppConfig.get_model_limits(selected_model)
            st.info(f"ğŸ“ æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³: **{limits['max_tokens']:,}**")

    uploaded_file = st.file_uploader(
        "ã‚«ã‚¹ã‚¿ãƒãƒ¼ã‚µãƒãƒ¼ãƒˆFAQãƒ‡ãƒ¼ã‚¿ã®CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„",
        type=['csv'],
        help="question, answer ã®2åˆ—ã‚’å«ã‚€CSVãƒ•ã‚¡ã‚¤ãƒ«"
    )

    if uploaded_file is not None:
        try:
            # ãƒ•ã‚¡ã‚¤ãƒ«æƒ…å ±ã®ç¢ºèª
            st.info(f"ğŸ“ ãƒ•ã‚¡ã‚¤ãƒ«: {uploaded_file.name} ({uploaded_file.size:,} bytes)")

            # ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã§ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†çŠ¶æ³ã‚’ç®¡ç†
            file_key = f"file_{uploaded_file.name}_{uploaded_file.size}"

            # ãƒ•ã‚¡ã‚¤ãƒ«ãŒå¤‰æ›´ã•ã‚ŒãŸå ´åˆã¯å†èª­ã¿è¾¼ã¿
            if st.session_state.get('current_file_key') != file_key:
                with st.spinner("ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿ä¸­..."):
                    df, validation_results = load_dataset(uploaded_file, DATASET_TYPE)

                # ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã«ä¿å­˜
                st.session_state['current_file_key'] = file_key
                st.session_state['original_df'] = df
                st.session_state['validation_results'] = validation_results
                st.session_state['original_rows'] = len(df)
                st.session_state['file_processed'] = False

                logger.info(f"æ–°ã—ã„ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿: {len(df)}è¡Œ")
            else:
                # ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã‹ã‚‰å–å¾—
                df = st.session_state['original_df']
                validation_results = st.session_state['validation_results']
                logger.info(f"ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã‹ã‚‰ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å–å¾—: {len(df)}è¡Œ")

            st.success(f"ãƒ•ã‚¡ã‚¤ãƒ«ãŒæ­£å¸¸ã«èª­ã¿è¾¼ã¾ã‚Œã¾ã—ãŸã€‚è¡Œæ•°: {len(df)}")

            # å…ƒãƒ‡ãƒ¼ã‚¿ã®è¡¨ç¤º
            st.subheader("ğŸ“‹ å…ƒãƒ‡ãƒ¼ã‚¿ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼")
            st.dataframe(df.head(10))

            # ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼çµæœã®è¡¨ç¤º
            if show_validation:
                st.subheader("ğŸ” ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼")

                # åŸºæœ¬æ¤œè¨¼çµæœ
                for issue in validation_results:
                    st.info(issue)

                # ã‚«ã‚¹ã‚¿ãƒãƒ¼ã‚µãƒãƒ¼ãƒˆãƒ‡ãƒ¼ã‚¿ç‰¹æœ‰ã®æ¤œè¨¼
                support_issues = validate_customer_support_data_specific(df)
                if support_issues:
                    st.write("**ã‚«ã‚¹ã‚¿ãƒãƒ¼ã‚µãƒãƒ¼ãƒˆãƒ‡ãƒ¼ã‚¿ç‰¹æœ‰ã®åˆ†æ:**")
                    for issue in support_issues:
                        st.info(issue)

            # å‰å‡¦ç†å®Ÿè¡Œ
            st.subheader("âš™ï¸ å‰å‡¦ç†å®Ÿè¡Œ")

            if st.button("å‰å‡¦ç†ã‚’å®Ÿè¡Œ", type="primary", key="process_button"):
                try:
                    with st.spinner("å‰å‡¦ç†ä¸­..."):
                        # RAGãƒ‡ãƒ¼ã‚¿ã®å‰å‡¦ç†
                        df_processed = process_rag_data(
                            df.copy(),
                            DATASET_TYPE,
                            combine_columns_option
                        )

                    st.success("å‰å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸï¼")

                    # ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã«å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜
                    st.session_state['processed_df'] = df_processed
                    st.session_state['file_processed'] = True

                    # å‰å‡¦ç†å¾Œã®ãƒ‡ãƒ¼ã‚¿è¡¨ç¤º
                    st.subheader("âœ… å‰å‡¦ç†å¾Œã®ãƒ‡ãƒ¼ã‚¿ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼")
                    st.dataframe(df_processed.head(10))

                    # çµ±è¨ˆæƒ…å ±ã®è¡¨ç¤º
                    display_statistics(df, df_processed, DATASET_TYPE)

                    # é¸æŠã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã§ã®ãƒˆãƒ¼ã‚¯ãƒ³ä½¿ç”¨é‡æ¨å®š
                    estimate_token_usage(df_processed, selected_model)

                    # ã‚«ã‚¹ã‚¿ãƒãƒ¼ã‚µãƒãƒ¼ãƒˆãƒ‡ãƒ¼ã‚¿ç‰¹æœ‰ã®å¾Œå‡¦ç†åˆ†æ
                    if 'Combined_Text' in df_processed.columns:
                        st.subheader("ğŸ’¬ ã‚«ã‚¹ã‚¿ãƒãƒ¼ã‚µãƒãƒ¼ãƒˆãƒ‡ãƒ¼ã‚¿ç‰¹æœ‰ã®åˆ†æ")

                        # çµåˆãƒ†ã‚­ã‚¹ãƒˆã®ã‚µãƒãƒ¼ãƒˆç”¨èªåˆ†æ
                        combined_texts = df_processed['Combined_Text']
                        support_keywords = ['å•é¡Œ', 'ã‚¨ãƒ©ãƒ¼', 'ãƒˆãƒ©ãƒ–ãƒ«', 'ã‚µãƒãƒ¼ãƒˆ', 'ãƒ˜ãƒ«ãƒ—']

                        keyword_counts = {}
                        for keyword in support_keywords:
                            count = combined_texts.str.contains(keyword, case=False).sum()
                            keyword_counts[keyword] = count

                        if keyword_counts:
                            st.write("**ã‚µãƒãƒ¼ãƒˆé–¢é€£ç”¨èªã®å‡ºç¾é »åº¦:**")
                            for keyword, count in keyword_counts.items():
                                percentage = (count / len(df_processed)) * 100
                                st.write(f"- {keyword}: {count}ä»¶ ({percentage:.1f}%)")

                        # è³ªå•ã®é•·ã•åˆ†å¸ƒ
                        if 'question' in df_processed.columns:
                            question_lengths = df_processed['question'].str.len()
                            st.write("**è³ªå•ã®é•·ã•çµ±è¨ˆ:**")
                            col1, col2, col3 = st.columns(3)
                            with col1:
                                st.metric("å¹³å‡è³ªå•é•·", f"{question_lengths.mean():.0f}æ–‡å­—")
                            with col2:
                                st.metric("æœ€é•·è³ªå•", f"{question_lengths.max()}æ–‡å­—")
                            with col3:
                                st.metric("æœ€çŸ­è³ªå•", f"{question_lengths.min()}æ–‡å­—")

                    logger.info(f"ã‚«ã‚¹ã‚¿ãƒãƒ¼ã‚µãƒãƒ¼ãƒˆFAQãƒ‡ãƒ¼ã‚¿å‡¦ç†å®Œäº†: {len(df)} â†’ {len(df_processed)}è¡Œ")

                except Exception as process_error:
                    st.error(f"å‰å‡¦ç†ã‚¨ãƒ©ãƒ¼: {str(process_error)}")
                    logger.error(f"å‰å‡¦ç†ã‚¨ãƒ©ãƒ¼: {process_error}")

            # å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚‹å ´åˆã®ã¿ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒ»ä¿å­˜ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¡¨ç¤º
            if st.session_state.get('file_processed', False) and 'processed_df' in st.session_state:
                df_processed = st.session_state['processed_df']

                # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒ»ä¿å­˜ã‚»ã‚¯ã‚·ãƒ§ãƒ³
                st.subheader("ğŸ’¾ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒ»ä¿å­˜")

                # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ç”¨ãƒ‡ãƒ¼ã‚¿ã®ä½œæˆï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ï¼‰
                if 'download_data' not in st.session_state or st.session_state.get('download_data_key') != file_key:
                    with st.spinner("ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ç”¨ãƒ‡ãƒ¼ã‚¿ã‚’æº–å‚™ä¸­..."):
                        csv_data, text_data = create_download_data(
                            df_processed,
                            combine_columns_option,
                            DATASET_TYPE
                        )
                        st.session_state['download_data'] = (csv_data, text_data)
                        st.session_state['download_data_key'] = file_key
                else:
                    csv_data, text_data = st.session_state['download_data']

                # ãƒ–ãƒ©ã‚¦ã‚¶ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
                st.write("**ğŸ“¥ ãƒ–ãƒ©ã‚¦ã‚¶ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰**")
                col1, col2 = st.columns(2)

                with col1:
                    st.download_button(
                        label="ğŸ“Š CSVå½¢å¼ã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                        data=csv_data,
                        file_name=f"preprocessed_{DATASET_TYPE}_{len(df_processed)}rows.csv",
                        mime="text/csv",
                        help="å‰å‡¦ç†æ¸ˆã¿ã®ã‚«ã‚¹ã‚¿ãƒãƒ¼ã‚µãƒãƒ¼ãƒˆFAQãƒ‡ãƒ¼ã‚¿ã‚’CSVå½¢å¼ã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰"
                    )

                with col2:
                    if text_data:
                        st.download_button(
                            label="ğŸ“ ãƒ†ã‚­ã‚¹ãƒˆå½¢å¼ã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                            data=text_data,
                            file_name=f"customer_support_faq.txt",
                            mime="text/plain",
                            help="Vector Store/RAGç”¨ã«æœ€é©åŒ–ã•ã‚ŒãŸçµåˆãƒ†ã‚­ã‚¹ãƒˆ"
                        )

                # ãƒ­ãƒ¼ã‚«ãƒ«ä¿å­˜
                st.write("**ğŸ’¾ ãƒ­ãƒ¼ã‚«ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜ï¼ˆOUTPUTãƒ•ã‚©ãƒ«ãƒ€ï¼‰**")

                if st.button("ğŸ”„ OUTPUTãƒ•ã‚©ãƒ«ãƒ€ã«ä¿å­˜", type="secondary", key="save_button"):
                    try:
                        with st.spinner("ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜ä¸­..."):
                            saved_files = save_files_to_output(
                                df_processed,
                                DATASET_TYPE,
                                csv_data,
                                text_data
                            )

                        if saved_files:
                            st.success("âœ… ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜å®Œäº†ï¼")

                            # ä¿å­˜ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§ã‚’è¡¨ç¤º
                            with st.expander("ğŸ“‚ ä¿å­˜ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§", expanded=True):
                                for file_type, file_path in saved_files.items():
                                    if Path(file_path).exists():
                                        file_size = Path(file_path).stat().st_size
                                        st.write(f"**{file_type.upper()}**: `{file_path}` ({file_size:,} bytes) âœ…")
                                    else:
                                        st.write(f"**{file_type.upper()}**: `{file_path}` âŒ ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")

                                # OUTPUTãƒ•ã‚©ãƒ«ãƒ€ã®å ´æ‰€ã‚’è¡¨ç¤º
                                output_path = Path("OUTPUT").resolve()
                                st.write(f"**ä¿å­˜å ´æ‰€**: `{output_path}`")
                                file_count = len(list(output_path.glob("*")))
                                st.write(f"**ãƒ•ã‚©ãƒ«ãƒ€å†…ãƒ•ã‚¡ã‚¤ãƒ«æ•°**: {file_count}å€‹")
                        else:
                            st.error("âŒ ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜ã«å¤±æ•—ã—ã¾ã—ãŸ")

                    except Exception as save_error:
                        st.error(f"âŒ ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜ã‚¨ãƒ©ãƒ¼: {str(save_error)}")
                        logger.error(f"ä¿å­˜ã‚¨ãƒ©ãƒ¼: {save_error}")

        except Exception as e:
            st.error(f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
            logger.error(f"ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")

    else:
        st.info("ğŸ‘† CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„")

    # ä½¿ç”¨æ–¹æ³•ã®èª¬æ˜
    show_usage_instructions(DATASET_TYPE)

    # ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã®è¡¨ç¤ºï¼ˆãƒ‡ãƒãƒƒã‚°ç”¨ï¼‰
    if st.sidebar.checkbox("ğŸ”§ ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã‚’è¡¨ç¤º", value=False):
        with st.sidebar.expander("ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹", expanded=False):
            st.write(f"**é¸æŠãƒ¢ãƒ‡ãƒ«**: {selected_model}")
            st.write(f"**ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†æ¸ˆã¿**: {st.session_state.get('file_processed', False)}")

            if 'original_df' in st.session_state:
                df = st.session_state['original_df']
                st.write(f"**å…ƒãƒ‡ãƒ¼ã‚¿**: {len(df)}è¡Œ")

            if 'processed_df' in st.session_state:
                df_processed = st.session_state['processed_df']
                st.write(f"**å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿**: {len(df_processed)}è¡Œ")


# ==================================================
# ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³å®Ÿè¡Œ
# ==================================================
if __name__ == "__main__":
    main()

# å®Ÿè¡Œã‚³ãƒãƒ³ãƒ‰:
# streamlit run a011_make_rag_data_customer.py --server.port=8501
