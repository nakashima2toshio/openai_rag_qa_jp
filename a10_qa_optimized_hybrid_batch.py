#!/usr/bin/env python3
"""
ãƒãƒƒãƒå‡¦ç†ç‰ˆãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰Q&Aç”Ÿæˆã‚·ã‚¹ãƒ†ãƒ 
APIå‘¼ã³å‡ºã—ã‚’æœ€å°åŒ–ã—ã€å‡¦ç†ã‚’é«˜é€ŸåŒ–
# ç¢ºå®Ÿã«95%é”æˆã™ã‚‹ãŸã‚ã®æ¨å¥¨ã‚³ãƒãƒ³ãƒ‰
  python a10_qa_optimized_hybrid_batch.py \
      --dataset cc_news \
      --model gpt-5-mini \
      --batch-size 10 \
      --embedding-batch-size 150 \
      --qa-count 12 \
      --max-docs 150 \
      --output qa_output


ä½¿ç”¨æ–¹æ³•:
    # åŸºæœ¬ä½¿ç”¨ï¼ˆãƒãƒƒãƒã‚µã‚¤ã‚º10ï¼‰
    python a10_qa_optimized_hybrid_batch.py --dataset cc_news

    # ãƒãƒƒãƒã‚µã‚¤ã‚ºæŒ‡å®š
    python a10_qa_optimized_hybrid_batch.py --dataset cc_news --batch-size 20

    # ãƒ¢ãƒ‡ãƒ«æŒ‡å®š
    python a10_qa_optimized_hybrid_batch.py --dataset cc_news --model gpt-5-mini

    # æ¯”è¼ƒå®Ÿè¡Œï¼ˆé€šå¸¸ç‰ˆ vs ãƒãƒƒãƒç‰ˆï¼‰
    python a10_qa_optimized_hybrid_batch.py --dataset cc_news --compare
"""

import os
import sys
import json
import argparse
import pandas as pd
import time
from pathlib import Path
from typing import List, Dict, Optional
from datetime import datetime
import logging
from tqdm import tqdm

# helper_rag_qa ã‹ã‚‰æ–°ã—ã„ãƒãƒƒãƒã‚¯ãƒ©ã‚¹ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from helper_rag_qa import BatchHybridQAGenerator, OptimizedHybridQAGenerator

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# ==========================================
# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆè¨­å®šï¼ˆa10_qa_optimized_hybrid.pyã¨åŒã˜ï¼‰
# ==========================================

DATASET_CONFIGS = {
    "cc_news": {
        "name": "CC-Newsè‹±èªãƒ‹ãƒ¥ãƒ¼ã‚¹",
        "file": "OUTPUT/preprocessed_cc_news.csv",
        "text_column": "Combined_Text",
        "title_column": "title",
        "lang": "en",
        "default_doc_type": "news"
    },
    "japanese_text": {
        "name": "æ—¥æœ¬èªWebãƒ†ã‚­ã‚¹ãƒˆ",
        "file": "OUTPUT/preprocessed_japanese_text.csv",
        "text_column": "Combined_Text",
        "title_column": None,
        "lang": "ja",
        "default_doc_type": "auto"
    },
    "wikipedia_ja": {
        "name": "Wikipediaæ—¥æœ¬èªç‰ˆ",
        "file": "OUTPUT/preprocessed_wikipedia_ja.csv",
        "text_column": "Combined_Text",
        "title_column": "title",
        "lang": "ja",
        "default_doc_type": "academic"
    }
}

# ==========================================
# ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
# ==========================================

def load_preprocessed_data(dataset_type: str, max_docs: Optional[int] = None) -> pd.DataFrame:
    """preprocessedãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿"""
    config = DATASET_CONFIGS.get(dataset_type)
    if not config:
        raise ValueError(f"æœªå¯¾å¿œã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ: {dataset_type}")

    file_path = config["file"]
    if not Path(file_path).exists():
        raise FileNotFoundError(f"ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {file_path}")

    logger.info(f"ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ä¸­: {file_path}")
    df = pd.read_csv(file_path)

    # å¿…è¦ãªã‚«ãƒ©ãƒ ã®ç¢ºèª
    text_col = config["text_column"]
    if text_col not in df.columns:
        raise ValueError(f"ãƒ†ã‚­ã‚¹ãƒˆã‚«ãƒ©ãƒ  '{text_col}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")

    # ç©ºã®ãƒ†ã‚­ã‚¹ãƒˆã‚’é™¤å¤–
    df = df[df[text_col].notna() & (df[text_col].str.strip() != '')]

    # æœ€å¤§æ–‡æ›¸æ•°ã§åˆ¶é™
    if max_docs:
        df = df.head(max_docs)

    logger.info(f"èª­ã¿è¾¼ã¿å®Œäº†: {len(df)}ä»¶ã®ãƒ‡ãƒ¼ã‚¿")
    return df

# ==========================================
# ãƒãƒƒãƒå‡¦ç†ã«ã‚ˆã‚‹Q/Aç”Ÿæˆ
# ==========================================

def generate_batch_qa_from_dataset(
    df: pd.DataFrame,
    dataset_type: str,
    model: str = "gpt-5-mini",
    batch_size: int = 10,
    embedding_batch_size: int = 100,
    qa_count: Optional[int] = None,
    use_llm: bool = True,
    calculate_coverage: bool = True,
    doc_type: Optional[str] = None,
    output_dir: str = "qa_output"
) -> Dict:
    """ãƒãƒƒãƒå‡¦ç†ã§ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‹ã‚‰Q/Aç”Ÿæˆ"""

    config = DATASET_CONFIGS[dataset_type]
    text_col = config["text_column"]
    title_col = config.get("title_column")

    # æ–‡æ›¸ã‚¿ã‚¤ãƒ—ã®æ±ºå®š
    if doc_type is None:
        doc_type = config.get("default_doc_type", "auto")

    logger.info(f"ãƒãƒƒãƒå‡¦ç†Q/Aç”Ÿæˆé–‹å§‹: {len(df)}ä»¶ã®æ–‡æ›¸")
    logger.info(f"ãƒãƒƒãƒã‚µã‚¤ã‚º: LLM={batch_size}, åŸ‹ã‚è¾¼ã¿={embedding_batch_size}")

    # ãƒ†ã‚­ã‚¹ãƒˆãƒªã‚¹ãƒˆã®æº–å‚™
    texts = df[text_col].tolist()

    # ãƒãƒƒãƒç”Ÿæˆå™¨ã®åˆæœŸåŒ–
    generator = BatchHybridQAGenerator(
        model=model,
        batch_size=batch_size,
        embedding_batch_size=embedding_batch_size
    )

    # å‡¦ç†æ™‚é–“ã®è¨ˆæ¸¬é–‹å§‹
    start_time = time.time()

    # ãƒãƒƒãƒå‡¦ç†ã§Q/Aç”Ÿæˆ
    batch_results = generator.generate_batch_hybrid_qa(
        texts=texts,
        qa_count=qa_count,
        use_llm=use_llm,
        calculate_coverage=calculate_coverage,
        document_type=doc_type,
        show_progress=True
    )

    # å‡¦ç†æ™‚é–“ã®è¨ˆç®—
    elapsed_time = time.time() - start_time

    # çµ±è¨ˆæƒ…å ±ã®é›†è¨ˆ
    total_qa_generated = sum(len(r["qa_pairs"]) for r in batch_results)
    total_cost = sum(r["api_usage"]["cost"] for r in batch_results)

    coverage_scores = []
    if calculate_coverage:
        coverage_scores = [r["coverage"].get("coverage_percentage", 0) for r in batch_results]

    # ã‚µãƒãƒªãƒ¼ä½œæˆ
    summary = {
        "dataset_type": dataset_type,
        "dataset_name": config["name"],
        "model_used": model if use_llm else "rule-based",
        "batch_processing": True,
        "batch_sizes": {
            "llm_batch_size": batch_size,
            "embedding_batch_size": embedding_batch_size
        },
        "documents_processed": len(batch_results),
        "total_qa_generated": total_qa_generated,
        "avg_qa_per_doc": total_qa_generated / len(batch_results) if batch_results else 0,
        "processing_time": {
            "total_seconds": elapsed_time,
            "minutes": elapsed_time / 60,
            "docs_per_second": len(batch_results) / elapsed_time if elapsed_time > 0 else 0
        },
        "api_usage": {
            "total_cost": total_cost,
            "cost_per_doc": total_cost / len(batch_results) if batch_results else 0,
            "batch_statistics": generator.batch_stats
        },
        "coverage": {
            "calculated": calculate_coverage,
            "avg_coverage": sum(coverage_scores) / len(coverage_scores) if coverage_scores else 0,
            "min_coverage": min(coverage_scores) if coverage_scores else 0,
            "max_coverage": max(coverage_scores) if coverage_scores else 0
        },
        "generation_timestamp": datetime.now().isoformat()
    }

    # å„æ–‡æ›¸ã«ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’è¿½åŠ 
    for i, result in enumerate(batch_results):
        result['doc_id'] = f"{dataset_type}_{i}"
        result['doc_idx'] = i
        if title_col and title_col in df.columns:
            result['title'] = df.iloc[i][title_col] if pd.notna(df.iloc[i][title_col]) else ""
        else:
            result['title'] = ""
        result['text_length'] = len(texts[i])

    return {
        "summary": summary,
        "results": batch_results
    }

# ==========================================
# é€šå¸¸ç‰ˆã¨ã®æ¯”è¼ƒå®Ÿè¡Œ
# ==========================================

def compare_with_normal_version(
    df: pd.DataFrame,
    dataset_type: str,
    model: str = "gpt-5-mini",
    sample_size: int = 10
) -> Dict:
    """é€šå¸¸ç‰ˆã¨ãƒãƒƒãƒç‰ˆã®æ€§èƒ½æ¯”è¼ƒ"""

    config = DATASET_CONFIGS[dataset_type]
    text_col = config["text_column"]

    # ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿
    sample_df = df.head(sample_size)
    texts = sample_df[text_col].tolist()

    logger.info(f"\n{'='*80}")
    logger.info(f"é€šå¸¸ç‰ˆ vs ãƒãƒƒãƒç‰ˆ æ¯”è¼ƒå®Ÿè¡Œï¼ˆ{sample_size}æ–‡æ›¸ï¼‰")
    logger.info(f"{'='*80}")

    # é€šå¸¸ç‰ˆã®å®Ÿè¡Œ
    logger.info("\nâ–  é€šå¸¸ç‰ˆï¼ˆå€‹åˆ¥å‡¦ç†ï¼‰å®Ÿè¡Œä¸­...")
    normal_generator = OptimizedHybridQAGenerator(model=model)

    normal_start = time.time()
    normal_results = []
    normal_api_calls = 0

    for text in tqdm(texts, desc="é€šå¸¸ç‰ˆ"):
        result = normal_generator.generate_hybrid_qa(
            text=text,
            qa_count=3,
            use_llm=True,
            calculate_coverage=True,
            document_type="auto"
        )
        normal_results.append(result)
        normal_api_calls += 3  # LLM + åŸ‹ã‚è¾¼ã¿Ã—2

    normal_elapsed = time.time() - normal_start

    # ãƒãƒƒãƒç‰ˆã®å®Ÿè¡Œ
    logger.info("\nâ–  ãƒãƒƒãƒç‰ˆï¼ˆãƒãƒƒãƒå‡¦ç†ï¼‰å®Ÿè¡Œä¸­...")
    batch_generator = BatchHybridQAGenerator(model=model, batch_size=5)

    batch_start = time.time()
    batch_results = batch_generator.generate_batch_hybrid_qa(
        texts=texts,
        qa_count=3,
        use_llm=True,
        calculate_coverage=True,
        document_type="auto",
        show_progress=True
    )
    batch_elapsed = time.time() - batch_start

    # æ¯”è¼ƒçµæœ
    comparison = {
        "sample_size": sample_size,
        "normal_version": {
            "total_time": normal_elapsed,
            "time_per_doc": normal_elapsed / sample_size,
            "api_calls": normal_api_calls,
            "calls_per_doc": normal_api_calls / sample_size
        },
        "batch_version": {
            "total_time": batch_elapsed,
            "time_per_doc": batch_elapsed / sample_size,
            "api_calls": batch_generator.batch_stats["total_llm_calls"] +
                        batch_generator.batch_stats["total_embedding_calls"],
            "calls_per_doc": (batch_generator.batch_stats["total_llm_calls"] +
                             batch_generator.batch_stats["total_embedding_calls"]) / sample_size
        },
        "improvement": {
            "time_reduction": f"{(1 - batch_elapsed/normal_elapsed) * 100:.1f}%",
            "api_call_reduction": f"{(1 - (batch_generator.batch_stats['total_llm_calls'] +
                                          batch_generator.batch_stats['total_embedding_calls']) /
                                          normal_api_calls) * 100:.1f}%",
            "speedup": f"{normal_elapsed/batch_elapsed:.2f}x"
        }
    }

    # çµæœè¡¨ç¤º
    print("\n" + "="*80)
    print("ğŸ“Š æ€§èƒ½æ¯”è¼ƒçµæœ")
    print("="*80)
    print(f"ã‚µãƒ³ãƒ—ãƒ«æ•°: {sample_size}æ–‡æ›¸\n")

    print("ã€é€šå¸¸ç‰ˆï¼ˆå€‹åˆ¥å‡¦ç†ï¼‰ã€‘")
    print(f"  å‡¦ç†æ™‚é–“: {normal_elapsed:.2f}ç§’")
    print(f"  APIå‘¼å‡º: {normal_api_calls}å›")
    print(f"  1æ–‡æ›¸ã‚ãŸã‚Š: {normal_elapsed/sample_size:.2f}ç§’, {normal_api_calls/sample_size:.1f}å›\n")

    print("ã€ãƒãƒƒãƒç‰ˆï¼ˆãƒãƒƒãƒå‡¦ç†ï¼‰ã€‘")
    print(f"  å‡¦ç†æ™‚é–“: {batch_elapsed:.2f}ç§’")
    print(f"  APIå‘¼å‡º: {comparison['batch_version']['api_calls']}å›")
    print(f"  1æ–‡æ›¸ã‚ãŸã‚Š: {batch_elapsed/sample_size:.2f}ç§’, "
          f"{comparison['batch_version']['calls_per_doc']:.1f}å›\n")

    print("ã€æ”¹å–„åŠ¹æœã€‘")
    print(f"  å‡¦ç†æ™‚é–“çŸ­ç¸®: {comparison['improvement']['time_reduction']}")
    print(f"  APIå‘¼å‡ºå‰Šæ¸›: {comparison['improvement']['api_call_reduction']}")
    print(f"  é«˜é€ŸåŒ–: {comparison['improvement']['speedup']}")
    print("="*80)

    return comparison

# ==========================================
# çµæœä¿å­˜
# ==========================================

def save_batch_results(
    generation_results: Dict,
    dataset_type: str,
    model: str,
    batch_size: int,
    output_dir: str = "qa_output"
) -> Dict[str, str]:
    """ãƒãƒƒãƒå‡¦ç†çµæœã‚’ä¿å­˜"""

    # qa_output/a10 ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ä¿å­˜
    output_path = Path(output_dir) / "a10"
    output_path.mkdir(parents=True, exist_ok=True)

    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    model_suffix = model.replace("-", "_").replace(".", "_")

    # 1. ã‚µãƒãƒªãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«
    summary_file = output_path / f"batch_summary_{dataset_type}_{model_suffix}_b{batch_size}_{timestamp}.json"
    with open(summary_file, 'w', encoding='utf-8') as f:
        json.dump(generation_results['summary'], f, ensure_ascii=False, indent=2)

    # 2. Q/Aãƒšã‚¢ï¼ˆCSVï¼‰
    qa_data = []
    for doc_result in generation_results['results']:
        doc_id = doc_result['doc_id']
        for qa in doc_result.get('qa_pairs', []):
            qa_data.append({
                'doc_id': doc_id,
                'question': qa.get('question', ''),
                'answer': qa.get('answer', ''),
                'doc_title': doc_result.get('title', ''),
                'text_length': doc_result.get('text_length', 0)
            })

    if qa_data:
        qa_df = pd.DataFrame(qa_data)
        qa_csv = output_path / f"batch_qa_pairs_{dataset_type}_{model_suffix}_b{batch_size}_{timestamp}.csv"
        qa_df.to_csv(qa_csv, index=False, encoding='utf-8')
    else:
        qa_csv = None

    logger.info(f"çµæœã‚’ä¿å­˜ã—ã¾ã—ãŸ: {output_path}")

    return {
        "summary": str(summary_file),
        "qa_pairs_csv": str(qa_csv) if qa_csv else None
    }

# ==========================================
# ãƒ¡ã‚¤ãƒ³å‡¦ç†
# ==========================================

def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    parser = argparse.ArgumentParser(
        description="ãƒãƒƒãƒå‡¦ç†ç‰ˆãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰Q&Aç”Ÿæˆã‚·ã‚¹ãƒ†ãƒ "
    )
    parser.add_argument(
        "--dataset",
        type=str,
        choices=list(DATASET_CONFIGS.keys()),
        default="cc_news",
        help="å‡¦ç†ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ"
    )
    parser.add_argument(
        "--model",
        type=str,
        default="gpt-5-mini",
        help="ä½¿ç”¨ã™ã‚‹LLMãƒ¢ãƒ‡ãƒ«"
    )
    parser.add_argument(
        "--batch-size",
        type=int,
        default=10,
        help="LLMãƒãƒƒãƒã‚µã‚¤ã‚ºï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 10ï¼‰"
    )
    parser.add_argument(
        "--embedding-batch-size",
        type=int,
        default=100,
        help="åŸ‹ã‚è¾¼ã¿ãƒãƒƒãƒã‚µã‚¤ã‚ºï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 100ï¼‰"
    )
    parser.add_argument(
        "--max-docs",
        type=int,
        default=None,
        help="å‡¦ç†ã™ã‚‹æœ€å¤§æ–‡æ›¸æ•°"
    )
    parser.add_argument(
        "--qa-count",
        type=int,
        default=None,
        help="æ–‡æ›¸ã‚ãŸã‚Šã®Q/Aæ•°"
    )
    parser.add_argument(
        "--doc-type",
        type=str,
        choices=["news", "technical", "academic", "auto"],
        default=None,
        help="æ–‡æ›¸ã‚¿ã‚¤ãƒ—"
    )
    parser.add_argument(
        "--no-llm",
        action="store_true",
        help="LLMã‚’ä½¿ç”¨ã—ãªã„"
    )
    parser.add_argument(
        "--no-coverage",
        action="store_true",
        help="ã‚«ãƒãƒ¬ãƒ¼ã‚¸è¨ˆç®—ã‚’è¡Œã‚ãªã„"
    )
    parser.add_argument(
        "--output",
        type=str,
        default="qa_output",
        help="å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª"
    )
    parser.add_argument(
        "--compare",
        action="store_true",
        help="é€šå¸¸ç‰ˆã¨ã®æ¯”è¼ƒå®Ÿè¡Œ"
    )
    parser.add_argument(
        "--compare-size",
        type=int,
        default=10,
        help="æ¯”è¼ƒå®Ÿè¡Œã®ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚º"
    )

    args = parser.parse_args()

    # OpenAI APIã‚­ãƒ¼ã®ç¢ºèª
    if not args.no_llm and not os.getenv('OPENAI_API_KEY'):
        logger.error("OpenAI APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚ç’°å¢ƒå¤‰æ•° OPENAI_API_KEY ã‚’è¨­å®šã—ã¦ãã ã•ã„ã€‚")
        sys.exit(1)

    logger.info(f"""
=====================================
ãƒãƒƒãƒå‡¦ç†ç‰ˆãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰Q&Aç”Ÿæˆ
=====================================
ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ: {DATASET_CONFIGS[args.dataset]['name']}
ãƒ¢ãƒ‡ãƒ«: {args.model if not args.no_llm else 'ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹ã®ã¿'}
ãƒãƒƒãƒã‚µã‚¤ã‚º: LLM={args.batch_size}, åŸ‹ã‚è¾¼ã¿={args.embedding_batch_size}
å‡ºåŠ›å…ˆ: {args.output}
æœ€å¤§æ–‡æ›¸æ•°: {args.max_docs if args.max_docs else 'åˆ¶é™ãªã—'}
""")

    try:
        # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
        logger.info("\n[1/3] ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿...")
        df = load_preprocessed_data(args.dataset, args.max_docs)

        # æ¯”è¼ƒå®Ÿè¡Œãƒ¢ãƒ¼ãƒ‰
        if args.compare:
            logger.info("\n[æ¯”è¼ƒå®Ÿè¡Œãƒ¢ãƒ¼ãƒ‰]")
            comparison_result = compare_with_normal_version(
                df, args.dataset, args.model, args.compare_size
            )

            # æ¯”è¼ƒçµæœã‚’ä¿å­˜
            output_path = Path(args.output)
            output_path.mkdir(exist_ok=True)
            comparison_file = output_path / f"comparison_{args.dataset}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
            with open(comparison_file, 'w', encoding='utf-8') as f:
                json.dump(comparison_result, f, ensure_ascii=False, indent=2)
            logger.info(f"æ¯”è¼ƒçµæœã‚’ä¿å­˜: {comparison_file}")
            return

        # é€šå¸¸ã®ãƒãƒƒãƒå‡¦ç†å®Ÿè¡Œ
        logger.info("\n[2/3] ãƒãƒƒãƒå‡¦ç†Q/Aç”Ÿæˆ...")
        generation_results = generate_batch_qa_from_dataset(
            df,
            args.dataset,
            model=args.model,
            batch_size=args.batch_size,
            embedding_batch_size=args.embedding_batch_size,
            qa_count=args.qa_count,
            use_llm=not args.no_llm,
            calculate_coverage=not args.no_coverage,
            doc_type=args.doc_type,
            output_dir=args.output
        )

        # çµæœä¿å­˜
        logger.info("\n[3/3] çµæœä¿å­˜...")
        saved_files = save_batch_results(
            generation_results,
            args.dataset,
            args.model,
            args.batch_size,
            args.output
        )

        # å®Œäº†ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
        summary = generation_results['summary']
        logger.info(f"""
=====================================
å‡¦ç†å®Œäº†
=====================================
å‡¦ç†æ–‡æ›¸æ•°: {summary['documents_processed']}
ç”ŸæˆQ/Aç·æ•°: {summary['total_qa_generated']}
å¹³å‡Q/Aæ•°/æ–‡æ›¸: {summary['avg_qa_per_doc']:.1f}

å‡¦ç†æ™‚é–“:
- åˆè¨ˆ: {summary['processing_time']['total_seconds']:.2f}ç§’
- åˆ†: {summary['processing_time']['minutes']:.2f}åˆ†
- å‡¦ç†é€Ÿåº¦: {summary['processing_time']['docs_per_second']:.2f}æ–‡æ›¸/ç§’

APIä½¿ç”¨çŠ¶æ³:
- LLMå‘¼ã³å‡ºã—: {summary['api_usage']['batch_statistics']['total_llm_calls']}å›
- åŸ‹ã‚è¾¼ã¿å‘¼ã³å‡ºã—: {summary['api_usage']['batch_statistics']['total_embedding_calls']}å›
- ç·ã‚³ã‚¹ãƒˆ: ${summary['api_usage']['total_cost']:.4f}

ã‚«ãƒãƒ¬ãƒ¼ã‚¸:
- å¹³å‡: {summary['coverage']['avg_coverage']:.1f}%
- æœ€å°: {summary['coverage']['min_coverage']:.1f}%
- æœ€å¤§: {summary['coverage']['max_coverage']:.1f}%

ä¿å­˜ãƒ•ã‚¡ã‚¤ãƒ«:
- ã‚µãƒãƒªãƒ¼: {saved_files['summary']}
- Q/A CSV: {saved_files['qa_pairs_csv']}
""")

        # ãƒãƒƒãƒå‡¦ç†ã®åŠ¹æœã‚’è¡¨ç¤º
        print("\n" + "="*80)
        print("ğŸš€ ãƒãƒƒãƒå‡¦ç†ã®åŠ¹æœ")
        print("="*80)

        original_calls = summary['documents_processed'] * 3  # é€šå¸¸ç‰ˆã®æ¨å®š
        actual_calls = (summary['api_usage']['batch_statistics']['total_llm_calls'] +
                       summary['api_usage']['batch_statistics']['total_embedding_calls'])

        print(f"""
ãƒãƒƒãƒå‡¦ç†ã«ã‚ˆã‚Šä»¥ä¸‹ã®æ”¹å–„ã‚’å®Ÿç¾ï¼š

1. **APIå‘¼ã³å‡ºã—å‰Šæ¸›**
   - é€šå¸¸ç‰ˆï¼ˆæ¨å®šï¼‰: {original_calls}å›
   - ãƒãƒƒãƒç‰ˆï¼ˆå®Ÿéš›ï¼‰: {actual_calls}å›
   - å‰Šæ¸›ç‡: {(1 - actual_calls/original_calls) * 100:.1f}%

2. **å‡¦ç†é€Ÿåº¦å‘ä¸Š**
   - å‡¦ç†é€Ÿåº¦: {summary['processing_time']['docs_per_second']:.2f}æ–‡æ›¸/ç§’
   - {summary['documents_processed']}æ–‡æ›¸ã‚’{summary['processing_time']['minutes']:.1f}åˆ†ã§å‡¦ç†

3. **ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£**
   - å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå‡¦ç†ãŒç¾å®Ÿçš„ã«
   - ãƒ¬ãƒ¼ãƒˆåˆ¶é™ãƒªã‚¹ã‚¯ã®å¤§å¹…ä½æ¸›
""")

    except Exception as e:
        logger.error(f"å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()