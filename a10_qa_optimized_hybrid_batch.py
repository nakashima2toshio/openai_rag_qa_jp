#!/usr/bin/env python3
"""
ãƒãƒƒãƒå‡¦ç†ç‰ˆãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰Q&Aç”Ÿæˆã‚·ã‚¹ãƒ†ãƒ 
APIå‘¼ã³å‡ºã—ã‚’æœ€å°åŒ–ã—ã€å‡¦ç†ã‚’é«˜é€ŸåŒ–

ã€MeCabå¯¾å¿œã€‘
- æ—¥æœ¬èªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼ˆjapanese_text, wikipedia_jaï¼‰ã§ã¯ã€
  ãƒãƒ£ãƒ³ã‚¯ä½œæˆæ™‚ã«MeCabã«ã‚ˆã‚‹é«˜ç²¾åº¦ãªæ–‡å¢ƒç•Œæ¤œå‡ºã‚’è‡ªå‹•çš„ã«ä½¿ç”¨
- è‹±èªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼ˆcc_newsï¼‰ã§ã¯ã€æ­£è¦è¡¨ç¾ãƒ™ãƒ¼ã‚¹ã®æ–‡åˆ†å‰²ã‚’ä½¿ç”¨
- MeCabåˆ©ç”¨å¯å¦ã¯è‡ªå‹•åˆ¤å®šã•ã‚Œã€æœªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ç’°å¢ƒã§ã¯è‡ªå‹•çš„ã«
  æ­£è¦è¡¨ç¾ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯

æœ€é©åŒ–ã‚ªãƒ—ã‚·ãƒ§ãƒ³
å®Ÿè¡Œã‚³ãƒãƒ³ãƒ‰ï¼ˆæœ€é©åŒ–ç‰ˆï¼‰ï¼š
497ä»¶ã®ãƒ‡ãƒ¼ã‚¿
[ãƒãƒƒãƒå‡¦ç†Q/Aç”Ÿæˆ]
å“è³ªé‡è¦–ãƒ¢ãƒ¼ãƒ‰: ç›®æ¨™ã‚«ãƒãƒ¬ãƒ¼ã‚¸ 95%
ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ¢ãƒ¼ãƒ‰: qa_cache
ãƒãƒƒãƒå‡¦ç†Q/Aç”Ÿæˆé–‹å§‹: 497ä»¶ã®æ–‡æ›¸
ãƒãƒƒãƒã‚µã‚¤ã‚º: LLM=10, åŸ‹ã‚è¾¼ã¿=300
ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆè¨€èª: en

1. **APIå‘¼ã³å‡ºã—å‰Šæ¸›**
   - é€šå¸¸ç‰ˆï¼ˆæ¨å®šï¼‰: 1491å›
   - ãƒãƒƒãƒç‰ˆï¼ˆå®Ÿéš›ï¼‰: 110å›
   - å‰Šæ¸›ç‡: 92.6%

2. **å‡¦ç†é€Ÿåº¦å‘ä¸Š**
   - å‡¦ç†é€Ÿåº¦: 0.14æ–‡æ›¸/ç§’
   - 497æ–‡æ›¸ã‚’61.3åˆ†ã§å‡¦ç†

3. **ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£**
   - å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå‡¦ç†ãŒç¾å®Ÿçš„ã«
   - ãƒ¬ãƒ¼ãƒˆåˆ¶é™ãƒªã‚¹ã‚¯ã®å¤§å¹…ä½æ¸›


python a10_qa_optimized_hybrid_batch.py \
--dataset cc_news \
--model gpt-5-mini \
--quality-mode \
--target-coverage 0.95 \
--batch-size 10 \
--embedding-batch-size 300 \
--use-cache \
--cache-dir qa_cache \
--output qa_output

# ã‚­ãƒ£ãƒƒã‚·ãƒ¥æ´»ç”¨ç‰ˆï¼ˆåŒã˜ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®å†å®Ÿè¡Œæ™‚ï¼‰
python a10_qa_optimized_hybrid_batch.py \
--dataset cc_news \
--model gpt-5-mini \
--quality-mode \
--target-coverage 0.95 \
--batch-size 10 \
--embedding-batch-size 300 \
--use-cache \
--cache-dir qa_cache \
--output qa_output

# æ®µéšçš„å“è³ªå‘ä¸Šç‰ˆï¼ˆåˆå›ã¯é€Ÿåº¦å„ªå…ˆã€å¾Œã§å“è³ªå‘ä¸Šï¼‰
python a10_qa_optimized_hybrid_batch.py \
--dataset cc_news \
--model gpt-5-mini \
--progressive-quality \
--initial-coverage 0.85 \
--final-coverage 0.95 \
--batch-size 15 \
--output qa_output


# ç¢ºå®Ÿã«95%é”æˆã™ã‚‹ãŸã‚ã®æ¨å¥¨ã‚³ãƒãƒ³ãƒ‰ï¼ˆå“è³ªé‡è¦–ãƒ¢ãƒ¼ãƒ‰ï¼‰
python a10_qa_optimized_hybrid_batch.py \
  --dataset cc_news \
  --model gpt-5-mini \
  --quality-mode \
  --target-coverage 0.95 \
  --batch-size 5 \
  --embedding-batch-size 150 \
  --output qa_output


ä½¿ç”¨æ–¹æ³•:
    # åŸºæœ¬ä½¿ç”¨ï¼ˆãƒãƒƒãƒã‚µã‚¤ã‚º10ï¼‰
    python a10_qa_optimized_hybrid_batch.py --dataset cc_news

    # å“è³ªé‡è¦–ãƒ¢ãƒ¼ãƒ‰ï¼ˆã‚«ãƒãƒ¬ãƒ¼ã‚¸95%ç›®æ¨™ï¼‰
    python a10_qa_optimized_hybrid_batch.py --dataset cc_news --quality-mode

    # ãƒãƒƒãƒã‚µã‚¤ã‚ºæŒ‡å®š
    python a10_qa_optimized_hybrid_batch.py --dataset cc_news --batch-size 20

    # ãƒ¢ãƒ‡ãƒ«æŒ‡å®š
    python a10_qa_optimized_hybrid_batch.py --dataset cc_news --model gpt-5-mini

    # æ—¥æœ¬èªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼ˆMeCabè‡ªå‹•åˆ©ç”¨ï¼‰
    python a10_qa_optimized_hybrid_batch.py --dataset japanese_text
    python a10_qa_optimized_hybrid_batch.py --dataset wikipedia_ja

    # æ¯”è¼ƒå®Ÿè¡Œï¼ˆé€šå¸¸ç‰ˆ vs ãƒãƒƒãƒç‰ˆï¼‰
    python a10_qa_optimized_hybrid_batch.py --dataset cc_news --compare
"""

import os
import sys
import json
import argparse
import pandas as pd
import time
from pathlib import Path
from typing import List, Dict, Optional
from datetime import datetime
import logging
from tqdm import tqdm

# .envãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ç’°å¢ƒå¤‰æ•°ã‚’èª­ã¿è¾¼ã‚€
from dotenv import load_dotenv
load_dotenv()

# helper_rag_qa ã‹ã‚‰æ–°ã—ã„ãƒãƒƒãƒã‚¯ãƒ©ã‚¹ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from helper_rag_qa import BatchHybridQAGenerator, OptimizedHybridQAGenerator

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# ==========================================
# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆè¨­å®šï¼ˆa10_qa_optimized_hybrid.pyã¨åŒã˜ï¼‰
# ==========================================

DATASET_CONFIGS = {
    "cc_news": {
        "name": "CC-Newsè‹±èªãƒ‹ãƒ¥ãƒ¼ã‚¹",
        "file": "OUTPUT/preprocessed_cc_news.csv",
        "text_column": "Combined_Text",
        "title_column": "title",
        "lang": "en",
        "default_doc_type": "news"
    },
    "japanese_text": {
        "name": "æ—¥æœ¬èªWebãƒ†ã‚­ã‚¹ãƒˆ",
        "file": "OUTPUT/preprocessed_japanese_text.csv",
        "text_column": "Combined_Text",
        "title_column": None,
        "lang": "ja",
        "default_doc_type": "auto"
    },
    "wikipedia_ja": {
        "name": "Wikipediaæ—¥æœ¬èªç‰ˆ",
        "file": "OUTPUT/preprocessed_wikipedia_ja.csv",
        "text_column": "Combined_Text",
        "title_column": "title",
        "lang": "ja",
        "default_doc_type": "academic"
    }
}

# ==========================================
# ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
# ==========================================

def load_preprocessed_data(dataset_type: str, max_docs: Optional[int] = None) -> pd.DataFrame:
    """preprocessedãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿"""
    config = DATASET_CONFIGS.get(dataset_type)
    if not config:
        raise ValueError(f"æœªå¯¾å¿œã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ: {dataset_type}")

    file_path = config["file"]
    if not Path(file_path).exists():
        raise FileNotFoundError(f"ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {file_path}")

    logger.info(f"ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ä¸­: {file_path}")
    df = pd.read_csv(file_path)

    # å¿…è¦ãªã‚«ãƒ©ãƒ ã®ç¢ºèª
    text_col = config["text_column"]
    if text_col not in df.columns:
        raise ValueError(f"ãƒ†ã‚­ã‚¹ãƒˆã‚«ãƒ©ãƒ  '{text_col}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")

    # ç©ºã®ãƒ†ã‚­ã‚¹ãƒˆã‚’é™¤å¤–
    df = df[df[text_col].notna() & (df[text_col].str.strip() != '')]

    # æœ€å¤§æ–‡æ›¸æ•°ã§åˆ¶é™
    if max_docs:
        df = df.head(max_docs)

    logger.info(f"èª­ã¿è¾¼ã¿å®Œäº†: {len(df)}ä»¶ã®ãƒ‡ãƒ¼ã‚¿")
    return df

# ==========================================
# ãƒãƒƒãƒå‡¦ç†ã«ã‚ˆã‚‹Q/Aç”Ÿæˆ
# ==========================================

def generate_batch_qa_from_dataset(
    df: pd.DataFrame,
    dataset_type: str,
    model: str = "gpt-5-mini",
    batch_size: int = 10,
    embedding_batch_size: int = 100,
    qa_count: Optional[int] = None,
    use_llm: bool = True,
    calculate_coverage: bool = True,
    doc_type: Optional[str] = None,
    output_dir: str = "qa_output",
    quality_mode: bool = False,
    target_coverage: float = 0.95,
    use_cache: bool = False,
    cache_dir: str = "qa_cache",
    progressive_quality: bool = False,
    initial_coverage: float = 0.85,
    final_coverage: float = 0.95
) -> Dict:
    """ãƒãƒƒãƒå‡¦ç†ã§ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‹ã‚‰Q/Aç”Ÿæˆ

    Args:
        df: å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ 
        dataset_type: ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚¿ã‚¤ãƒ—
        model: ä½¿ç”¨ã™ã‚‹LLMãƒ¢ãƒ‡ãƒ«
        batch_size: ãƒãƒƒãƒã‚µã‚¤ã‚º
        embedding_batch_size: åŸ‹ã‚è¾¼ã¿ãƒãƒƒãƒã‚µã‚¤ã‚º
        qa_count: Q/Aæ•°
        use_llm: LLMä½¿ç”¨ãƒ•ãƒ©ã‚°
        calculate_coverage: ã‚«ãƒãƒ¬ãƒ¼ã‚¸è¨ˆç®—ãƒ•ãƒ©ã‚°
        doc_type: æ–‡æ›¸ã‚¿ã‚¤ãƒ—
        output_dir: å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        quality_mode: å“è³ªé‡è¦–ãƒ¢ãƒ¼ãƒ‰
        target_coverage: ç›®æ¨™ã‚«ãƒãƒ¬ãƒ¼ã‚¸ç‡
    """

    config = DATASET_CONFIGS[dataset_type]
    text_col = config["text_column"]
    title_col = config.get("title_column")

    # æ–‡æ›¸ã‚¿ã‚¤ãƒ—ã®æ±ºå®š
    if doc_type is None:
        doc_type = config.get("default_doc_type", "auto")

    logger.info(f"ãƒãƒƒãƒå‡¦ç†Q/Aç”Ÿæˆé–‹å§‹: {len(df)}ä»¶ã®æ–‡æ›¸")
    logger.info(f"ãƒãƒƒãƒã‚µã‚¤ã‚º: LLM={batch_size}, åŸ‹ã‚è¾¼ã¿={embedding_batch_size}")

    # è¨€èªæƒ…å ±ã‚’ãƒ­ã‚°å‡ºåŠ›
    lang = config.get("lang", "en")
    logger.info(f"ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆè¨€èª: {lang}")
    if lang == "ja":
        logger.info("  â†’ æ—¥æœ¬èªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ: ãƒãƒ£ãƒ³ã‚¯ä½œæˆæ™‚ã«MeCabã«ã‚ˆã‚‹é«˜ç²¾åº¦æ–‡å¢ƒç•Œæ¤œå‡ºã‚’ä½¿ç”¨ï¼ˆåˆ©ç”¨å¯èƒ½ãªå ´åˆï¼‰")
    else:
        logger.info("  â†’ è‹±èªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ: æ­£è¦è¡¨ç¾ãƒ™ãƒ¼ã‚¹ã®æ–‡åˆ†å‰²ã‚’ä½¿ç”¨")

    # ãƒ†ã‚­ã‚¹ãƒˆãƒªã‚¹ãƒˆã®æº–å‚™
    texts = df[text_col].tolist()

    # ãƒãƒƒãƒç”Ÿæˆå™¨ã®åˆæœŸåŒ–ï¼ˆå“è³ªãƒ¢ãƒ¼ãƒ‰ã‚’è¿½åŠ ï¼‰
    generator = BatchHybridQAGenerator(
        model=model,
        batch_size=batch_size,
        embedding_batch_size=embedding_batch_size,
        quality_mode=quality_mode,
        target_coverage=target_coverage
    )

    # MeCabåˆ©ç”¨çŠ¶æ³ã‚’ç¢ºèªï¼ˆSemanticCoverageã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹çµŒç”±ï¼‰
    if hasattr(generator, 'semantic_coverage') and hasattr(generator.semantic_coverage, 'mecab_available'):
        mecab_status = "åˆ©ç”¨å¯èƒ½" if generator.semantic_coverage.mecab_available else "åˆ©ç”¨ä¸å¯ï¼ˆæ­£è¦è¡¨ç¾ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰"
        logger.info(f"  MeCabçŠ¶æ…‹: {mecab_status}")

    # å‡¦ç†æ™‚é–“ã®è¨ˆæ¸¬é–‹å§‹
    start_time = time.time()

    # ãƒãƒƒãƒå‡¦ç†ã§Q/Aç”Ÿæˆï¼ˆè¨€èªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æ¸¡ã™ï¼‰
    batch_results = generator.generate_batch_hybrid_qa(
        texts=texts,
        qa_count=qa_count,
        use_llm=use_llm,
        calculate_coverage=calculate_coverage,
        document_type=doc_type,
        show_progress=True,
        lang=lang
    )

    # å‡¦ç†æ™‚é–“ã®è¨ˆç®—
    elapsed_time = time.time() - start_time

    # çµ±è¨ˆæƒ…å ±ã®é›†è¨ˆ
    total_qa_generated = sum(len(r["qa_pairs"]) for r in batch_results)
    total_cost = sum(r["api_usage"]["cost"] for r in batch_results)

    coverage_scores = []
    if calculate_coverage:
        coverage_scores = [r["coverage"].get("coverage_percentage", 0) for r in batch_results]

    # ã‚µãƒãƒªãƒ¼ä½œæˆ
    summary = {
        "dataset_type": dataset_type,
        "dataset_name": config["name"],
        "model_used": model if use_llm else "rule-based",
        "batch_processing": True,
        "batch_sizes": {
            "llm_batch_size": batch_size,
            "embedding_batch_size": embedding_batch_size
        },
        "documents_processed": len(batch_results),
        "total_qa_generated": total_qa_generated,
        "avg_qa_per_doc": total_qa_generated / len(batch_results) if batch_results else 0,
        "processing_time": {
            "total_seconds": elapsed_time,
            "minutes": elapsed_time / 60,
            "docs_per_second": len(batch_results) / elapsed_time if elapsed_time > 0 else 0
        },
        "api_usage": {
            "total_cost": total_cost,
            "cost_per_doc": total_cost / len(batch_results) if batch_results else 0,
            "batch_statistics": generator.batch_stats
        },
        "coverage": {
            "calculated": calculate_coverage,
            "avg_coverage": sum(coverage_scores) / len(coverage_scores) if coverage_scores else 0,
            "min_coverage": min(coverage_scores) if coverage_scores else 0,
            "max_coverage": max(coverage_scores) if coverage_scores else 0
        },
        "generation_timestamp": datetime.now().isoformat()
    }

    # å„æ–‡æ›¸ã«ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’è¿½åŠ 
    for i, result in enumerate(batch_results):
        result['doc_id'] = f"{dataset_type}_{i}"
        result['doc_idx'] = i
        if title_col and title_col in df.columns:
            result['title'] = df.iloc[i][title_col] if pd.notna(df.iloc[i][title_col]) else ""
        else:
            result['title'] = ""
        result['text_length'] = len(texts[i])

    return {
        "summary": summary,
        "results": batch_results
    }

# ==========================================
# é€šå¸¸ç‰ˆã¨ã®æ¯”è¼ƒå®Ÿè¡Œ
# ==========================================

def compare_with_normal_version(
    df: pd.DataFrame,
    dataset_type: str,
    model: str = "gpt-5-mini",
    sample_size: int = 10
) -> Dict:
    """é€šå¸¸ç‰ˆã¨ãƒãƒƒãƒç‰ˆã®æ€§èƒ½æ¯”è¼ƒ"""

    config = DATASET_CONFIGS[dataset_type]
    text_col = config["text_column"]

    # ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿
    sample_df = df.head(sample_size)
    texts = sample_df[text_col].tolist()

    logger.info(f"\n{'='*80}")
    logger.info(f"é€šå¸¸ç‰ˆ vs ãƒãƒƒãƒç‰ˆ æ¯”è¼ƒå®Ÿè¡Œï¼ˆ{sample_size}æ–‡æ›¸ï¼‰")
    logger.info(f"{'='*80}")

    # é€šå¸¸ç‰ˆã®å®Ÿè¡Œ
    logger.info("\nâ–  é€šå¸¸ç‰ˆï¼ˆå€‹åˆ¥å‡¦ç†ï¼‰å®Ÿè¡Œä¸­...")
    normal_generator = OptimizedHybridQAGenerator(model=model)

    normal_start = time.time()
    normal_results = []
    normal_api_calls = 0

    for text in tqdm(texts, desc="é€šå¸¸ç‰ˆ"):
        result = normal_generator.generate_hybrid_qa(
            text=text,
            qa_count=3,
            use_llm=True,
            calculate_coverage=True,
            document_type="auto"
        )
        normal_results.append(result)
        normal_api_calls += 3  # LLM + åŸ‹ã‚è¾¼ã¿Ã—2

    normal_elapsed = time.time() - normal_start

    # ãƒãƒƒãƒç‰ˆã®å®Ÿè¡Œ
    logger.info("\nâ–  ãƒãƒƒãƒç‰ˆï¼ˆãƒãƒƒãƒå‡¦ç†ï¼‰å®Ÿè¡Œä¸­...")
    batch_generator = BatchHybridQAGenerator(model=model, batch_size=5)

    batch_start = time.time()
    batch_results = batch_generator.generate_batch_hybrid_qa(
        texts=texts,
        qa_count=3,
        use_llm=True,
        calculate_coverage=True,
        document_type="auto",
        show_progress=True
    )
    batch_elapsed = time.time() - batch_start

    # æ¯”è¼ƒçµæœ
    comparison = {
        "sample_size": sample_size,
        "normal_version": {
            "total_time": normal_elapsed,
            "time_per_doc": normal_elapsed / sample_size,
            "api_calls": normal_api_calls,
            "calls_per_doc": normal_api_calls / sample_size
        },
        "batch_version": {
            "total_time": batch_elapsed,
            "time_per_doc": batch_elapsed / sample_size,
            "api_calls": batch_generator.batch_stats["total_llm_calls"] +
                        batch_generator.batch_stats["total_embedding_calls"],
            "calls_per_doc": (batch_generator.batch_stats["total_llm_calls"] +
                             batch_generator.batch_stats["total_embedding_calls"]) / sample_size
        },
        "improvement": {
            "time_reduction": f"{(1 - batch_elapsed/normal_elapsed) * 100:.1f}%",
            "api_call_reduction": f"{(1 - (batch_generator.batch_stats['total_llm_calls'] +
                                          batch_generator.batch_stats['total_embedding_calls']) /
                                          normal_api_calls) * 100:.1f}%",
            "speedup": f"{normal_elapsed/batch_elapsed:.2f}x"
        }
    }

    # çµæœè¡¨ç¤º
    print("\n" + "="*80)
    print("ğŸ“Š æ€§èƒ½æ¯”è¼ƒçµæœ")
    print("="*80)
    print(f"ã‚µãƒ³ãƒ—ãƒ«æ•°: {sample_size}æ–‡æ›¸\n")

    print("ã€é€šå¸¸ç‰ˆï¼ˆå€‹åˆ¥å‡¦ç†ï¼‰ã€‘")
    print(f"  å‡¦ç†æ™‚é–“: {normal_elapsed:.2f}ç§’")
    print(f"  APIå‘¼å‡º: {normal_api_calls}å›")
    print(f"  1æ–‡æ›¸ã‚ãŸã‚Š: {normal_elapsed/sample_size:.2f}ç§’, {normal_api_calls/sample_size:.1f}å›\n")

    print("ã€ãƒãƒƒãƒç‰ˆï¼ˆãƒãƒƒãƒå‡¦ç†ï¼‰ã€‘")
    print(f"  å‡¦ç†æ™‚é–“: {batch_elapsed:.2f}ç§’")
    print(f"  APIå‘¼å‡º: {comparison['batch_version']['api_calls']}å›")
    print(f"  1æ–‡æ›¸ã‚ãŸã‚Š: {batch_elapsed/sample_size:.2f}ç§’, "
          f"{comparison['batch_version']['calls_per_doc']:.1f}å›\n")

    print("ã€æ”¹å–„åŠ¹æœã€‘")
    print(f"  å‡¦ç†æ™‚é–“çŸ­ç¸®: {comparison['improvement']['time_reduction']}")
    print(f"  APIå‘¼å‡ºå‰Šæ¸›: {comparison['improvement']['api_call_reduction']}")
    print(f"  é«˜é€ŸåŒ–: {comparison['improvement']['speedup']}")
    print("="*80)

    return comparison

# ==========================================
# çµæœä¿å­˜
# ==========================================

def save_batch_results(
    generation_results: Dict,
    dataset_type: str,
    model: str,
    batch_size: int,
    output_dir: str = "qa_output"
) -> Dict[str, str]:
    """ãƒãƒƒãƒå‡¦ç†çµæœã‚’ä¿å­˜"""

    # qa_output/a10 ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ä¿å­˜
    output_path = Path(output_dir) / "a10"
    output_path.mkdir(parents=True, exist_ok=True)

    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    model_suffix = model.replace("-", "_").replace(".", "_")

    # å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰ dataset_name ã‚’æŠ½å‡º
    config = DATASET_CONFIGS[dataset_type]
    input_file = config["file"]
    # ä¾‹: "OUTPUT/preprocessed_cc_news.csv" â†’ "cc_news"
    dataset_name = Path(input_file).stem.replace("preprocessed_", "")

    # 1. ã‚µãƒãƒªãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«
    summary_file = output_path / f"batch_summary_{dataset_name}_{model_suffix}_b{batch_size}_{timestamp}.json"
    with open(summary_file, 'w', encoding='utf-8') as f:
        json.dump(generation_results['summary'], f, ensure_ascii=False, indent=2)

    # 2. Q/Aãƒšã‚¢ï¼ˆCSVï¼‰
    qa_data = []
    for doc_result in generation_results['results']:
        doc_id = doc_result['doc_id']
        for qa in doc_result.get('qa_pairs', []):
            qa_data.append({
                'doc_id': doc_id,
                'question': qa.get('question', ''),
                'answer': qa.get('answer', ''),
                'doc_title': doc_result.get('title', ''),
                'text_length': doc_result.get('text_length', 0)
            })

    if qa_data:
        qa_df = pd.DataFrame(qa_data)
        qa_csv = output_path / f"batch_qa_pairs_{dataset_name}_{model_suffix}_b{batch_size}_{timestamp}.csv"
        qa_df.to_csv(qa_csv, index=False, encoding='utf-8')
    else:
        qa_csv = None

    logger.info(f"çµæœã‚’ä¿å­˜ã—ã¾ã—ãŸ: {output_path}")

    return {
        "summary": str(summary_file),
        "qa_pairs_csv": str(qa_csv) if qa_csv else None
    }

# ==========================================
# ãƒ¡ã‚¤ãƒ³å‡¦ç†
# ==========================================

def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    parser = argparse.ArgumentParser(
        description="ãƒãƒƒãƒå‡¦ç†ç‰ˆãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰Q&Aç”Ÿæˆã‚·ã‚¹ãƒ†ãƒ "
    )
    parser.add_argument(
        "--dataset",
        type=str,
        choices=list(DATASET_CONFIGS.keys()),
        default="cc_news",
        help="å‡¦ç†ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ"
    )
    parser.add_argument(
        "--model",
        type=str,
        default="gpt-5-mini",
        help="ä½¿ç”¨ã™ã‚‹LLMãƒ¢ãƒ‡ãƒ«"
    )
    parser.add_argument(
        "--batch-size",
        type=int,
        default=10,
        help="LLMãƒãƒƒãƒã‚µã‚¤ã‚ºï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 10ï¼‰"
    )
    parser.add_argument(
        "--embedding-batch-size",
        type=int,
        default=100,
        help="åŸ‹ã‚è¾¼ã¿ãƒãƒƒãƒã‚µã‚¤ã‚ºï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 100ï¼‰"
    )
    parser.add_argument(
        "--max-docs",
        type=int,
        default=None,
        help="å‡¦ç†ã™ã‚‹æœ€å¤§æ–‡æ›¸æ•°"
    )
    parser.add_argument(
        "--qa-count",
        type=int,
        default=None,
        help="æ–‡æ›¸ã‚ãŸã‚Šã®Q/Aæ•°"
    )
    parser.add_argument(
        "--doc-type",
        type=str,
        choices=["news", "technical", "academic", "auto"],
        default=None,
        help="æ–‡æ›¸ã‚¿ã‚¤ãƒ—"
    )
    parser.add_argument(
        "--no-llm",
        action="store_true",
        help="LLMã‚’ä½¿ç”¨ã—ãªã„"
    )
    parser.add_argument(
        "--no-coverage",
        action="store_true",
        help="ã‚«ãƒãƒ¬ãƒ¼ã‚¸è¨ˆç®—ã‚’è¡Œã‚ãªã„"
    )
    parser.add_argument(
        "--output",
        type=str,
        default="qa_output",
        help="å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª"
    )
    parser.add_argument(
        "--compare",
        action="store_true",
        help="é€šå¸¸ç‰ˆã¨ã®æ¯”è¼ƒå®Ÿè¡Œ"
    )
    parser.add_argument(
        "--compare-size",
        type=int,
        default=10,
        help="æ¯”è¼ƒå®Ÿè¡Œã®ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚º"
    )
    parser.add_argument(
        "--quality-mode",
        action="store_true",
        help="å“è³ªé‡è¦–ãƒ¢ãƒ¼ãƒ‰ï¼ˆã‚«ãƒãƒ¬ãƒ¼ã‚¸95%ç›®æ¨™ï¼‰"
    )
    parser.add_argument(
        "--target-coverage",
        type=float,
        default=0.95,
        help="ç›®æ¨™ã‚«ãƒãƒ¬ãƒ¼ã‚¸ç‡ï¼ˆå“è³ªãƒ¢ãƒ¼ãƒ‰æ™‚ï¼‰"
    )
    parser.add_argument(
        "--use-cache",
        action="store_true",
        help="åŸ‹ã‚è¾¼ã¿ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ä½¿ç”¨"
    )
    parser.add_argument(
        "--cache-dir",
        type=str,
        default="qa_cache",
        help="ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª"
    )
    parser.add_argument(
        "--progressive-quality",
        action="store_true",
        help="æ®µéšçš„å“è³ªå‘ä¸Šãƒ¢ãƒ¼ãƒ‰"
    )
    parser.add_argument(
        "--initial-coverage",
        type=float,
        default=0.85,
        help="åˆæœŸç›®æ¨™ã‚«ãƒãƒ¬ãƒ¼ã‚¸ç‡ï¼ˆæ®µéšçš„å“è³ªå‘ä¸Šãƒ¢ãƒ¼ãƒ‰æ™‚ï¼‰"
    )
    parser.add_argument(
        "--final-coverage",
        type=float,
        default=0.95,
        help="æœ€çµ‚ç›®æ¨™ã‚«ãƒãƒ¬ãƒ¼ã‚¸ç‡ï¼ˆæ®µéšçš„å“è³ªå‘ä¸Šãƒ¢ãƒ¼ãƒ‰æ™‚ï¼‰"
    )

    args = parser.parse_args()

    # OpenAI APIã‚­ãƒ¼ã®ç¢ºèª
    if not args.no_llm and not os.getenv('OPENAI_API_KEY'):
        logger.error("OpenAI APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚ç’°å¢ƒå¤‰æ•° OPENAI_API_KEY ã‚’è¨­å®šã—ã¦ãã ã•ã„ã€‚")
        sys.exit(1)

    logger.info(f"""
=====================================
ãƒãƒƒãƒå‡¦ç†ç‰ˆãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰Q&Aç”Ÿæˆ
=====================================
ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ: {DATASET_CONFIGS[args.dataset]['name']}
ãƒ¢ãƒ‡ãƒ«: {args.model if not args.no_llm else 'ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹ã®ã¿'}
ãƒãƒƒãƒã‚µã‚¤ã‚º: LLM={args.batch_size}, åŸ‹ã‚è¾¼ã¿={args.embedding_batch_size}
å‡ºåŠ›å…ˆ: {args.output}
æœ€å¤§æ–‡æ›¸æ•°: {args.max_docs if args.max_docs else 'åˆ¶é™ãªã—'}
""")

    try:
        # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
        logger.info("\n[1/3] ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿...")
        df = load_preprocessed_data(args.dataset, args.max_docs)

        # æ¯”è¼ƒå®Ÿè¡Œãƒ¢ãƒ¼ãƒ‰
        if args.compare:
            logger.info("\n[æ¯”è¼ƒå®Ÿè¡Œãƒ¢ãƒ¼ãƒ‰]")
            comparison_result = compare_with_normal_version(
                df, args.dataset, args.model, args.compare_size
            )

            # æ¯”è¼ƒçµæœã‚’ä¿å­˜
            output_path = Path(args.output)
            output_path.mkdir(exist_ok=True)
            comparison_file = output_path / f"comparison_{args.dataset}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
            with open(comparison_file, 'w', encoding='utf-8') as f:
                json.dump(comparison_result, f, ensure_ascii=False, indent=2)
            logger.info(f"æ¯”è¼ƒçµæœã‚’ä¿å­˜: {comparison_file}")
            return

        # é€šå¸¸ã®ãƒãƒƒãƒå‡¦ç†å®Ÿè¡Œ
        logger.info("\n[2/3] ãƒãƒƒãƒå‡¦ç†Q/Aç”Ÿæˆ...")
        if args.quality_mode:
            logger.info(f"ğŸ¯ å“è³ªé‡è¦–ãƒ¢ãƒ¼ãƒ‰: ç›®æ¨™ã‚«ãƒãƒ¬ãƒ¼ã‚¸ {args.target_coverage*100:.0f}%")
        if args.use_cache:
            logger.info(f"ğŸ“¦ ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ¢ãƒ¼ãƒ‰: {args.cache_dir}")
        if args.progressive_quality:
            logger.info(f"ğŸ“ˆ æ®µéšçš„å“è³ªå‘ä¸Šãƒ¢ãƒ¼ãƒ‰: {args.initial_coverage*100:.0f}% â†’ {args.final_coverage*100:.0f}%")

        generation_results = generate_batch_qa_from_dataset(
            df,
            args.dataset,
            model=args.model,
            batch_size=args.batch_size,
            embedding_batch_size=args.embedding_batch_size,
            qa_count=args.qa_count,
            use_llm=not args.no_llm,
            calculate_coverage=not args.no_coverage,
            doc_type=args.doc_type,
            output_dir=args.output,
            quality_mode=args.quality_mode,
            target_coverage=args.target_coverage,
            use_cache=args.use_cache,
            cache_dir=args.cache_dir,
            progressive_quality=args.progressive_quality,
            initial_coverage=args.initial_coverage,
            final_coverage=args.final_coverage
        )

        # çµæœä¿å­˜
        logger.info("\n[3/3] çµæœä¿å­˜...")
        saved_files = save_batch_results(
            generation_results,
            args.dataset,
            args.model,
            args.batch_size,
            args.output
        )

        # å®Œäº†ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
        summary = generation_results['summary']
        logger.info(f"""
=====================================
å‡¦ç†å®Œäº†
=====================================
å‡¦ç†æ–‡æ›¸æ•°: {summary['documents_processed']}
ç”ŸæˆQ/Aç·æ•°: {summary['total_qa_generated']}
å¹³å‡Q/Aæ•°/æ–‡æ›¸: {summary['avg_qa_per_doc']:.1f}

å‡¦ç†æ™‚é–“:
- åˆè¨ˆ: {summary['processing_time']['total_seconds']:.2f}ç§’
- åˆ†: {summary['processing_time']['minutes']:.2f}åˆ†
- å‡¦ç†é€Ÿåº¦: {summary['processing_time']['docs_per_second']:.2f}æ–‡æ›¸/ç§’

APIä½¿ç”¨çŠ¶æ³:
- LLMå‘¼ã³å‡ºã—: {summary['api_usage']['batch_statistics']['total_llm_calls']}å›
- åŸ‹ã‚è¾¼ã¿å‘¼ã³å‡ºã—: {summary['api_usage']['batch_statistics']['total_embedding_calls']}å›
- ç·ã‚³ã‚¹ãƒˆ: ${summary['api_usage']['total_cost']:.4f}

ã‚«ãƒãƒ¬ãƒ¼ã‚¸:
- å¹³å‡: {summary['coverage']['avg_coverage']:.1f}%
- æœ€å°: {summary['coverage']['min_coverage']:.1f}%
- æœ€å¤§: {summary['coverage']['max_coverage']:.1f}%

ä¿å­˜ãƒ•ã‚¡ã‚¤ãƒ«:
- ã‚µãƒãƒªãƒ¼: {saved_files['summary']}
- Q/A CSV: {saved_files['qa_pairs_csv']}
""")

        # ãƒãƒƒãƒå‡¦ç†ã®åŠ¹æœã‚’è¡¨ç¤º
        print("\n" + "="*80)
        print("ğŸš€ ãƒãƒƒãƒå‡¦ç†ã®åŠ¹æœ")
        print("="*80)

        original_calls = summary['documents_processed'] * 3  # é€šå¸¸ç‰ˆã®æ¨å®š
        actual_calls = (summary['api_usage']['batch_statistics']['total_llm_calls'] +
                       summary['api_usage']['batch_statistics']['total_embedding_calls'])

        print(f"""
ãƒãƒƒãƒå‡¦ç†ã«ã‚ˆã‚Šä»¥ä¸‹ã®æ”¹å–„ã‚’å®Ÿç¾ï¼š

1. **APIå‘¼ã³å‡ºã—å‰Šæ¸›**
   - é€šå¸¸ç‰ˆï¼ˆæ¨å®šï¼‰: {original_calls}å›
   - ãƒãƒƒãƒç‰ˆï¼ˆå®Ÿéš›ï¼‰: {actual_calls}å›
   - å‰Šæ¸›ç‡: {(1 - actual_calls/original_calls) * 100:.1f}%

2. **å‡¦ç†é€Ÿåº¦å‘ä¸Š**
   - å‡¦ç†é€Ÿåº¦: {summary['processing_time']['docs_per_second']:.2f}æ–‡æ›¸/ç§’
   - {summary['documents_processed']}æ–‡æ›¸ã‚’{summary['processing_time']['minutes']:.1f}åˆ†ã§å‡¦ç†

3. **ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£**
   - å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå‡¦ç†ãŒç¾å®Ÿçš„ã«
   - ãƒ¬ãƒ¼ãƒˆåˆ¶é™ãƒªã‚¹ã‚¯ã®å¤§å¹…ä½æ¸›
""")

    except Exception as e:
        logger.error(f"å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()