# a31_make_cloud_vector_store_vsid.py
# Vector Storeä½œæˆStreamlitã‚¢ãƒ—ãƒªï¼ˆå®Œå…¨ä¿®æ­£ç‰ˆï¼‰
# streamlit run a31_make_cloud_vector_store_vsid.py --server.port=8502

import streamlit as st
import pandas as pd
import os
import re
import time
import json
import tempfile
from pathlib import Path
from typing import List, Dict, Any, Optional, Tuple, Callable
from datetime import datetime
import logging
from dataclasses import dataclass
from abc import ABC, abstractmethod

# OpenAI SDK ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
try:
    from openai import OpenAI

    OPENAI_AVAILABLE = True
except ImportError as e:
    OPENAI_AVAILABLE = False
    st.error(f"OpenAI SDK ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {e}")
    st.stop()

# å…±é€šæ©Ÿèƒ½ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
try:
    from helper_rag import (
        AppConfig, RAGConfig, TokenManager, safe_execute,
        select_model, show_model_info,
        setup_page_config, setup_page_header, setup_sidebar_header,
        create_output_directory
    )

    HELPER_AVAILABLE = True
except ImportError as e:
    HELPER_AVAILABLE = False
    logging.warning(f"ãƒ˜ãƒ«ãƒ‘ãƒ¼ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã«å¤±æ•—: {e}")

# ===================================================================
# ãƒ­ã‚°è¨­å®š
# ===================================================================
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


# ===================================================================
# è¨­å®šã‚¯ãƒ©ã‚¹
# ===================================================================
@dataclass
class VectorStoreConfig:
    """Vector Storeè¨­å®šãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ©ã‚¹"""
    dataset_type: str
    filename: str
    store_name: str
    description: str
    chunk_size: int = 1000
    overlap: int = 100
    max_file_size_mb: int = 400  # OpenAIåˆ¶é™ã‚ˆã‚Šå°‘ã—ä½™è£•ã‚’æŒã£ã¦è¨­å®š
    max_chunks_per_file: int = 40000  # ãƒãƒ£ãƒ³ã‚¯æ•°åˆ¶é™
    csv_text_column: str = "Combined_Text"  # CSVãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰èª­ã¿è¾¼ã‚€ãƒ†ã‚­ã‚¹ãƒˆã‚«ãƒ©ãƒ å

    @classmethod
    def get_unified_config(cls) -> 'VectorStoreConfig':
        """çµ±åˆVector Storeç”¨è¨­å®šã‚’å–å¾—"""
        return cls(
            dataset_type="unified_all",
            filename="unified_datasets.csv",  # ä»®æƒ³ãƒ•ã‚¡ã‚¤ãƒ«å
            store_name="Unified Knowledge Base - All Domains",
            description="å…¨ãƒ‰ãƒ¡ã‚¤ãƒ³çµ±åˆãƒŠãƒ¬ãƒƒã‚¸ãƒ™ãƒ¼ã‚¹ï¼ˆåŒ»ç™‚ãƒ»æ³•å¾‹ãƒ»ç§‘å­¦ãƒ»FAQãƒ»é›‘å­¦ï¼‰",
            chunk_size=3000,  # ä¸­é–“çš„ãªã‚µã‚¤ã‚º
            overlap=200,
            max_file_size_mb=100,  # çµ±åˆæ™‚ã®åˆ¶é™ã‚’ç·©å’Œ
            max_chunks_per_file=50000,  # ãƒãƒ£ãƒ³ã‚¯æ•°åˆ¶é™ã‚’æ‹¡å¤§
            csv_text_column="Combined_Text"
        )
    
    @classmethod
    def get_all_configs(cls) -> Dict[str, 'VectorStoreConfig']:
        """å…¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆè¨­å®šã‚’å–å¾—ï¼ˆCSVãƒ•ã‚¡ã‚¤ãƒ«å¯¾å¿œç‰ˆï¼‰"""
        return {
            "a02_cc_news": cls(
                dataset_type="a02_cc_news",
                filename="a02_qa_pairs_cc_news.csv",
                store_name="CC News Q&A - Basic Generation (a02_make_qa)",
                description="CC Newsãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆQ&Aï¼ˆåŸºæœ¬ç”Ÿæˆæ–¹å¼ï¼‰",
                chunk_size=2000,
                overlap=100,
                max_file_size_mb=30,
                max_chunks_per_file=4000,
                csv_text_column="question"  # questionã‚«ãƒ©ãƒ ã‚’ä½¿ç”¨
            ),
            "a02_livedoor": cls(
                dataset_type="a02_livedoor",
                filename="a02_qa_pairs_livedoor.csv",
                store_name="Livedoor Q&A - Basic Generation (a02_make_qa)",
                description="Livedoorãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆQ&Aï¼ˆåŸºæœ¬ç”Ÿæˆæ–¹å¼ï¼‰",
                chunk_size=2000,
                overlap=100,
                max_file_size_mb=30,
                max_chunks_per_file=4000,
                csv_text_column="question"
            ),
            "a03_cc_news": cls(
                dataset_type="a03_cc_news",
                filename="a03_qa_pairs_cc_news.csv",
                store_name="CC News Q&A - Coverage Improved (a03_coverage)",
                description="CC Newsãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆQ&Aï¼ˆã‚«ãƒãƒ¬ãƒƒã‚¸æ”¹è‰¯æ–¹å¼ï¼‰",
                chunk_size=2000,
                overlap=100,
                max_file_size_mb=30,
                max_chunks_per_file=4000,
                csv_text_column="question"
            ),
            "a03_livedoor": cls(
                dataset_type="a03_livedoor",
                filename="a03_qa_pairs_livedoor.csv",
                store_name="Livedoor Q&A - Coverage Improved (a03_coverage)",
                description="Livedoorãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆQ&Aï¼ˆã‚«ãƒãƒ¬ãƒƒã‚¸æ”¹è‰¯æ–¹å¼ï¼‰",
                chunk_size=2000,
                overlap=100,
                max_file_size_mb=30,
                max_chunks_per_file=4000,
                csv_text_column="question"
            ),
            "a10_cc_news": cls(
                dataset_type="a10_cc_news",
                filename="a10_qa_pairs_cc_news.csv",
                store_name="CC News Q&A - Hybrid Method (a10_hybrid)",
                description="CC Newsãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆQ&Aï¼ˆãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ç”Ÿæˆæ–¹å¼ï¼‰",
                chunk_size=2000,
                overlap=100,
                max_file_size_mb=30,
                max_chunks_per_file=4000,
                csv_text_column="question"
            ),
            "a10_livedoor": cls(
                dataset_type="a10_livedoor",
                filename="a10_qa_pairs_livedoor.csv",
                store_name="Livedoor Q&A - Hybrid Method (a10_hybrid)",
                description="Livedoorãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆQ&Aï¼ˆãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ç”Ÿæˆæ–¹å¼ï¼‰",
                chunk_size=2000,
                overlap=100,
                max_file_size_mb=30,
                max_chunks_per_file=4000,
                csv_text_column="question"
            )
        }


# ===================================================================
# Vector Storeå‡¦ç†ã‚¯ãƒ©ã‚¹
# ===================================================================
class VectorStoreProcessor:
    """Vector Storeç”¨ãƒ‡ãƒ¼ã‚¿å‡¦ç†ã‚¯ãƒ©ã‚¹"""

    def __init__(self):
        self.configs = VectorStoreConfig.get_all_configs()

    def load_csv_file(self, filepath: Path, text_column: str = "Combined_Text") -> List[str]:
        """CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿ã€æŒ‡å®šã‚«ãƒ©ãƒ ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒªã‚¹ãƒˆã¨ã—ã¦è¿”ã™"""
        try:
            # CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿
            df = pd.read_csv(filepath, encoding='utf-8')
            
            # æŒ‡å®šã‚«ãƒ©ãƒ ãŒå­˜åœ¨ã™ã‚‹ã‹ç¢ºèª
            if text_column not in df.columns:
                logger.error(f"æŒ‡å®šã•ã‚ŒãŸã‚«ãƒ©ãƒ  '{text_column}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚åˆ©ç”¨å¯èƒ½ãªã‚«ãƒ©ãƒ : {df.columns.tolist()}")
                return []
            
            # ãƒ†ã‚­ã‚¹ãƒˆã‚«ãƒ©ãƒ ã‹ã‚‰å€¤ã‚’å–å¾—ï¼ˆNaNã‚’é™¤å¤–ï¼‰
            texts = df[text_column].dropna().astype(str).tolist()
            
            # ç©ºæ–‡å­—åˆ—ã¨çŸ­ã™ãã‚‹ãƒ†ã‚­ã‚¹ãƒˆã‚’é™¤å»
            cleaned_lines = []
            for text in texts:
                text = text.strip()
                if text and len(text) > 10:  # 10æ–‡å­—ä»¥ä¸Šã®ãƒ†ã‚­ã‚¹ãƒˆã®ã¿ä¿æŒ
                    cleaned_lines.append(text)

            logger.info(f"CSVãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿å®Œäº†: {filepath.name} - {len(cleaned_lines)}ä»¶ã®ãƒ†ã‚­ã‚¹ãƒˆ")
            return cleaned_lines

        except FileNotFoundError:
            logger.error(f"ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {filepath}")
            return []
        except pd.errors.EmptyDataError:
            logger.error(f"CSVãƒ•ã‚¡ã‚¤ãƒ«ãŒç©ºã§ã™: {filepath}")
            return []
        except Exception as e:
            logger.error(f"CSVãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {filepath} - {e}")
            return []

    def chunk_text(self, text: str, chunk_size: int = 1000, overlap: int = 100) -> List[str]:
        """é•·ã„ãƒ†ã‚­ã‚¹ãƒˆã‚’æŒ‡å®šã‚µã‚¤ã‚ºã®ãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰²"""
        if len(text) <= chunk_size:
            return [text]

        chunks = []
        start = 0

        while start < len(text):
            end = start + chunk_size

            # æ–‡ã®å¢ƒç•Œã§åˆ†å‰²ã™ã‚‹ã‚ˆã†ã«èª¿æ•´
            if end < len(text):
                # å¥èª­ç‚¹ã‚’æ¢ã™
                for punct in ['ã€‚', 'ï¼', 'ï¼Ÿ', '.', '!', '?']:
                    punct_pos = text.rfind(punct, start, end)
                    if punct_pos > start + chunk_size // 2:
                        end = punct_pos + 1
                        break

            chunk = text[start:end].strip()
            if chunk:
                chunks.append(chunk)

            # æ¬¡ã®é–‹å§‹ä½ç½®ã‚’è¨­å®šï¼ˆã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—ã‚’è€ƒæ…®ï¼‰
            start = max(start + 1, end - overlap)

            # ç„¡é™ãƒ«ãƒ¼ãƒ—é˜²æ­¢
            if start >= len(text):
                break

        return chunks

    def clean_text(self, text: str) -> str:
        """ãƒ†ã‚­ã‚¹ãƒˆã®ã‚¯ãƒ¬ãƒ³ã‚¸ãƒ³ã‚°å‡¦ç†"""
        if not text:
            return ""

        # æ”¹è¡Œã‚’ç©ºç™½ã«ç½®æ›
        text = text.replace('\n', ' ').replace('\r', ' ')

        # é€£ç¶šã—ãŸç©ºç™½ã‚’1ã¤ã®ç©ºç™½ã«ã¾ã¨ã‚ã‚‹
        text = re.sub(r'\s+', ' ', text)

        # å…ˆé ­ãƒ»æœ«å°¾ã®ç©ºç™½ã‚’é™¤å»
        text = text.strip()

        return text

    def text_to_jsonl_data(self, lines: List[str], dataset_type: str, source_dataset: str = None) -> Tuple[List[Dict[str, Any]], Dict[str, Any]]:
        """ãƒ†ã‚­ã‚¹ãƒˆè¡Œã‚’JSONLç”¨ã®ãƒ‡ãƒ¼ã‚¿æ§‹é€ ã«å¤‰æ›ï¼ˆã‚µã‚¤ã‚ºåˆ¶é™ãƒã‚§ãƒƒã‚¯ä»˜ãï¼‰
        
        Args:
            lines: ãƒ†ã‚­ã‚¹ãƒˆè¡Œã®ãƒªã‚¹ãƒˆ
            dataset_type: ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚¿ã‚¤ãƒ—ï¼ˆè¨­å®šã‚­ãƒ¼ï¼‰
            source_dataset: å…ƒã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆåï¼ˆçµ±åˆæ™‚ã«ä½¿ç”¨ï¼‰
        """
        # çµ±åˆãƒ¢ãƒ¼ãƒ‰ã®å ´åˆã¯çµ±åˆè¨­å®šã‚’ä½¿ç”¨
        if dataset_type == "unified_all":
            config = VectorStoreConfig.get_unified_config()
        else:
            config = self.configs.get(dataset_type)
            if not config:
                raise ValueError(f"æœªçŸ¥ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚¿ã‚¤ãƒ—: {dataset_type}")

        chunk_size = config.chunk_size
        overlap = config.overlap
        max_chunks = config.max_chunks_per_file

        jsonl_data = []
        total_size = 0
        warnings = []

        for idx, line in enumerate(lines):
            # ãƒ†ã‚­ã‚¹ãƒˆã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°
            cleaned_text = self.clean_text(line)

            if not cleaned_text:
                continue

            # é•·ã„ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰²
            chunks = self.chunk_text(cleaned_text, chunk_size, overlap)

            for chunk_idx, chunk in enumerate(chunks):
                # ãƒãƒ£ãƒ³ã‚¯æ•°åˆ¶é™ãƒã‚§ãƒƒã‚¯
                if len(jsonl_data) >= max_chunks:
                    warnings.append(
                        f"âš ï¸ ãƒãƒ£ãƒ³ã‚¯æ•°ãŒä¸Šé™({max_chunks:,})ã«é”ã—ã¾ã—ãŸã€‚æ®‹ã‚Š{len(lines) - idx:,}è¡Œã¯ã‚¹ã‚­ãƒƒãƒ—ã•ã‚Œã¾ã™ã€‚")
                    break

                # çµ±åˆãƒ¢ãƒ¼ãƒ‰ç”¨ã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿è¨­å®š
                metadata = {
                    "dataset"      : source_dataset if source_dataset else dataset_type,
                    "original_line": idx,
                    "chunk_index"  : chunk_idx,
                    "total_chunks" : len(chunks)
                }
                
                # ãƒ‰ãƒ¡ã‚¤ãƒ³æƒ…å ±è¿½åŠ ï¼ˆçµ±åˆãƒ¢ãƒ¼ãƒ‰æ™‚ï¼‰
                if source_dataset:
                    if "medical" in source_dataset:
                        metadata["domain"] = "medical"
                    elif "legal" in source_dataset:
                        metadata["domain"] = "legal"
                    elif "sciq" in source_dataset or "science" in source_dataset:
                        metadata["domain"] = "science"
                    elif "customer" in source_dataset or "faq" in source_dataset:
                        metadata["domain"] = "customer_support"
                    elif "trivia" in source_dataset:
                        metadata["domain"] = "trivia"
                    else:
                        metadata["domain"] = "general"

                jsonl_entry = {
                    "id"      : f"{source_dataset if source_dataset else dataset_type}_{idx}_{chunk_idx}",
                    "text"    : chunk,
                    "metadata": metadata
                }

                jsonl_data.append(jsonl_entry)
                total_size += len(json.dumps(jsonl_entry, ensure_ascii=False))

                # ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºåˆ¶é™ãƒã‚§ãƒƒã‚¯ï¼ˆæ¦‚ç®—ï¼‰
                estimated_size_mb = total_size / (1024 * 1024)
                if estimated_size_mb > config.max_file_size_mb:
                    warnings.append(
                        f"âš ï¸ ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºãŒä¸Šé™({config.max_file_size_mb}MB)ã«é”ã—ã¾ã—ãŸã€‚æ®‹ã‚Šã¯ã‚¹ã‚­ãƒƒãƒ—ã•ã‚Œã¾ã™ã€‚")
                    break

            # åˆ¶é™ã«é”ã—ãŸå ´åˆã¯ãƒ«ãƒ¼ãƒ—ã‚’æŠœã‘ã‚‹
            if len(jsonl_data) >= max_chunks or total_size / (1024 * 1024) > config.max_file_size_mb:
                break

        # çµ±è¨ˆæƒ…å ±
        stats = {
            "original_lines"   : len(lines),
            "processed_lines"  : idx + 1 if jsonl_data else 0,
            "total_chunks"     : len(jsonl_data),
            "estimated_size_mb": total_size / (1024 * 1024),
            "warnings"         : warnings,
            "chunk_size_used"  : chunk_size,
            "overlap_used"     : overlap,
            "source_dataset"   : source_dataset if source_dataset else dataset_type
        }

        dataset_label = source_dataset if source_dataset else dataset_type
        logger.info(
            f"{dataset_label}: {len(lines)}è¡Œ -> {len(jsonl_data)}ãƒãƒ£ãƒ³ã‚¯ (æ¨å®š{stats['estimated_size_mb']:.1f}MB)")

        if warnings:
            for warning in warnings:
                logger.warning(warning)

        return jsonl_data, stats


# ===================================================================
# Vector Storeç®¡ç†ã‚¯ãƒ©ã‚¹
# ===================================================================
class VectorStoreManager:
    """Vector Storeç®¡ç†ã‚¯ãƒ©ã‚¹"""

    def __init__(self, api_key: str = None):
        if api_key is None:
            api_key = os.getenv("OPENAI_API_KEY")

        if not api_key:
            raise ValueError("OpenAI APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚ç’°å¢ƒå¤‰æ•° OPENAI_API_KEY ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")

        self.client = OpenAI(api_key=api_key)
        self.processor = VectorStoreProcessor()
        self.configs = VectorStoreConfig.get_all_configs()
        self.created_stores = {}

    def create_vector_store_from_jsonl_data(self, jsonl_data: List[Dict], store_name: str) -> Optional[str]:
        """JSONLå½¢å¼ã®ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰Vector Storeã‚’ä½œæˆ"""
        temp_file_path = None
        uploaded_file_id = None

        try:
            # å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã®æ¤œè¨¼
            if not isinstance(jsonl_data, list):
                logger.error(f"âŒ jsonl_dataãŒãƒªã‚¹ãƒˆã§ã¯ã‚ã‚Šã¾ã›ã‚“: {type(jsonl_data)}")
                return None

            if not jsonl_data:
                logger.error("âŒ jsonl_dataãŒç©ºã§ã™")
                return None

            logger.info(f"Vector Storeä½œæˆé–‹å§‹: {len(jsonl_data)}ã‚¨ãƒ³ãƒˆãƒª")

            # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆã—ã¦ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ç”¨ã®JSONLãƒ•ã‚¡ã‚¤ãƒ«ã‚’æº–å‚™
            with tempfile.NamedTemporaryFile(mode='w', suffix='.txt', delete=False, encoding='utf-8') as temp_file:
                for i, entry in enumerate(jsonl_data):
                    # ã‚¨ãƒ³ãƒˆãƒªãŒè¾æ›¸å‹ã§ã‚ã‚‹ã“ã¨ã‚’ç¢ºèª
                    if not isinstance(entry, dict):
                        logger.error(f"ã‚¨ãƒ³ãƒˆãƒª {i} ãŒè¾æ›¸å‹ã§ã¯ã‚ã‚Šã¾ã›ã‚“: {type(entry)}")
                        return None

                    # å¿…è¦ãªã‚­ãƒ¼ãŒå­˜åœ¨ã™ã‚‹ã“ã¨ã‚’ç¢ºèª
                    if "id" not in entry or "text" not in entry:
                        logger.error(f"ã‚¨ãƒ³ãƒˆãƒª {i} ã«å¿…è¦ãªã‚­ãƒ¼ãŒã‚ã‚Šã¾ã›ã‚“: {list(entry.keys())}")
                        return None

                    # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã¯æ–‡å­—åˆ—ã¨ã—ã¦ä¿å­˜ï¼ˆOpenAIå´ã®åˆ¶é™å¯¾å¿œï¼‰
                    jsonl_entry = {
                        "id"  : entry["id"],
                        "text": entry["text"]
                    }
                    temp_file.write(json.dumps(jsonl_entry, ensure_ascii=False) + '\n')

                temp_file_path = temp_file.name

            logger.info(f"JSONLãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆå®Œäº†: {len(jsonl_data)}ã‚¨ãƒ³ãƒˆãƒª")

            # Step 1: ãƒ•ã‚¡ã‚¤ãƒ«ã‚’OpenAIã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
            with open(temp_file_path, 'rb') as file:
                uploaded_file = self.client.files.create(
                    file=file,
                    purpose="assistants"
                )
                uploaded_file_id = uploaded_file.id

            logger.info(f"ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å®Œäº†: File ID={uploaded_file_id}")

            # Step 2: Vector Storeã‚’ä½œæˆ
            vector_store = self.client.vector_stores.create(
                name=store_name,
                metadata={
                    "created_by" : "vector_store_streamlit_app",
                    "version"    : "2025.1",
                    "data_format": "jsonl_as_txt",
                    "entry_count": str(len(jsonl_data))
                }
            )

            logger.info(f"Vector Storeä½œæˆå®Œäº†: ID={vector_store.id}")

            # Step 3: ãƒ•ã‚¡ã‚¤ãƒ«ã‚’Vector Storeã«è¿½åŠ 
            vector_store_file = self.client.vector_stores.files.create(
                vector_store_id=vector_store.id,
                file_id=uploaded_file_id
            )

            logger.info(f"Vector StoreFileãƒªãƒ³ã‚¯ä½œæˆå®Œäº†")

            # Step 4: ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†å®Œäº†ã‚’å¾…æ©Ÿ
            max_wait_time = 600  # æœ€å¤§10åˆ†å¾…æ©Ÿ
            wait_interval = 5  # 5ç§’é–“éš”ã§ãƒã‚§ãƒƒã‚¯
            waited_time = 0
            initial_wait = 3  # æœ€åˆã®å¾…æ©Ÿæ™‚é–“ï¼ˆç§’ï¼‰

            # ãƒ•ã‚¡ã‚¤ãƒ«ç™»éŒ²ç›´å¾Œã¯å°‘ã—å¾…æ©Ÿ
            time.sleep(initial_wait)
            waited_time += initial_wait

            while waited_time < max_wait_time:
                try:
                    file_status = self.client.vector_stores.files.retrieve(
                        vector_store_id=vector_store.id,
                        file_id=uploaded_file_id
                    )

                    if file_status.status == "completed":
                        updated_vector_store = self.client.vector_stores.retrieve(vector_store.id)

                        logger.info(f"âœ… Vector Storeä½œæˆå®Œäº†:")
                        logger.info(f"  - ID: {vector_store.id}")
                        logger.info(f"  - Name: {vector_store.name}")
                        logger.info(f"  - ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {updated_vector_store.file_counts.total}")
                        logger.info(f"  - ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ä½¿ç”¨é‡: {updated_vector_store.usage_bytes} bytes")

                        return vector_store.id

                    elif file_status.status == "failed":
                        error_msg = getattr(file_status, 'last_error', 'Unknown error')
                        logger.error(f"âŒ ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†å¤±æ•—: {error_msg}")

                        # è©³ç´°ã‚¨ãƒ©ãƒ¼æƒ…å ±ã‚’ãƒ­ã‚°å‡ºåŠ›
                        if hasattr(file_status, 'last_error') and file_status.last_error:
                            logger.error(f"ã‚¨ãƒ©ãƒ¼ã‚³ãƒ¼ãƒ‰: {getattr(file_status.last_error, 'code', 'N/A')}")
                            logger.error(f"ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸: {getattr(file_status.last_error, 'message', 'N/A')}")

                        return None

                    elif file_status.status in ["in_progress", "cancelling"]:
                        time.sleep(wait_interval)
                        waited_time += wait_interval
                    else:
                        logger.warning(f"âš ï¸ äºˆæœŸã—ãªã„ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹: {file_status.status}")
                        time.sleep(wait_interval)
                        waited_time += wait_interval

                except Exception as e:
                    # 404ã‚¨ãƒ©ãƒ¼ã¯ãƒ•ã‚¡ã‚¤ãƒ«ãŒã¾ã ç™»éŒ²ä¸­ã®å¯èƒ½æ€§ãŒã‚ã‚‹ãŸã‚ãƒªãƒˆãƒ©ã‚¤
                    if "404" in str(e) or "not found" in str(e).lower():
                        logger.debug(f"ãƒ•ã‚¡ã‚¤ãƒ«ç™»éŒ²å¾…æ©Ÿä¸­... (å¾…æ©Ÿæ™‚é–“: {waited_time}ç§’)")
                        time.sleep(wait_interval)
                        waited_time += wait_interval
                    else:
                        # ãã®ä»–ã®ã‚¨ãƒ©ãƒ¼ã¯å†ã‚¹ãƒ­ãƒ¼
                        raise

            logger.error(f"âŒ Vector Storeä½œæˆã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ (åˆ¶é™æ™‚é–“: {max_wait_time}ç§’)")
            return None

        except Exception as e:
            logger.error(f"Vector Storeä½œæˆã‚¨ãƒ©ãƒ¼: {e}")
            return None

        finally:
            # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤
            if temp_file_path and os.path.exists(temp_file_path):
                os.unlink(temp_file_path)
                logger.info("ğŸ—‘ï¸ ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤ã—ã¾ã—ãŸ")

    def process_unified_datasets(self, selected_datasets: List[str], output_dir: Path = None) -> Dict[str, Any]:
        """è¤‡æ•°ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’çµ±åˆã—ã¦Vector Storeã‚’ä½œæˆ"""
        if output_dir is None:
            output_dir = Path("OUTPUT")
        
        # çµ±åˆè¨­å®šã‚’å–å¾—
        unified_config = VectorStoreConfig.get_unified_config()
        
        # å…¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆåˆ†ã®JSONLãƒ‡ãƒ¼ã‚¿ã‚’é›†ç©
        all_jsonl_data = []
        total_lines = 0
        processed_lines = 0
        dataset_stats = {}
        all_warnings = []
        
        logger.info(f"çµ±åˆVector Storeä½œæˆé–‹å§‹: {len(selected_datasets)}ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ")
        
        # å„ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’å‡¦ç†
        for dataset_type in selected_datasets:
            config = self.configs.get(dataset_type)
            if not config:
                logger.warning(f"ä¸æ˜ãªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ: {dataset_type}")
                continue
            
            filepath = output_dir / config.filename
            if not filepath.exists():
                logger.warning(f"ãƒ•ã‚¡ã‚¤ãƒ«ä¸åœ¨: {filepath}")
                all_warnings.append(f"âš ï¸ {config.description}ã®ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                continue
            
            try:
                # CSVãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿
                text_lines = self.processor.load_csv_file(filepath, config.csv_text_column)
                
                if not text_lines:
                    logger.warning(f"æœ‰åŠ¹ãªãƒ†ã‚­ã‚¹ãƒˆãªã—: {filepath}")
                    continue
                
                # çµ±åˆãƒ¢ãƒ¼ãƒ‰ç”¨ã«JSONLå¤‰æ›ï¼ˆsource_datasetãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æ¸¡ã™ï¼‰
                jsonl_data, stats = self.processor.text_to_jsonl_data(
                    text_lines, 
                    "unified_all",  # çµ±åˆè¨­å®šã‚’ä½¿ç”¨
                    source_dataset=dataset_type  # å…ƒã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆåã‚’ä¿æŒ
                )
                
                # çµ±è¨ˆæƒ…å ±åé›†
                dataset_stats[dataset_type] = {
                    "original_lines": len(text_lines),
                    "chunks": len(jsonl_data),
                    "size_mb": stats.get("estimated_size_mb", 0)
                }
                
                total_lines += len(text_lines)
                processed_lines += stats.get("processed_lines", 0)
                
                # è­¦å‘Šåé›†
                if stats.get("warnings"):
                    all_warnings.extend([f"[{config.description}] {w}" for w in stats["warnings"]])
                
                # ãƒ‡ãƒ¼ã‚¿ã‚’çµ±åˆãƒªã‚¹ãƒˆã«è¿½åŠ 
                all_jsonl_data.extend(jsonl_data)
                
                logger.info(f"  {config.description}: {len(jsonl_data)}ãƒãƒ£ãƒ³ã‚¯è¿½åŠ ")
                
            except Exception as e:
                logger.error(f"{dataset_type}å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
                all_warnings.append(f"âŒ {config.description}ã®å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼: {str(e)}")
        
        # çµ±åˆãƒ‡ãƒ¼ã‚¿ãŒç©ºã®å ´åˆ
        if not all_jsonl_data:
            return {
                "success": False,
                "error": "çµ±åˆå¯èƒ½ãªãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ",
                "warnings": all_warnings
            }
        
        # çµ±åˆãƒ‡ãƒ¼ã‚¿ã®ã‚µã‚¤ã‚ºãƒã‚§ãƒƒã‚¯
        total_size = sum(len(json.dumps(entry, ensure_ascii=False)) for entry in all_jsonl_data)
        total_size_mb = total_size / (1024 * 1024)
        
        logger.info(f"çµ±åˆãƒ‡ãƒ¼ã‚¿: åˆè¨ˆ{len(all_jsonl_data)}ãƒãƒ£ãƒ³ã‚¯, {total_size_mb:.1f}MB")
        
        # ã‚µã‚¤ã‚ºåˆ¶é™ãƒã‚§ãƒƒã‚¯ï¼ˆçµ±åˆæ™‚ã¯100MBã¾ã§è¨±å¯ï¼‰
        if total_size_mb > unified_config.max_file_size_mb:
            # ãƒ‡ãƒ¼ã‚¿ã‚’ãƒˆãƒªãƒŸãƒ³ã‚°
            target_size_mb = unified_config.max_file_size_mb * 0.9  # 90%ã‚’ç›®æ¨™
            reduction_ratio = target_size_mb / total_size_mb
            target_chunks = int(len(all_jsonl_data) * reduction_ratio)
            
            logger.warning(f"çµ±åˆãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚ºè¶…é: {total_size_mb:.1f}MB -> {target_size_mb:.1f}MB")
            all_warnings.append(f"âš ï¸ ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚ºåˆ¶é™ã«ã‚ˆã‚Š{len(all_jsonl_data)}ãƒãƒ£ãƒ³ã‚¯ã‹ã‚‰{target_chunks}ãƒãƒ£ãƒ³ã‚¯ã«å‰Šæ¸›")
            
            # å„ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‹ã‚‰å‡ç­‰ã«å‰Šæ¸›
            all_jsonl_data = all_jsonl_data[:target_chunks]
            
            # ã‚µã‚¤ã‚ºå†è¨ˆç®—
            total_size = sum(len(json.dumps(entry, ensure_ascii=False)) for entry in all_jsonl_data)
            total_size_mb = total_size / (1024 * 1024)
        
        # Vector Storeä½œæˆ
        try:
            store_name = unified_config.store_name
            logger.info(f"çµ±åˆVector Storeä½œæˆé–‹å§‹: {store_name}")
            
            vector_store_id = self.create_vector_store_from_jsonl_data(all_jsonl_data, store_name)
            
            if vector_store_id:
                self.created_stores["unified_all"] = vector_store_id
                
                return {
                    "success": True,
                    "vector_store_id": vector_store_id,
                    "store_name": store_name,
                    "processed_lines": processed_lines,
                    "total_lines": total_lines,
                    "created_chunks": len(all_jsonl_data),
                    "estimated_size_mb": total_size_mb,
                    "warnings": all_warnings,
                    "dataset_stats": dataset_stats,
                    "config_used": {
                        "chunk_size": unified_config.chunk_size,
                        "overlap": unified_config.overlap,
                        "datasets_included": selected_datasets
                    }
                }
            else:
                return {
                    "success": False,
                    "error": "çµ±åˆVector Storeä½œæˆã«å¤±æ•—ã—ã¾ã—ãŸ",
                    "warnings": all_warnings
                }
                
        except Exception as e:
            logger.error(f"çµ±åˆVector Storeä½œæˆã‚¨ãƒ©ãƒ¼: {e}")
            return {
                "success": False,
                "error": f"çµ±åˆå‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ: {str(e)}",
                "warnings": all_warnings
            }
    
    def process_single_dataset(self, dataset_type: str, output_dir: Path = None) -> Dict[str, Any]:
        """å˜ä¸€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®å‡¦ç†ï¼ˆå®Œå…¨ä¿®æ­£ç‰ˆï¼‰"""
        if output_dir is None:
            output_dir = Path("qa_output")

        config = self.configs.get(dataset_type)
        if not config:
            return {"success": False, "error": f"æœªçŸ¥ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚¿ã‚¤ãƒ—: {dataset_type}"}

        # ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã®æ§‹ç¯‰
        filepath = output_dir / config.filename

        if not filepath.exists():
            return {"success": False, "error": f"ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {filepath}"}

        try:
            # Step 1: CSVãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿
            text_lines = self.processor.load_csv_file(filepath, config.csv_text_column)

            if not text_lines:
                return {"success": False, "error": f"æœ‰åŠ¹ãªãƒ†ã‚­ã‚¹ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {filepath}"}

            # Step 2: JSONLå½¢å¼ã«å¤‰æ›ï¼ˆä¿®æ­£ç‰ˆï¼‰
            try:
                result = self.processor.text_to_jsonl_data(text_lines, dataset_type)

                # ã‚¿ãƒ—ãƒ«ã§ã‚ã‚‹ã“ã¨ã‚’ç¢ºèª
                if not isinstance(result, tuple) or len(result) != 2:
                    logger.error(f"text_to_jsonl_dataã®æˆ»ã‚Šå€¤ãŒæœŸå¾…ã™ã‚‹å½¢å¼ã§ã¯ã‚ã‚Šã¾ã›ã‚“: {type(result)}")
                    return {"success": False, "error": "å†…éƒ¨ã‚¨ãƒ©ãƒ¼: å¤‰æ›å‡¦ç†ã®æˆ»ã‚Šå€¤ãŒä¸æ­£"}

                # ã‚¿ãƒ—ãƒ«ã‚’åˆ†é›¢
                jsonl_data_list, stats_dict = result

                # å‹ãƒã‚§ãƒƒã‚¯
                if not isinstance(jsonl_data_list, list):
                    logger.error(f"jsonl_dataãŒãƒªã‚¹ãƒˆã§ã¯ã‚ã‚Šã¾ã›ã‚“: {type(jsonl_data_list)}")
                    return {"success": False, "error": "å†…éƒ¨ã‚¨ãƒ©ãƒ¼: JSONLãƒ‡ãƒ¼ã‚¿ãŒãƒªã‚¹ãƒˆå‹ã§ã¯ãªã„"}

                if not isinstance(stats_dict, dict):
                    logger.error(f"statsãŒè¾æ›¸ã§ã¯ã‚ã‚Šã¾ã›ã‚“: {type(stats_dict)}")
                    return {"success": False, "error": "å†…éƒ¨ã‚¨ãƒ©ãƒ¼: çµ±è¨ˆãƒ‡ãƒ¼ã‚¿ãŒè¾æ›¸å‹ã§ã¯ãªã„"}

                logger.info(f"âœ… JSONLå¤‰æ›æˆåŠŸ: {len(jsonl_data_list)}ãƒãƒ£ãƒ³ã‚¯ä½œæˆ")

            except Exception as convert_error:
                logger.error(f"JSONLå¤‰æ›ã§ã‚¨ãƒ©ãƒ¼: {convert_error}")
                return {"success": False, "error": f"JSONLå¤‰æ›ã‚¨ãƒ©ãƒ¼: {str(convert_error)}"}

            if not jsonl_data_list:
                return {"success": False, "error": f"JSONLå¤‰æ›ã«å¤±æ•—: {dataset_type}"}

            # ã‚µã‚¤ã‚ºåˆ¶é™è­¦å‘Šã®ãƒã‚§ãƒƒã‚¯
            if stats_dict.get("warnings"):
                warning_msg = "; ".join(stats_dict["warnings"])
                logger.warning(f"{dataset_type}: {warning_msg}")

            # äº‹å‰ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºãƒã‚§ãƒƒã‚¯
            estimated_size_mb = stats_dict.get("estimated_size_mb", 0)
            if estimated_size_mb > 25:  # 25MBåˆ¶é™ã«å³æ ¼åŒ–
                logger.error(f"âŒ ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºãŒå¤§ãã™ãã¾ã™: {estimated_size_mb:.1f}MB > 25MB")
                return {
                    "success": False,
                    "error"  : f"ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºãŒåˆ¶é™ã‚’è¶…ãˆã¦ã„ã¾ã™: {estimated_size_mb:.1f}MB (åˆ¶é™: 25MB). ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚ºã‚’å¤§ããã™ã‚‹ã‹ã€ãƒ‡ãƒ¼ã‚¿ã‚’åˆ†å‰²ã—ã¦ãã ã•ã„ã€‚"
                }

            # åŒ»ç™‚QAç‰¹æœ‰ã®è¿½åŠ åˆ¶é™
            if dataset_type == "medical_qa":
                if len(jsonl_data_list) > 5000:  # ã•ã‚‰ã«å³æ ¼ãªåˆ¶é™
                    logger.warning(f"åŒ»ç™‚QAãƒ‡ãƒ¼ã‚¿ã®ãƒãƒ£ãƒ³ã‚¯æ•°åˆ¶é™é©ç”¨: {len(jsonl_data_list):,} -> 5,000")
                    jsonl_data_list = jsonl_data_list[:5000]
                    stats_dict["warnings"].append("åŒ»ç™‚QAãƒ‡ãƒ¼ã‚¿ã®ãƒãƒ£ãƒ³ã‚¯æ•°ã‚’5,000ã«åˆ¶é™ã—ã¾ã—ãŸ")

                # ã‚µã‚¤ã‚ºå†è¨ˆç®—
                total_size_recalc = sum(len(json.dumps(entry, ensure_ascii=False)) for entry in jsonl_data_list)
                estimated_size_mb_recalc = total_size_recalc / (1024 * 1024)

                # 15MBåˆ¶é™ã§ã•ã‚‰ã«ãƒã‚§ãƒƒã‚¯
                if estimated_size_mb_recalc > 15:
                    # ã•ã‚‰ã«å‰Šæ¸›ãŒå¿…è¦
                    target_chunks = int(len(jsonl_data_list) * 15 / estimated_size_mb_recalc)
                    target_chunks = max(1000, target_chunks)  # æœ€ä½1000ãƒãƒ£ãƒ³ã‚¯ã¯ä¿è¨¼
                    logger.warning(
                        f"åŒ»ç™‚QAãƒ‡ãƒ¼ã‚¿ã®ã‚µã‚¤ã‚ºåˆ¶é™é©ç”¨: {len(jsonl_data_list):,} -> {target_chunks:,}ãƒãƒ£ãƒ³ã‚¯")
                    jsonl_data_list = jsonl_data_list[:target_chunks]
                    stats_dict["warnings"].append(
                        f"åŒ»ç™‚QAãƒ‡ãƒ¼ã‚¿ã‚’15MBä»¥ä¸‹ã«ã™ã‚‹ãŸã‚{target_chunks:,}ãƒãƒ£ãƒ³ã‚¯ã«åˆ¶é™ã—ã¾ã—ãŸ")

                    # æœ€çµ‚ã‚µã‚¤ã‚ºå†è¨ˆç®—
                    total_size_recalc = sum(len(json.dumps(entry, ensure_ascii=False)) for entry in jsonl_data_list)
                    estimated_size_mb_recalc = total_size_recalc / (1024 * 1024)

                stats_dict["estimated_size_mb"] = estimated_size_mb_recalc
                stats_dict["total_chunks"] = len(jsonl_data_list)

                logger.info(f"åŒ»ç™‚QAæœ€é©åŒ–å¾Œ: {len(jsonl_data_list):,}ãƒãƒ£ãƒ³ã‚¯, {estimated_size_mb_recalc:.1f}MB")

            # Step 3: Vector Storeä½œæˆï¼ˆä¿®æ­£ç‰ˆ - jsonl_data_listã‚’æ¸¡ã™ï¼‰
            store_name = config.store_name
            logger.info(f"Vector Storeä½œæˆé–‹å§‹: {store_name}")

            # ã“ã“ãŒé‡è¦ï¼šjsonl_data_listã‚’æ¸¡ã™ï¼ˆã‚¿ãƒ—ãƒ«ã§ã¯ãªãï¼‰
            vector_store_id = self.create_vector_store_from_jsonl_data(jsonl_data_list, store_name)

            if vector_store_id:
                self.created_stores[dataset_type] = vector_store_id

                return {
                    "success"          : True,
                    "vector_store_id"  : vector_store_id,
                    "store_name"       : store_name,
                    "processed_lines"  : stats_dict.get("processed_lines", 0),
                    "total_lines"      : stats_dict.get("original_lines", 0),
                    "created_chunks"   : len(jsonl_data_list),
                    "estimated_size_mb": stats_dict.get("estimated_size_mb", 0),
                    "warnings"         : stats_dict.get("warnings", []),
                    "config_used"      : {
                        "chunk_size": stats_dict.get("chunk_size_used", 0),
                        "overlap"   : stats_dict.get("overlap_used", 0)
                    }
                }
            else:
                return {"success": False, "error": f"Vector Storeä½œæˆã«å¤±æ•—: {dataset_type}"}

        except Exception as e:
            logger.error(f"{dataset_type} å‡¦ç†ã‚¨ãƒ©ãƒ¼: {str(e)}")
            return {"success": False, "error": f"å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ: {str(e)}"}

    def list_vector_stores(self) -> List[Dict]:
        """æ—¢å­˜ã®Vector Storeã‚’ä¸€è¦§è¡¨ç¤º"""
        try:
            stores = self.client.vector_stores.list()
            store_list = []

            for store in stores.data:
                store_info = {
                    "id"         : store.id,
                    "name"       : store.name,
                    "file_counts": store.file_counts.total if store.file_counts else 0,
                    "created_at" : store.created_at,
                    "usage_bytes": store.usage_bytes
                }
                store_list.append(store_info)

            return store_list
        except Exception as e:
            logger.error(f"Vector Storeä¸€è¦§å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
            return []


# ===================================================================
# Streamlit UIç®¡ç†ã‚¯ãƒ©ã‚¹
# ===================================================================
class VectorStoreUI:
    """Vector Storeç”¨Streamlit UIç®¡ç†ã‚¯ãƒ©ã‚¹"""

    def __init__(self):
        self.configs = VectorStoreConfig.get_all_configs()
        self.manager = None

    def setup_page(self):
        """ãƒšãƒ¼ã‚¸è¨­å®š"""
        try:
            st.set_page_config(
                page_title="Vector Storeä½œæˆã‚¢ãƒ—ãƒªï¼ˆå®Œå…¨ç‰ˆï¼‰",
                page_icon="ğŸ”—",
                layout="wide",
                initial_sidebar_state="expanded"
            )
        except st.errors.StreamlitAPIException:
            pass

    def setup_header(self):
        """ãƒ˜ãƒƒãƒ€ãƒ¼è¨­å®š"""
        st.title("ğŸ”— Vector Storeä½œæˆã‚¢ãƒ—ãƒªï¼ˆå®Œå…¨ç‰ˆï¼‰")
        st.caption("OpenAI Vector Storeã®è‡ªå‹•ä½œæˆãƒ»ç®¡ç†ã‚·ã‚¹ãƒ†ãƒ ")
        st.markdown("---")

    def setup_sidebar(self) -> Tuple[str, bool]:
        """ã‚µã‚¤ãƒ‰ãƒãƒ¼è¨­å®š"""
        st.sidebar.title("ğŸ”— Vector Storeä½œæˆ")
        st.sidebar.markdown("---")

        # ãƒ¢ãƒ‡ãƒ«é¸æŠï¼ˆå‚è€ƒè¡¨ç¤ºç”¨ï¼‰
        if HELPER_AVAILABLE:
            selected_model = select_model(key="vector_store_model")
            show_model_info(selected_model)
        else:
            selected_model = st.sidebar.selectbox(
                "ğŸ¤– å‚è€ƒãƒ¢ãƒ‡ãƒ«",
                ["gpt-4o-mini", "gpt-4o", "gpt-4.1-mini", "gpt-4.1"],
                help="å‚è€ƒè¡¨ç¤ºç”¨ï¼ˆVector Storeä½œæˆã«ã¯ç›´æ¥å½±éŸ¿ã—ã¾ã›ã‚“ï¼‰"
            )

        st.sidebar.markdown("---")

        # å…¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå‡¦ç†ã‚ªãƒ—ã‚·ãƒ§ãƒ³
        process_all = st.sidebar.checkbox(
            "ğŸš€ å…¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä¸€æ‹¬å‡¦ç†",
            value=False,
            help="5ã¤ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä¸€æ‹¬ã§Vector StoreåŒ–"
        )

        # APIã‚­ãƒ¼ç¢ºèª
        with st.sidebar.expander("ğŸ”‘ APIè¨­å®šç¢ºèª", expanded=False):
            api_key_status = "âœ… è¨­å®šæ¸ˆã¿" if os.getenv("OPENAI_API_KEY") else "âŒ æœªè¨­å®š"
            st.write(f"**OpenAI APIã‚­ãƒ¼**: {api_key_status}")

            if not os.getenv("OPENAI_API_KEY"):
                st.error("ç’°å¢ƒå¤‰æ•° OPENAI_API_KEY ã‚’è¨­å®šã—ã¦ãã ã•ã„")
                st.code("export OPENAI_API_KEY='your-api-key-here'")

        return selected_model, process_all

    def display_dataset_selection(self) -> List[str]:
        """ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆé¸æŠUI"""
        st.subheader("ğŸ“‹ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆé¸æŠ")

        # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä¸€è¦§è¡¨ç¤º
        col1, col2 = st.columns(2)
        selected_datasets = []

        for idx, (dataset_type, config) in enumerate(self.configs.items()):
            col = col1 if idx % 2 == 0 else col2

            with col:
                # ãƒã‚§ãƒƒã‚¯ãƒœãƒƒã‚¯ã‚¹ã§ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆé¸æŠ
                selected = st.checkbox(
                    f"{config.description}",
                    key=f"dataset_{dataset_type}",
                    help=f"ãƒ•ã‚¡ã‚¤ãƒ«: {config.filename}"
                )

                if selected:
                    selected_datasets.append(dataset_type)

                # ãƒ•ã‚¡ã‚¤ãƒ«å­˜åœ¨ç¢ºèª
                output_dir = Path("qa_output")
                filepath = output_dir / config.filename

                if filepath.exists():
                    file_size = filepath.stat().st_size
                    st.success(f"âœ… ãƒ•ã‚¡ã‚¤ãƒ«å­˜åœ¨ ({file_size:,} bytes)")
                else:
                    st.error(f"âŒ ãƒ•ã‚¡ã‚¤ãƒ«ä¸åœ¨: {config.filename}")

        return selected_datasets

    def display_file_status(self):
        """ãƒ•ã‚¡ã‚¤ãƒ«çŠ¶æ³è¡¨ç¤º"""
        st.subheader("ğŸ“ ãƒ•ã‚¡ã‚¤ãƒ«çŠ¶æ³")

        output_dir = Path("OUTPUT")

        if not output_dir.exists():
            st.error(f"âŒ OUTPUTãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {output_dir}")
            return

        # ãƒ•ã‚¡ã‚¤ãƒ«çŠ¶æ³ã‚’ãƒ†ãƒ¼ãƒ–ãƒ«è¡¨ç¤º
        file_data = []
        for dataset_type, config in self.configs.items():
            filepath = output_dir / config.filename

            if filepath.exists():
                file_size = filepath.stat().st_size
                modified_time = datetime.fromtimestamp(filepath.stat().st_mtime)
                status = "âœ… åˆ©ç”¨å¯èƒ½"
            else:
                file_size = 0
                modified_time = None
                status = "âŒ ãƒ•ã‚¡ã‚¤ãƒ«ä¸åœ¨"

            file_data.append({
                "ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ": config.description,
                "ãƒ•ã‚¡ã‚¤ãƒ«å"  : config.filename,
                "ã‚µã‚¤ã‚º"      : f"{file_size:,} bytes" if file_size > 0 else "-",
                "æ›´æ–°æ—¥æ™‚"    : modified_time.strftime("%Y-%m-%d %H:%M:%S") if modified_time else "-",
                "çŠ¶æ…‹"        : status
            })

        df_files = pd.DataFrame(file_data)
        st.dataframe(df_files, use_container_width=True)

    def display_results(self, results: Dict[str, Dict]):
        """å‡¦ç†çµæœè¡¨ç¤ºï¼ˆæ”¹å–„ç‰ˆï¼‰"""
        st.subheader("ğŸ“Š å‡¦ç†çµæœ")

        successful = {k: v for k, v in results.items() if v.get("success", False)}
        failed = {k: v for k, v in results.items() if not v.get("success", False)}

        # ã‚µãƒãƒªãƒ¼è¡¨ç¤º
        col1, col2, col3 = st.columns(3)
        with col1:
            st.metric("å‡¦ç†å¯¾è±¡", len(results))
        with col2:
            st.metric("æˆåŠŸ", len(successful))
        with col3:
            st.metric("å¤±æ•—", len(failed))

        # æˆåŠŸçµæœã®è©³ç´°
        if successful:
            st.write("### âœ… ä½œæˆæˆåŠŸ")
            success_data = []
            for dataset_type, result in successful.items():
                config = self.configs[dataset_type]

                # è­¦å‘ŠãŒã‚ã‚‹å ´åˆã®è¡¨ç¤ºæº–å‚™
                warning_text = ""
                if result.get("warnings"):
                    warning_text = f" âš ï¸ {len(result['warnings'])}ä»¶ã®è­¦å‘Š"

                success_data.append({
                    "ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ"   : config.description,
                    "Vector Store ID": result["vector_store_id"],
                    "Storeå"        : result["store_name"],
                    "å‡¦ç†è¡Œæ•°"       : f"{result.get('processed_lines', 0):,} / {result.get('total_lines', 0):,}",
                    "ãƒãƒ£ãƒ³ã‚¯æ•°"     : f"{result['created_chunks']:,}",
                    "æ¨å®šã‚µã‚¤ã‚º"     : f"{result.get('estimated_size_mb', 0):.1f} MB",
                    "çŠ¶æ…‹"           : f"å®Œäº†{warning_text}"
                })

            df_success = pd.DataFrame(success_data)
            st.dataframe(df_success, use_container_width=True)

            # è­¦å‘Šè©³ç´°è¡¨ç¤º
            for dataset_type, result in successful.items():
                if result.get("warnings"):
                    config = self.configs[dataset_type]
                    with st.expander(f"âš ï¸ {config.description} ã®è­¦å‘Šè©³ç´°", expanded=False):
                        for warning in result["warnings"]:
                            st.warning(warning)

                        # è¨­å®šæƒ…å ±ã‚‚è¡¨ç¤º
                        config_used = result.get("config_used", {})
                        st.info(
                            f"ä½¿ç”¨è¨­å®š: ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚º={config_used.get('chunk_size', 'N/A')}, ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—={config_used.get('overlap', 'N/A')}")

        # å¤±æ•—çµæœã®è©³ç´°
        if failed:
            st.write("### âŒ ä½œæˆå¤±æ•—")
            for dataset_type, result in failed.items():
                config = self.configs[dataset_type]
                st.error(f"**{config.description}**: {result['error']}")

                # å¯¾å‡¦æ³•ã®ææ¡ˆ
                if "too large" in result['error'].lower():
                    st.info(
                        "ğŸ’¡ **å¯¾å‡¦æ³•**: ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºãŒå¤§ãã™ãã¾ã™ã€‚ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚ºã‚’å¤§ããã™ã‚‹ã‹ã€ãƒ‡ãƒ¼ã‚¿ã‚’åˆ†å‰²ã—ã¦ãã ã•ã„ã€‚")
                elif "not found" in result['error'].lower():
                    st.info("ğŸ’¡ **å¯¾å‡¦æ³•**: å¿…è¦ãªãƒ•ã‚¡ã‚¤ãƒ«ãŒOUTPUTãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«å­˜åœ¨ã™ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")

        return successful, failed

    def display_existing_stores(self, manager: VectorStoreManager):
        """æ—¢å­˜Vector Storeè¡¨ç¤ºï¼ˆä¿®æ­£ç‰ˆï¼‰"""
        st.subheader("ğŸ“š æ—¢å­˜Vector Storeä¸€è¦§")

        try:
            stores = manager.list_vector_stores()

            if not stores:
                st.info("Vector StoreãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                return

            # Vector Storeä¸€è¦§ã‚’ãƒ†ãƒ¼ãƒ–ãƒ«è¡¨ç¤ºï¼ˆä¿®æ­£ç‰ˆï¼‰
            store_data = []
            for idx, store in enumerate(stores[:20]):  # æœ€æ–°20ä»¶
                created_date = datetime.fromtimestamp(store['created_at'])
                store_data.append({
                    "ç•ªå·"            : idx + 1,
                    "åå‰"            : store['name'],
                    "ID"              : store['id'],
                    "ãƒ•ã‚¡ã‚¤ãƒ«æ•°"      : store['file_counts'],
                    "ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ä½¿ç”¨é‡": f"{store['usage_bytes']:,} bytes",
                    "ä½œæˆæ—¥æ™‚"        : created_date.strftime("%Y-%m-%d %H:%M:%S")
                })

            df_stores = pd.DataFrame(store_data)
            st.dataframe(df_stores, use_container_width=True, hide_index=True)

            # çµ±è¨ˆæƒ…å ±
            total_storage = sum(store['usage_bytes'] for store in stores)
            col1, col2, col3 = st.columns(3)
            with col1:
                st.metric("ç·Vector Storeæ•°", len(stores))
            with col2:
                st.metric("ç·ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ä½¿ç”¨é‡", f"{total_storage / (1024 * 1024):.1f} MB")
            with col3:
                total_files = sum(store['file_counts'] for store in stores)
                st.metric("ç·ãƒ•ã‚¡ã‚¤ãƒ«æ•°", total_files)

        except Exception as e:
            st.error(f"Vector Storeä¸€è¦§ã®å–å¾—ã«å¤±æ•—: {e}")
            logger.error(f"Vector Storeä¸€è¦§å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")


# ===================================================================
# ãƒ¡ã‚¤ãƒ³é–¢æ•°
# ===================================================================
def initialize_session_state():
    """ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã®åˆæœŸåŒ–"""
    if 'vector_store_results' not in st.session_state:
        st.session_state.vector_store_results = {}
    if 'processing_status' not in st.session_state:
        st.session_state.processing_status = {}


def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†é–¢æ•°"""

    # ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã®åˆæœŸåŒ–
    initialize_session_state()

    # UIç®¡ç†ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
    ui = VectorStoreUI()

    # ãƒšãƒ¼ã‚¸è¨­å®š
    ui.setup_page()
    ui.setup_header()

    # OpenAI SDKåˆ©ç”¨å¯èƒ½æ€§ç¢ºèª
    if not OPENAI_AVAILABLE:
        st.error("OpenAI SDKãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚`pip install openai` ã§ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ãã ã•ã„ã€‚")
        return

    # APIã‚­ãƒ¼ç¢ºèª
    if not os.getenv("OPENAI_API_KEY"):
        st.error("ğŸ”‘ OpenAI APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
        st.code("export OPENAI_API_KEY='your-api-key-here'")
        st.info("APIã‚­ãƒ¼ã‚’è¨­å®šã—ã¦ã‹ã‚‰ã‚¢ãƒ—ãƒªã‚’å†èµ·å‹•ã—ã¦ãã ã•ã„")
        return

    # ã‚µã‚¤ãƒ‰ãƒãƒ¼è¨­å®š
    selected_model, process_all = ui.setup_sidebar()

    # Vector Store Manager ã®åˆæœŸåŒ–
    try:
        manager = VectorStoreManager()
        ui.manager = manager
    except Exception as e:
        st.error(f"Vector Store Manager ã®åˆæœŸåŒ–ã«å¤±æ•—: {e}")
        return

    # ãƒ¡ã‚¤ãƒ³ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚¨ãƒªã‚¢
    tab1, tab2, tab3, tab4 = st.tabs(["ğŸ”— å€‹åˆ¥ä½œæˆ", "ğŸŒ çµ±åˆä½œæˆ", "ğŸ“ ãƒ•ã‚¡ã‚¤ãƒ«çŠ¶æ³", "ğŸ“š æ—¢å­˜Storeä¸€è¦§"])

    with tab1:
        # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆé¸æŠã¾ãŸã¯ä¸€æ‹¬å‡¦ç†
        if process_all:
            st.subheader("ğŸš€ å…¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä¸€æ‹¬å‡¦ç†")
            selected_datasets = list(ui.configs.keys())

            # å…¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆçŠ¶æ³è¡¨ç¤º
            all_files_exist = True
            output_dir = Path("OUTPUT")

            for dataset_type in selected_datasets:
                config = ui.configs[dataset_type]
                filepath = output_dir / config.filename

                if filepath.exists():
                    st.success(f"âœ… {config.description}: {config.filename}")
                else:
                    st.error(f"âŒ {config.description}: {config.filename} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                    all_files_exist = False

            if not all_files_exist:
                st.warning("âš ï¸ ä¸€éƒ¨ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚å€‹åˆ¥å‡¦ç†ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„ã€‚")
                selected_datasets = []

        else:
            # å€‹åˆ¥ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆé¸æŠ
            selected_datasets = ui.display_dataset_selection()

        # Vector Storeä½œæˆå®Ÿè¡Œ
        if selected_datasets:
            st.markdown("---")
            col1, col2 = st.columns([3, 1])

            with col1:
                st.write(f"**é¸æŠãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæ•°**: {len(selected_datasets)}")
                for dataset_type in selected_datasets:
                    config = ui.configs[dataset_type]
                    st.write(f"- {config.description}")

            with col2:
                create_button = st.button(
                    "ğŸš€ Vector Storeä½œæˆé–‹å§‹",
                    type="primary",
                    use_container_width=True
                )

            if create_button:
                # å‡¦ç†å®Ÿè¡Œ
                results = {}

                # å…¨ä½“ã®é€²è¡ŒçŠ¶æ³è¡¨ç¤º
                st.markdown("---")
                st.subheader("ğŸš€ Vector Storeä½œæˆé€²è¡ŒçŠ¶æ³")

                overall_progress = st.progress(0)
                overall_status = st.empty()

                for idx, dataset_type in enumerate(selected_datasets):
                    config = ui.configs[dataset_type]

                    # å€‹åˆ¥ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®é€²è¡ŒçŠ¶æ³
                    with st.container():
                        st.write(f"### ğŸ“‹ {idx + 1}/{len(selected_datasets)}: {config.description}")

                        # å€‹åˆ¥é€²è¡ŒçŠ¶æ³ãƒãƒ¼
                        dataset_progress = st.progress(0)
                        dataset_status = st.empty()

                        try:
                            # Vector Storeä½œæˆå®Ÿè¡Œ
                            dataset_progress.progress(0.1)
                            dataset_status.text("ğŸ”„ å‡¦ç†é–‹å§‹...")

                            with st.spinner(f"ğŸ”„ {config.description} ã‚’å‡¦ç†ä¸­..."):
                                result = manager.process_single_dataset(dataset_type)
                                results[dataset_type] = result

                            if result["success"]:
                                dataset_progress.progress(1.0)
                                dataset_status.success(f"âœ… å®Œäº† - Vector Store ID: `{result['vector_store_id']}`")

                                # è©³ç´°æƒ…å ±è¡¨ç¤º
                                col1, col2, col3 = st.columns(3)
                                with col1:
                                    st.metric("å‡¦ç†è¡Œæ•°",
                                              f"{result.get('processed_lines', 0):,} / {result.get('total_lines', 0):,}")
                                with col2:
                                    st.metric("ä½œæˆãƒãƒ£ãƒ³ã‚¯æ•°", f"{result['created_chunks']:,}")
                                with col3:
                                    st.metric("æ¨å®šã‚µã‚¤ã‚º", f"{result.get('estimated_size_mb', 0):.1f} MB")

                                # è­¦å‘ŠãŒã‚ã‚‹å ´åˆã¯è¡¨ç¤º
                                if result.get("warnings"):
                                    for warning in result["warnings"]:
                                        st.warning(warning)

                            else:
                                dataset_progress.progress(0)
                                dataset_status.error(f"âŒ å¤±æ•—: {result['error']}")

                                # ã‚¨ãƒ©ãƒ¼å¯¾å‡¦æ³•ã®ææ¡ˆ
                                if "too large" in result['error'].lower():
                                    st.info("ğŸ’¡ ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºãŒå¤§ãã™ãã¾ã™ã€‚è¨­å®šã‚’èª¿æ•´ã—ã¦å†è©¦è¡Œã—ã¦ãã ã•ã„ã€‚")
                                elif "not found" in result['error'].lower():
                                    st.info("ğŸ’¡ å¿…è¦ãªãƒ•ã‚¡ã‚¤ãƒ«ãŒOUTPUTãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«å­˜åœ¨ã™ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")

                        except Exception as e:
                            error_msg = f"äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼: {str(e)}"
                            results[dataset_type] = {"success": False, "error": error_msg}
                            dataset_progress.progress(0)
                            dataset_status.error(f"âŒ {error_msg}")
                            logger.error(f"{dataset_type} å‡¦ç†ä¸­ã®ä¾‹å¤–: {e}")

                        st.markdown("---")

                        # å…¨ä½“é€²è¡ŒçŠ¶æ³æ›´æ–°
                        overall_progress.progress((idx + 1) / len(selected_datasets))
                        overall_status.text(f"é€²è¡ŒçŠ¶æ³: {idx + 1}/{len(selected_datasets)} å®Œäº†")

                # å…¨ä½“å®Œäº†
                overall_progress.progress(1.0)
                overall_status.success("ğŸ‰ å…¨ä½“å‡¦ç†å®Œäº†!")

                # çµæœã‚’ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã«ä¿å­˜
                st.session_state.vector_store_results = results

                # çµæœè¡¨ç¤º
                st.markdown("---")
                successful, failed = ui.display_results(results)

                # çµæœã®JSONãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
                if results:
                    result_json = json.dumps(results, indent=2, ensure_ascii=False)
                    st.download_button(
                        label="ğŸ“„ çµæœã‚’JSONã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                        data=result_json,
                        file_name=f"vector_store_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json",
                        mime="application/json"
                    )

                    # æˆåŠŸã—ãŸVector Store IDã®ä¸€è¦§è¡¨ç¤º
                    if successful:
                        st.subheader("ğŸ”— ä½œæˆã•ã‚ŒãŸVector Store IDä¸€è¦§")
                        id_list = []
                        for dataset_type, result in successful.items():
                            config = ui.configs[dataset_type]
                            id_list.append(f"# {config.description}")
                            id_list.append(f"{dataset_type.upper()}_VECTOR_STORE_ID = \"{result['vector_store_id']}\"")
                            id_list.append("")

                        id_text = "\n".join(id_list)
                        st.code(id_text, language="python")

                        st.download_button(
                            label="ğŸ“‹ Vector Store IDã‚’ã‚³ãƒ”ãƒ¼ç”¨ãƒ†ã‚­ã‚¹ãƒˆã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                            data=id_text,
                            file_name=f"vector_store_ids_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt",
                            mime="text/plain"
                        )

        else:
            st.info("ğŸ‘† ä½œæˆã™ã‚‹Vector Storeã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’é¸æŠã—ã¦ãã ã•ã„")

    with tab2:
        # çµ±åˆVector Storeä½œæˆã‚¿ãƒ–
        st.header("ğŸŒ çµ±åˆVector Storeä½œæˆ")
        st.markdown("è¤‡æ•°ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’1ã¤ã®çµ±åˆVector Storeã«ã¾ã¨ã‚ã¾ã™")
        
        # çµ±åˆè¨­å®šã®è¡¨ç¤º
        unified_config = VectorStoreConfig.get_unified_config()
        with st.expander("âš™ï¸ çµ±åˆè¨­å®š", expanded=True):
            col1, col2, col3 = st.columns(3)
            with col1:
                st.write("**ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚º**: ", unified_config.chunk_size)
                st.write("**ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—**: ", unified_config.overlap)
            with col2:
                st.write("**æœ€å¤§ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º**: ", f"{unified_config.max_file_size_mb} MB")
                st.write("**æœ€å¤§ãƒãƒ£ãƒ³ã‚¯æ•°**: ", f"{unified_config.max_chunks_per_file:,}")
            with col3:
                st.write("**Storeå**: ", unified_config.store_name)
        
        # çµ±åˆã™ã‚‹ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆé¸æŠ
        st.subheader("ğŸ“‹ çµ±åˆã™ã‚‹ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆé¸æŠ")
        
        # ã‚¯ã‚¤ãƒƒã‚¯é¸æŠãƒœã‚¿ãƒ³
        col1, col2, col3 = st.columns(3)
        with col1:
            if st.button("ğŸ”„ å…¨ã¦é¸æŠ", key="select_all_unified"):
                for dataset_type in ui.configs.keys():
                    st.session_state[f"unified_{dataset_type}"] = True
                st.rerun()
        with col2:
            if st.button("âŒ å…¨ã¦è§£é™¤", key="deselect_all_unified"):
                for dataset_type in ui.configs.keys():
                    st.session_state[f"unified_{dataset_type}"] = False
                st.rerun()
        
        # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆé¸æŠãƒã‚§ãƒƒã‚¯ãƒœãƒƒã‚¯ã‚¹
        selected_for_unified = []
        output_dir = Path("OUTPUT")
        
        for dataset_type, config in ui.configs.items():
            filepath = output_dir / config.filename
            file_exists = filepath.exists()
            
            col1, col2 = st.columns([3, 1])
            with col1:
                selected = st.checkbox(
                    config.description,
                    key=f"unified_{dataset_type}",
                    disabled=not file_exists,
                    help=f"ãƒ•ã‚¡ã‚¤ãƒ«: {config.filename}"
                )
                if selected:
                    selected_for_unified.append(dataset_type)
            
            with col2:
                if file_exists:
                    file_size = filepath.stat().st_size
                    st.success(f"âœ… {file_size / (1024*1024):.1f} MB")
                else:
                    st.error("âŒ ãƒ•ã‚¡ã‚¤ãƒ«ä¸åœ¨")
        
        # çµ±åˆå®Ÿè¡Œãƒœã‚¿ãƒ³
        if selected_for_unified:
            st.markdown("---")
            
            # é¸æŠçŠ¶æ³ã‚µãƒãƒªãƒ¼
            st.write(f"**é¸æŠã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ**: {len(selected_for_unified)}å€‹")
            
            # æ¨å®šçµ±è¨ˆ
            total_estimated_size = 0
            for dataset_type in selected_for_unified:
                config = ui.configs[dataset_type]
                filepath = output_dir / config.filename
                if filepath.exists():
                    total_estimated_size += filepath.stat().st_size
            
            col1, col2, col3 = st.columns(3)
            with col1:
                st.metric("é¸æŠãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæ•°", len(selected_for_unified))
            with col2:
                st.metric("æ¨å®šåˆè¨ˆã‚µã‚¤ã‚º", f"{total_estimated_size / (1024*1024):.1f} MB")
            with col3:
                st.metric("åˆ¶é™ã‚µã‚¤ã‚º", f"{unified_config.max_file_size_mb} MB")
            
            # çµ±åˆå®Ÿè¡Œãƒœã‚¿ãƒ³
            if st.button("ğŸš€ çµ±åˆVector Storeä½œæˆ", type="primary", key="create_unified"):
                with st.spinner("çµ±åˆVector Storeã‚’ä½œæˆä¸­..."):
                    # ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼
                    progress = st.progress(0)
                    status = st.empty()
                    
                    # çµ±åˆå‡¦ç†å®Ÿè¡Œ
                    status.text("ğŸ“Š ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’çµ±åˆä¸­...")
                    progress.progress(0.3)
                    
                    result = manager.process_unified_datasets(selected_for_unified)
                    
                    progress.progress(1.0)
                    
                    # çµæœè¡¨ç¤º
                    if result["success"]:
                        status.success("âœ… çµ±åˆVector Storeä½œæˆå®Œäº†!")
                        
                        # æˆåŠŸæƒ…å ±è¡¨ç¤º
                        st.success(f"Vector Store ID: `{result['vector_store_id']}`")
                        
                        col1, col2, col3, col4 = st.columns(4)
                        with col1:
                            st.metric("å‡¦ç†è¡Œæ•°", f"{result['processed_lines']:,}/{result['total_lines']:,}")
                        with col2:
                            st.metric("ä½œæˆãƒãƒ£ãƒ³ã‚¯æ•°", f"{result['created_chunks']:,}")
                        with col3:
                            st.metric("æœ€çµ‚ã‚µã‚¤ã‚º", f"{result['estimated_size_mb']:.1f} MB")
                        with col4:
                            st.metric("å«ã¾ã‚Œã‚‹ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ", len(result['config_used']['datasets_included']))
                        
                        # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆåˆ¥çµ±è¨ˆ
                        if result.get('dataset_stats'):
                            with st.expander("ğŸ“Š ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆåˆ¥çµ±è¨ˆ", expanded=True):
                                stats_data = []
                                for ds_type, stats in result['dataset_stats'].items():
                                    stats_data.append({
                                        "ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ": ui.configs[ds_type].description,
                                        "å…ƒã®è¡Œæ•°": f"{stats['original_lines']:,}",
                                        "ãƒãƒ£ãƒ³ã‚¯æ•°": f"{stats['chunks']:,}",
                                        "ã‚µã‚¤ã‚º(MB)": f"{stats['size_mb']:.1f}"
                                    })
                                df_stats = pd.DataFrame(stats_data)
                                st.dataframe(df_stats, use_container_width=True)
                        
                        # è­¦å‘Šè¡¨ç¤º
                        if result.get('warnings'):
                            with st.expander(f"âš ï¸ è­¦å‘Š ({len(result['warnings'])}ä»¶)", expanded=False):
                                for warning in result['warnings']:
                                    st.warning(warning)
                        
                        # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒœã‚¿ãƒ³
                        result_json = json.dumps(result, indent=2, ensure_ascii=False)
                        st.download_button(
                            label="ğŸ“„ çµ±åˆçµæœã‚’JSONã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                            data=result_json,
                            file_name=f"unified_vector_store_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json",
                            mime="application/json"
                        )
                        
                        # IDä¿å­˜ç”¨ãƒ†ã‚­ã‚¹ãƒˆ
                        id_text = f"# çµ±åˆVector Store\nUNIFIED_VECTOR_STORE_ID = \"{result['vector_store_id']}\""
                        st.code(id_text, language="python")
                        
                    else:
                        status.error("âŒ çµ±åˆVector Storeä½œæˆå¤±æ•—")
                        st.error(f"ã‚¨ãƒ©ãƒ¼: {result['error']}")
                        
                        if result.get('warnings'):
                            for warning in result['warnings']:
                                st.warning(warning)
        else:
            st.info("ğŸ‘† çµ±åˆã™ã‚‹ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’é¸æŠã—ã¦ãã ã•ã„")

    with tab3:
        # ãƒ•ã‚¡ã‚¤ãƒ«çŠ¶æ³è¡¨ç¤º
        ui.display_file_status()

        # è©³ç´°æƒ…å ±
        with st.expander("ğŸ“ ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªè©³ç´°æƒ…å ±", expanded=False):
            output_dir = Path("OUTPUT")
            if output_dir.exists():
                st.write(f"**ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãƒ‘ã‚¹**: `{output_dir.absolute()}`")

                # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§ï¼ˆCSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’å¯¾è±¡ï¼‰
                csv_files = list(output_dir.glob("preprocessed_*.csv"))
                txt_files = list(output_dir.glob("*.txt"))
                st.write(f"**å‰å‡¦ç†æ¸ˆã¿CSVãƒ•ã‚¡ã‚¤ãƒ«æ•°**: {len(csv_files)}")
                st.write(f"**ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«æ•°**: {len(txt_files)}")

                # CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’å…ˆã«è¡¨ç¤º
                if csv_files:
                    st.write("**å‰å‡¦ç†æ¸ˆã¿CSVãƒ•ã‚¡ã‚¤ãƒ«:**")
                    for file in csv_files:
                        file_size = file.stat().st_size
                        modified_time = datetime.fromtimestamp(file.stat().st_mtime)
                        st.write(f"- {file.name}: {file_size:,} bytes ({modified_time.strftime('%Y-%m-%d %H:%M:%S')})")
                
                # ãã®ä»–ã®ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«
                if txt_files:
                    st.write("**ãã®ä»–ã®ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«:**")
                    for file in txt_files:
                        file_size = file.stat().st_size
                        modified_time = datetime.fromtimestamp(file.stat().st_mtime)
                        st.write(f"- {file.name}: {file_size:,} bytes ({modified_time.strftime('%Y-%m-%d %H:%M:%S')})")
            else:
                st.error(f"OUTPUTãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒå­˜åœ¨ã—ã¾ã›ã‚“: {output_dir}")

    with tab4:
        # æ—¢å­˜Vector Storeä¸€è¦§
        ui.display_existing_stores(manager)

        # ãƒªãƒ•ãƒ¬ãƒƒã‚·ãƒ¥ãƒœã‚¿ãƒ³
        if st.button("ğŸ”„ ä¸€è¦§ã‚’æ›´æ–°", type="secondary"):
            st.rerun()

    # ãƒ•ãƒƒã‚¿ãƒ¼
    st.markdown("---")
    st.markdown("### ğŸ”— Vector Storeä½œæˆã‚¢ãƒ—ãƒªï¼ˆå®Œå…¨ä¿®æ­£ç‰ˆï¼‰")
    st.markdown("âœ¨ **OpenAI Vector Storeè‡ªå‹•ä½œæˆã‚·ã‚¹ãƒ†ãƒ ** - ã‚¨ãƒ©ãƒ¼å®Œå…¨ä¿®æ­£ç‰ˆ")
    st.markdown("ğŸš€ **æ©Ÿèƒ½**: è¤‡æ•°ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå¯¾å¿œã€å‹å®‰å…¨å‡¦ç†ã€ã‚¨ãƒ©ãƒ¼è§£æ±ºæ¸ˆã¿")


if __name__ == "__main__":
    main()

# å®Ÿè¡Œã‚³ãƒãƒ³ãƒ‰:
# streamlit run a31_make_cloud_vector_store_vsid.py --server.port=8502
