#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
èµ·å‹•: streamlit run a00_rag_data_evaluation.py --server.port=8501

Q&Aæ­£è§£çŽ‡æ¤œè¨¼ãƒ„ãƒ¼ãƒ«
QAãƒšã‚¢ã¨Qdrantã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã®å›žç­”ã‚’æ¯”è¼ƒã—ã€ã‚³ã‚µã‚¤ãƒ³é¡žä¼¼åº¦ã«ã‚ˆã‚‹æ­£è§£çŽ‡ã‚’è©•ä¾¡
"""

import streamlit as st
import pandas as pd
import numpy as np
from qdrant_client import QdrantClient
from openai import OpenAI
import os
from sklearn.metrics.pairwise import cosine_similarity
from typing import List, Dict, Tuple
import time
from pathlib import Path

# Streamlitãƒšãƒ¼ã‚¸è¨­å®š
st.set_page_config(
    page_title="Q&Aæ­£è§£çŽ‡æ¤œè¨¼ãƒ„ãƒ¼ãƒ«",
    page_icon="âœ…",
    layout="wide"
)

# å®šæ•°
QDRANT_URL = os.getenv("QDRANT_URL", "http://localhost:6333")
EMBEDDING_MODEL = "text-embedding-3-small"
EMBEDDING_DIM = 1536

# Q/Aãƒšã‚¢ã¨ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã®å¯¾å¿œ
QA_DATASETS = {
    "CC News (4,646ä»¶)": {
        "path": "qa_output/a02_qa_pairs_cc_news.csv",
        "collections": [
            ("raw_cc_news", "Raw ãƒ‡ãƒ¼ã‚¿ (660ä»¶)"),
            ("qa_cc_news_a02_llm", "LLMãƒ™ãƒ¼ã‚¹ (9,290ä»¶)"),
            ("qa_cc_news_a03_rule", "ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹ (8,556ä»¶)"),
            ("qa_cc_news_a10_hybrid", "ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ (4,334ä»¶)")
        ]
    },
    "Livedoor (965ä»¶)": {
        "path": "qa_output/a02_qa_pairs_livedoor.csv",
        "collections": [
            ("raw_livedoor", "Raw ãƒ‡ãƒ¼ã‚¿ (20,193ä»¶)"),
            ("qa_livedoor_a02_20_llm", "LLMãƒ™ãƒ¼ã‚¹ (964ä»¶)"),
            ("qa_livedoor_a03_rule", "ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹ (466ä»¶)"),
            ("qa_livedoor_a10_hybrid", "ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ (2,175ä»¶)")
        ]
    }
}

# OpenAI/Qdrantã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåˆæœŸåŒ–
@st.cache_resource
def init_clients():
    """APIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®åˆæœŸåŒ–"""
    openai_client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
    qdrant_client = QdrantClient(url=QDRANT_URL)
    return openai_client, qdrant_client

@st.cache_data
def load_qa_data(file_path: str):
    """Q&Aãƒšã‚¢ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿"""
    try:
        df = pd.read_csv(file_path)
        # ãƒ‡ãƒ¼ã‚¿ã®ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³
        if 'question' not in df.columns or 'answer' not in df.columns:
            st.error("CSVãƒ•ã‚¡ã‚¤ãƒ«ã«'question'ã¨'answer'ã‚«ãƒ©ãƒ ãŒå¿…è¦ã§ã™")
            return pd.DataFrame()
        return df
    except FileNotFoundError:
        st.error(f"ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {file_path}")
        return pd.DataFrame()
    except Exception as e:
        st.error(f"ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
        return pd.DataFrame()

def get_embedding(text: str, client: OpenAI) -> List[float]:
    """ãƒ†ã‚­ã‚¹ãƒˆã®embeddingãƒ™ã‚¯ãƒˆãƒ«ã‚’å–å¾—"""
    try:
        response = client.embeddings.create(
            input=text,
            model=EMBEDDING_MODEL
        )
        return response.data[0].embedding
    except Exception as e:
        st.error(f"Embeddingå–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
        return []

def search_qdrant(query: str, collection_name: str, openai_client: OpenAI, qdrant_client: QdrantClient, top_k: int = 1) -> Tuple[str, float]:
    """Qdrantã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã‹ã‚‰æ¤œç´¢ã—ã€æœ€é«˜ã‚¹ã‚³ã‚¢ã®çµæžœã‚’è¿”ã™"""
    try:
        # ã‚¯ã‚¨ãƒªã®embeddingå–å¾—
        query_vector = get_embedding(query, openai_client)
        if not query_vector:
            return "", 0.0

        # Qdrantæ¤œç´¢
        results = qdrant_client.query_points(
            collection_name=collection_name,
            query=query_vector,
            limit=top_k
        ).points

        if results:
            # æœ€é«˜ã‚¹ã‚³ã‚¢ã®çµæžœã‚’å–å¾—
            top_result = results[0]
            answer = top_result.payload.get('answer', '') if top_result.payload else ''
            score = top_result.score
            return answer, score

        return "", 0.0
    except Exception as e:
        st.error(f"Qdrantæ¤œç´¢ã‚¨ãƒ©ãƒ¼ ({collection_name}): {e}")
        return "", 0.0

def calculate_cosine_similarity(text1: str, text2: str, openai_client: OpenAI) -> float:
    """2ã¤ã®ãƒ†ã‚­ã‚¹ãƒˆé–“ã®ã‚³ã‚µã‚¤ãƒ³é¡žä¼¼åº¦ã‚’è¨ˆç®—"""
    try:
        # ä¸¡æ–¹ã®ãƒ†ã‚­ã‚¹ãƒˆã®embeddingã‚’å–å¾—
        vec1 = get_embedding(text1, openai_client)
        vec2 = get_embedding(text2, openai_client)

        if not vec1 or not vec2:
            return 0.0

        # ã‚³ã‚µã‚¤ãƒ³é¡žä¼¼åº¦è¨ˆç®—
        similarity = cosine_similarity([vec1], [vec2])[0][0]
        return float(similarity)
    except Exception as e:
        st.error(f"é¡žä¼¼åº¦è¨ˆç®—ã‚¨ãƒ©ãƒ¼: {e}")
        return 0.0

def process_qa_pairs(df: pd.DataFrame, collection_name: str, openai_client: OpenAI, qdrant_client: QdrantClient, progress_bar=None) -> pd.DataFrame:
    """Q&Aãƒšã‚¢ã‚’å‡¦ç†ã—ã€é¡žä¼¼åº¦ã‚’è¨ˆç®—"""
    results = []
    total = len(df)

    for idx, row in df.iterrows():
        if progress_bar:
            progress_bar.progress((idx + 1) / total, text=f"å‡¦ç†ä¸­: {idx + 1}/{total}")

        qa_question = row['question']
        qa_answer = row['answer']

        # Qdrantã‹ã‚‰å›žç­”å–å¾—
        ans_output, search_score = search_qdrant(qa_question, collection_name, openai_client, qdrant_client)

        # ã‚³ã‚µã‚¤ãƒ³é¡žä¼¼åº¦è¨ˆç®—
        if ans_output:
            similarity = calculate_cosine_similarity(qa_answer, ans_output, openai_client)
        else:
            similarity = 0.0

        results.append({
            'question': qa_question,
            'correct_answer': qa_answer,  # CSVã‹ã‚‰ã®æ­£è§£ç­”
            'retrieved_answer': ans_output,  # Qdrantã‹ã‚‰ã®æ¤œç´¢çµæžœ
            'search_score': search_score,
            'similarity': similarity * 100  # ãƒ‘ãƒ¼ã‚»ãƒ³ãƒˆè¡¨ç¤º
        })

    return pd.DataFrame(results)

def create_summary_stats(df: pd.DataFrame) -> pd.DataFrame:
    """çµ±è¨ˆã‚µãƒžãƒªãƒ¼ãƒ†ãƒ¼ãƒ–ãƒ«ã®ä½œæˆ"""
    if df.empty or 'similarity' not in df:
        return pd.DataFrame()

    total = len(df)

    # ã‚¹ã‚³ã‚¢ãƒ¬ãƒ³ã‚¸ã”ã¨ã®é›†è¨ˆ
    ranges = [
        ('80%ä»¥ä¸Š', df[df['similarity'] >= 80].shape[0]),
        ('60%ä»¥ä¸Š', df[(df['similarity'] >= 60) & (df['similarity'] < 80)].shape[0]),
        ('40%ä»¥ä¸Š', df[(df['similarity'] >= 40) & (df['similarity'] < 60)].shape[0]),
        ('39%ä»¥ä¸‹', df[df['similarity'] < 40].shape[0])
    ]

    summary_data = []
    for label, count in ranges:
        percentage = (count / total * 100) if total > 0 else 0
        summary_data.append({
            'ã‚¹ã‚³ã‚¢ç¯„å›²': label,
            'ä»¶æ•°': str(count),  # æ–‡å­—åˆ—ã«çµ±ä¸€
            'æ§‹æˆæ¯”(%)': f"{percentage:.1f}%"
        })

    # å¹³å‡å€¤ã‚’è¿½åŠ 
    avg_similarity = df['similarity'].mean()
    summary_data.append({
        'ã‚¹ã‚³ã‚¢ç¯„å›²': 'å¹³å‡ã‚¹ã‚³ã‚¢',
        'ä»¶æ•°': '-',
        'æ§‹æˆæ¯”(%)': f"{avg_similarity:.1f}%"
    })

    return pd.DataFrame(summary_data)

def main():
    """ãƒ¡ã‚¤ãƒ³ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³"""
    st.title("ðŸŽ¯ Q&Aæ­£è§£çŽ‡æ¤œè¨¼ãƒ„ãƒ¼ãƒ«")
    st.markdown("QAãƒšã‚¢ã¨Qdrantã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã®å›žç­”ã‚’æ¯”è¼ƒã—ã€ã‚³ã‚µã‚¤ãƒ³é¡žä¼¼åº¦ã«ã‚ˆã‚‹æ­£è§£çŽ‡ã‚’è©•ä¾¡ã—ã¾ã™")

    # ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåˆæœŸåŒ–
    openai_client, qdrant_client = init_clients()

    # ã‚µã‚¤ãƒ‰ãƒãƒ¼ï¼šè¨­å®š
    with st.sidebar:
        st.header("âš™ï¸ è¨­å®š")

        # Q/Aãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆé¸æŠž
        st.subheader("Q/Aãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ")
        selected_dataset = st.selectbox(
            "æ¤œè¨¼å…ƒãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ",
            options=list(QA_DATASETS.keys())
        )

        # é¸æŠžã—ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«å¯¾å¿œã™ã‚‹ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³è¡¨ç¤º
        dataset_config = QA_DATASETS[selected_dataset]
        st.subheader("æ¤œè¨¼å¯¾è±¡ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³")

        collection_options = {
            name: f"{display} - {name}"
            for name, display in dataset_config["collections"]
        }
        selected_collection = st.selectbox(
            "ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³é¸æŠž",
            options=list(collection_options.keys()),
            format_func=lambda x: collection_options[x]
        )

        # å‡¦ç†ä»¶æ•°åˆ¶é™
        st.subheader("å‡¦ç†ã‚ªãƒ—ã‚·ãƒ§ãƒ³")
        limit_processing = st.checkbox("å‡¦ç†ä»¶æ•°ã‚’åˆ¶é™", value=True)
        if limit_processing:
            max_rows = st.number_input("æœ€å¤§å‡¦ç†ä»¶æ•°", min_value=1, max_value=5000, value=100, step=10)
        else:
            max_rows = None

        # å®Ÿè¡Œãƒœã‚¿ãƒ³
        run_analysis = st.button("ðŸš€ æ¤œè¨¼é–‹å§‹", type="primary", use_container_width=True)

    # ãƒ¡ã‚¤ãƒ³ã‚³ãƒ³ãƒ†ãƒ³ãƒ„
    col1, col2 = st.columns([1, 2])

    with col1:
        st.subheader("ðŸ“Š ãƒ‡ãƒ¼ã‚¿æƒ…å ±")
        # é¸æŠžã—ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ãƒ‘ã‚¹ã‹ã‚‰Q&Aãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
        qa_file_path = dataset_config["path"]
        qa_df = load_qa_data(qa_file_path)
        if not qa_df.empty:
            st.metric("ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ", selected_dataset)
            st.metric("Q&Aãƒšã‚¢ç·æ•°", f"{len(qa_df):,}ä»¶")
            st.metric("é¸æŠžã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³", selected_collection)
            if limit_processing and max_rows:
                st.info(f"å‡¦ç†å¯¾è±¡: æœ€åˆã®{max_rows}ä»¶")

    with col2:
        st.subheader("ðŸ“ˆ æ¤œè¨¼çµæžœã‚µãƒžãƒªãƒ¼")
        result_container = st.container()

    # æ¤œè¨¼å®Ÿè¡Œ
    if run_analysis and not qa_df.empty:
        with st.spinner(f"ðŸ”„ {selected_collection}ã®æ¤œè¨¼ã‚’å®Ÿè¡Œä¸­..."):
            # å‡¦ç†å¯¾è±¡ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™
            process_df = qa_df.head(max_rows) if max_rows else qa_df

            # ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼
            progress_bar = st.progress(0, text="åˆæœŸåŒ–ä¸­...")

            # Q&Aãƒšã‚¢å‡¦ç†
            results_df = process_qa_pairs(
                process_df,
                selected_collection,
                openai_client,
                qdrant_client,
                progress_bar
            )

            progress_bar.empty()

            if not results_df.empty:
                # ã‚µãƒžãƒªãƒ¼çµ±è¨ˆ
                with result_container:
                    summary_stats = create_summary_stats(results_df)
                    st.dataframe(summary_stats, use_container_width=True, hide_index=True)

                # è©³ç´°çµæžœ
                st.subheader("ðŸ” è©³ç´°çµæžœ")

                # ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
                col1, col2, col3 = st.columns(3)
                with col1:
                    min_score = st.slider("æœ€å°é¡žä¼¼åº¦(%)", 0, 100, 0)
                with col2:
                    max_score = st.slider("æœ€å¤§é¡žä¼¼åº¦(%)", 0, 100, 100)
                with col3:
                    sort_order = st.selectbox("ä¸¦ã³é †", ["é¡žä¼¼åº¦é™é †", "é¡žä¼¼åº¦æ˜‡é †"])

                # ãƒ•ã‚£ãƒ«ã‚¿é©ç”¨
                filtered_df = results_df[
                    (results_df['similarity'] >= min_score) &
                    (results_df['similarity'] <= max_score)
                ]

                # ã‚½ãƒ¼ãƒˆ
                ascending = sort_order == "é¡žä¼¼åº¦æ˜‡é †"
                filtered_df = filtered_df.sort_values('similarity', ascending=ascending)

                # è¡¨ç¤º
                display_df = filtered_df[['question', 'correct_answer', 'retrieved_answer', 'similarity', 'search_score']].copy()
                display_df = display_df.rename(columns={
                    'question': 'è³ªå•',
                    'correct_answer': 'æ­£è§£ç­”',
                    'retrieved_answer': 'æ¤œç´¢å›žç­”',
                    'similarity': 'é¡žä¼¼åº¦(%)',
                    'search_score': 'æ¤œç´¢ã‚¹ã‚³ã‚¢'
                })
                st.dataframe(
                    display_df.round(2),
                    use_container_width=True,
                    height=400
                )

                # CSVå‡ºåŠ›
                csv = filtered_df.to_csv(index=False, encoding='utf-8-sig')
                st.download_button(
                    label="ðŸ“¥ çµæžœã‚’CSVãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                    data=csv,
                    file_name=f"qa_verification_{selected_collection}_{time.strftime('%Y%m%d_%H%M%S')}.csv",
                    mime="text/csv"
                )

                # çµ±è¨ˆæƒ…å ±
                with st.expander("ðŸ“Š çµ±è¨ˆè©³ç´°"):
                    col1, col2, col3, col4 = st.columns(4)
                    with col1:
                        st.metric("å¹³å‡é¡žä¼¼åº¦", f"{results_df['similarity'].mean():.1f}%")
                    with col2:
                        st.metric("ä¸­å¤®å€¤", f"{results_df['similarity'].median():.1f}%")
                    with col3:
                        st.metric("æ¨™æº–åå·®", f"{results_df['similarity'].std():.1f}%")
                    with col4:
                        st.metric("æœ€é«˜é¡žä¼¼åº¦", f"{results_df['similarity'].max():.1f}%")

                st.success("âœ… æ¤œè¨¼å®Œäº†ï¼")
            else:
                st.error("æ¤œè¨¼å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ")

if __name__ == "__main__":
    main()