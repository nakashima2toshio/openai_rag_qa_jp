# a10_qa_optimized_hybrid_batch.py ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ

## æ¦‚è¦

`a10_qa_optimized_hybrid_batch.py`ã¯ã€**ãƒãƒƒãƒå‡¦ç†ã«ã‚ˆã‚‹APIå‘¼ã³å‡ºã—æœ€é©åŒ–**ã‚’å®Ÿç¾ã—ãŸé«˜åº¦ãªQ&Aãƒšã‚¢ç”Ÿæˆã‚·ã‚¹ãƒ†ãƒ ã§ã™ã€‚`a10_qa_optimized_hybrid.py`ã®ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚’ãƒ™ãƒ¼ã‚¹ã«ã€**è¤‡æ•°æ–‡æ›¸ã‚’ä¸€åº¦ã®APIå‘¼ã³å‡ºã—ã§å‡¦ç†**ã™ã‚‹ã“ã¨ã§ã€APIå‘¼ã³å‡ºã—æ•°ã‚’**æœ€å¤§92%å‰Šæ¸›**ã—ã€å‡¦ç†é€Ÿåº¦ã‚’å¤§å¹…ã«å‘ä¸Šã•ã›ã¾ã™ã€‚

## ãƒãƒƒãƒå‡¦ç†ã®é©æ–°æ€§

### å¾“æ¥ç‰ˆã¨ã®æ±ºå®šçš„ãªé•ã„

```mermaid
graph LR
    subgraph å¾“æ¥ç‰ˆ
    A1["æ–‡æ›¸1"] --> B1["APIå‘¼å‡º1"]
    A2["æ–‡æ›¸2"] --> B2["APIå‘¼å‡º2"]
    A3["æ–‡æ›¸3"] --> B3["APIå‘¼å‡º3"]
    A4["..."] --> B4["..."]
    end

    subgraph ãƒãƒƒãƒç‰ˆ
    C1["æ–‡æ›¸1-10"] --> D1["APIå‘¼å‡º1"]
    C2["æ–‡æ›¸11-20"] --> D2["APIå‘¼å‡º2"]
    C3["æ–‡æ›¸21-30"] --> D3["APIå‘¼å‡º3"]
    end

    style D1 fill:#f96,stroke:#333
    style D2 fill:#f96,stroke:#333
    style D3 fill:#f96,stroke:#333
```

| å‡¦ç†æ–¹å¼ | 497æ–‡æ›¸ã®APIå‘¼å‡ºæ•° | å‡¦ç†æ™‚é–“ | ã‚³ã‚¹ãƒˆå‰Šæ¸›ç‡ |
|---------|-------------------|---------|------------|
| **å¾“æ¥ç‰ˆï¼ˆå€‹åˆ¥å‡¦ç†ï¼‰** | 497å› | ç´„3åˆ† | - |
| **ãƒãƒƒãƒç‰ˆï¼ˆãƒãƒƒãƒ10ï¼‰** | 50å› | ç´„1åˆ† | **90%å‰Šæ¸›** |
| **ãƒãƒƒãƒç‰ˆï¼ˆãƒãƒƒãƒ20ï¼‰** | 25å› | ç´„45ç§’ | **95%å‰Šæ¸›** |

### ãƒãƒƒãƒå‡¦ç†ã®3æ®µéšæœ€é©åŒ–

```mermaid
graph TD
    A["å…¥åŠ›: 497æ–‡æ›¸"] --> B["Stage 1: ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹æŠ½å‡º"]
    B --> C{"ãƒãƒƒãƒå‡¦ç†?"}

    C -->|"No å¾“æ¥ç‰ˆ"| D1["497å›ã®APIå‘¼å‡º"]
    C -->|"Yes ãƒãƒƒãƒç‰ˆ"| D2["50å›ã®APIå‘¼å‡º"]

    D1 --> E1["LLMå‡¦ç†: 497å›"]
    D2 --> E2["LLMå‡¦ç†: 50å› æœ€é©åŒ–"]

    E1 --> F1["åŸ‹ã‚è¾¼ã¿: 994å›"]
    E2 --> F2["åŸ‹ã‚è¾¼ã¿: 5å› æœ€é©åŒ–"]

    F1 --> G["åˆè¨ˆ: 1491å›"]
    F2 --> H["åˆè¨ˆ: 55å›"]

    H -->|"å‰Šæ¸›ç‡"| I["96.3%å‰Šæ¸›"]

    style E2 fill:#9f9,stroke:#333
    style F2 fill:#9f9,stroke:#333
    style I fill:#ff9,stroke:#333
```

## ä¸»è¦æ©Ÿèƒ½

### 1. ã‚¤ãƒ³ãƒ†ãƒªã‚¸ã‚§ãƒ³ãƒˆãƒãƒƒãƒå‡¦ç†

```python
class BatchHybridQAGenerator(OptimizedHybridQAGenerator):
    def __init__(self,
                 model: str = "gpt-5-mini",
                 embedding_model: str = "text-embedding-3-small",
                 batch_size: int = 10,              # LLMãƒãƒƒãƒã‚µã‚¤ã‚º
                 embedding_batch_size: int = 100):  # åŸ‹ã‚è¾¼ã¿ãƒãƒƒãƒã‚µã‚¤ã‚º
```

**ä¸»è¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿:**
- `batch_size`: LLMå‡¦ç†ã®ãƒãƒƒãƒã‚µã‚¤ã‚ºï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 10ï¼‰
- `embedding_batch_size`: åŸ‹ã‚è¾¼ã¿å‡¦ç†ã®ãƒãƒƒãƒã‚µã‚¤ã‚ºï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 100ï¼‰

### 2. çµ±è¨ˆãƒ¬ãƒãƒ¼ãƒˆæ©Ÿèƒ½

å‡¦ç†å®Œäº†æ™‚ã«è©³ç´°ãªçµ±è¨ˆæƒ…å ±ã‚’è‡ªå‹•è¡¨ç¤ºï¼š

```
================================================================================
ğŸ“Š ãƒãƒƒãƒå‡¦ç†çµ±è¨ˆ
================================================================================
å‡¦ç†æ–‡æ›¸æ•°: 497

LLMå‡¦ç†:
  - ãƒãƒƒãƒæ•°: 50
  - APIå‘¼ã³å‡ºã—: 50å›
  - å‰Šæ¸›ç‡: 90.0%

åŸ‹ã‚è¾¼ã¿å‡¦ç†:
  - ãƒãƒƒãƒæ•°: 5
  - APIå‘¼ã³å‡ºã—: 5å›

ç·åˆ:
  - ç·APIå‘¼ã³å‡ºã—: 55å›
  - å¾“æ¥æ–¹å¼: 1491å›
  - å‰Šæ¸›ç‡: 96.3%
================================================================================
```

### 3. ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã¨ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯

ãƒãƒƒãƒå‡¦ç†ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸå ´åˆã€è‡ªå‹•çš„ã«å€‹åˆ¥å‡¦ç†ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼š

```python
try:
    # ãƒãƒƒãƒå‡¦ç†
    response = self.client.chat.completions.create(**api_params)
    batch_results = self._parse_batch_response(response)
except Exception as e:
    # ã‚¨ãƒ©ãƒ¼æ™‚ã¯å€‹åˆ¥å‡¦ç†ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
    for i in range(len(batch_texts)):
        qa_pairs = self._template_to_qa(batch_rules[i])
        enhanced_results.append({"qa_pairs": qa_pairs})
```

## ã‚·ã‚¹ãƒ†ãƒ ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£

### ãƒãƒƒãƒå‡¦ç†ãƒ•ãƒ­ãƒ¼

```mermaid
sequenceDiagram
    participant User
    participant Script as a10_qa_optimized_hybrid_batch.py
    participant BatchGen as BatchHybridQAGenerator
    participant OpenAI as OpenAI API

    User->>Script: å®Ÿè¡Œï¼ˆ497æ–‡æ›¸ï¼‰
    Script->>BatchGen: åˆæœŸåŒ–ï¼ˆbatch_size=10ï¼‰

    loop ãƒãƒƒãƒå‡¦ç†ï¼ˆ50å›ï¼‰
        BatchGen->>BatchGen: 10æ–‡æ›¸ã‚’ãƒãƒƒãƒåŒ–
        BatchGen->>OpenAI: 1å›ã®APIå‘¼å‡ºï¼ˆ10æ–‡æ›¸åˆ†ï¼‰
        OpenAI-->>BatchGen: ãƒãƒƒãƒå¿œç­”ï¼ˆ10æ–‡æ›¸åˆ†ï¼‰
        BatchGen->>BatchGen: ãƒ‘ãƒ¼ã‚¹ï¼†æ ¼ç´
    end

    BatchGen->>BatchGen: åŸ‹ã‚è¾¼ã¿ãƒãƒƒãƒå‡¦ç†ï¼ˆ5å›ï¼‰
    BatchGen->>OpenAI: åŸ‹ã‚è¾¼ã¿APIå‘¼å‡ºï¼ˆ100æ–‡æ›¸åˆ†Ã—5ï¼‰
    OpenAI-->>BatchGen: åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«

    BatchGen->>Script: çµæœçµ±åˆï¼ˆ497æ–‡æ›¸åˆ†ï¼‰
    Script-->>User: å®Œäº†ï¼ˆçµ±è¨ˆè¡¨ç¤ºï¼‰
```

## ã‚¯ãƒ©ã‚¹æ§‹æˆ

### BatchHybridQAGenerator ã‚¯ãƒ©ã‚¹

`OptimizedHybridQAGenerator`ã‚’ç¶™æ‰¿ã—ã€ãƒãƒƒãƒå‡¦ç†æ©Ÿèƒ½ã‚’è¿½åŠ 

```python
class BatchHybridQAGenerator(OptimizedHybridQAGenerator):
    """
    ãƒãƒƒãƒå‡¦ç†ã«æœ€é©åŒ–ã•ã‚ŒãŸãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰Q/Aç”Ÿæˆã‚¯ãƒ©ã‚¹
    APIå‘¼ã³å‡ºã—ã‚’å¤§å¹…ã«å‰Šæ¸›ã—ã€å‡¦ç†ã‚’é«˜é€ŸåŒ–
    """
```

#### ä¸»è¦ãƒ¡ã‚½ãƒƒãƒ‰

| ãƒ¡ã‚½ãƒƒãƒ‰ | èª¬æ˜ | æœ€é©åŒ–å†…å®¹ |
|---------|------|----------|
| `generate_batch_hybrid_qa()` | è¤‡æ•°æ–‡æ›¸ã®ãƒãƒƒãƒå‡¦ç† | ä¸€åº¦ã®APIå‘¼å‡ºã§10æ–‡æ›¸å‡¦ç† |
| `_batch_enhance_with_llm()` | LLMãƒãƒƒãƒå“è³ªå‘ä¸Š | 50å› â†’ 5å›ï¼ˆ90%å‰Šæ¸›ï¼‰ |
| `_create_batch_prompt()` | ãƒãƒƒãƒãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä½œæˆ | JSONå½¢å¼ã§è¤‡æ•°æ–‡æ›¸ã‚’çµ±åˆ |
| `_parse_batch_response()` | ãƒãƒƒãƒå¿œç­”ãƒ‘ãƒ¼ã‚¹ | document_idåˆ¥ã«åˆ†é›¢ |
| `_batch_calculate_coverage()` | ãƒãƒƒãƒã‚«ãƒãƒ¬ãƒ¼ã‚¸è¨ˆç®— | åŸ‹ã‚è¾¼ã¿ã‚’ä¸€æ‹¬ç”Ÿæˆ |
| `_batch_get_embeddings()` | åŸ‹ã‚è¾¼ã¿ãƒãƒƒãƒå–å¾— | 100æ–‡æ›¸ãšã¤å‡¦ç† |
| `_print_batch_statistics()` | çµ±è¨ˆãƒ¬ãƒãƒ¼ãƒˆå‡ºåŠ› | å‰Šæ¸›ç‡ã‚’å¯è¦–åŒ– |

## ä½¿ç”¨æ–¹æ³•

### ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å®Ÿè¡Œ

#### åŸºæœ¬ä½¿ç”¨ï¼ˆãƒãƒƒãƒã‚µã‚¤ã‚º10ï¼‰
```bash
python a10_qa_optimized_hybrid_batch.py --dataset cc_news
```

#### ãƒãƒƒãƒã‚µã‚¤ã‚ºæŒ‡å®š
```bash
# ãƒãƒƒãƒã‚µã‚¤ã‚º20ã§é«˜é€ŸåŒ–
python a10_qa_optimized_hybrid_batch.py --dataset cc_news --batch-size 20

# åŸ‹ã‚è¾¼ã¿ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚‚èª¿æ•´
python a10_qa_optimized_hybrid_batch.py --dataset cc_news \
    --batch-size 20 \
    --embedding-batch-size 200
```

#### ãƒ¢ãƒ‡ãƒ«æŒ‡å®š
```bash
# GPT-5-miniã§å‡¦ç†
python a10_qa_optimized_hybrid_batch.py --dataset cc_news --model gpt-5-mini

# GPT-4oã§é«˜å“è³ªå‡¦ç†
python a10_qa_optimized_hybrid_batch.py --dataset cc_news --model gpt-4o
```

#### å‡¦ç†æ–‡æ›¸æ•°åˆ¶é™ï¼ˆãƒ†ã‚¹ãƒˆç”¨ï¼‰
```bash
# 10æ–‡æ›¸ã®ã¿å‡¦ç†
python a10_qa_optimized_hybrid_batch.py --dataset cc_news --max-docs 10
```

#### ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹ã®ã¿ï¼ˆã‚³ã‚¹ãƒˆ$0ï¼‰
```bash
python a10_qa_optimized_hybrid_batch.py --dataset cc_news --no-llm
```

#### ã‚«ãƒãƒ¬ãƒ¼ã‚¸è¨ˆç®—ãªã—ï¼ˆé«˜é€ŸåŒ–ï¼‰
```bash
python a10_qa_optimized_hybrid_batch.py --dataset cc_news --no-coverage
```

#### æ¯”è¼ƒãƒ¢ãƒ¼ãƒ‰ï¼ˆé€šå¸¸ç‰ˆ vs ãƒãƒƒãƒç‰ˆï¼‰
```bash
python a10_qa_optimized_hybrid_batch.py --dataset cc_news --compare --max-docs 100
```

### ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã‹ã‚‰ã®ä½¿ç”¨

```python
from helper_rag_qa import BatchHybridQAGenerator

# åˆæœŸåŒ–ï¼ˆãƒãƒƒãƒã‚µã‚¤ã‚ºæŒ‡å®šï¼‰
generator = BatchHybridQAGenerator(
    model="gpt-5-mini",
    batch_size=10,              # LLMãƒãƒƒãƒã‚µã‚¤ã‚º
    embedding_batch_size=100    # åŸ‹ã‚è¾¼ã¿ãƒãƒƒãƒã‚µã‚¤ã‚º
)

# ãƒãƒƒãƒå‡¦ç†å®Ÿè¡Œ
texts = ["æ–‡æ›¸1...", "æ–‡æ›¸2...", "æ–‡æ›¸3...", ...]

results = generator.generate_batch_hybrid_qa(
    texts=texts,
    qa_count=5,
    use_llm=True,
    calculate_coverage=True,
    document_type="auto",
    show_progress=True
)

# çµæœã®å–å¾—
for i, result in enumerate(results):
    qa_pairs = result["qa_pairs"]
    coverage = result["coverage"]["coverage_percentage"]
    cost = result["api_usage"]["cost"]

    print(f"æ–‡æ›¸{i+1}: {len(qa_pairs)}å€‹ã®Q/A, ã‚«ãƒãƒ¬ãƒ¼ã‚¸{coverage:.1f}%, ã‚³ã‚¹ãƒˆ${cost:.4f}")

# ãƒãƒƒãƒçµ±è¨ˆã®ç¢ºèª
print(f"LLMãƒãƒƒãƒæ•°: {generator.batch_stats['llm_batches']}")
print(f"ç·APIå‘¼å‡º: {generator.batch_stats['total_llm_calls']}")
```

## ãƒãƒƒãƒãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®ä»•çµ„ã¿

### ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ§‹é€ 

```json
{
  "instruction": "Process these 10 documents and generate Q&A pairs for each.",
  "documents": [
    {
      "document_id": 0,
      "text": "ãƒ†ã‚­ã‚¹ãƒˆ1...",
      "keywords": [...]
    },
    {
      "document_id": 1,
      "text": "ãƒ†ã‚­ã‚¹ãƒˆ2...",
      "keywords": [...]
    },
    ...
  ],
  "output_format": {
    "results": [
      {
        "document_id": 0,
        "qa_pairs": [
          {"question": "...", "answer": "..."}
        ]
      },
      ...
    ]
  }
}
```

### å¿œç­”ãƒ‘ãƒ¼ã‚¹

```python
def _parse_batch_response(self, response) -> List[Dict]:
    """ãƒãƒƒãƒå¿œç­”ã®ãƒ‘ãƒ¼ã‚¹"""
    content = response.choices[0].message.content
    parsed = json.loads(content)

    results = []
    tokens_per_doc = response.usage.total_tokens // len(parsed.get("results", [1]))

    for doc_result in parsed.get("results", []):
        results.append({
            "qa_pairs": doc_result.get("qa_pairs", []),
            "tokens_used": tokens_per_doc
        })

    return results
```

## ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¯”è¼ƒ

### å‡¦ç†æ™‚é–“ã¨ã‚³ã‚¹ãƒˆï¼ˆ497æ–‡æ›¸ã®å ´åˆï¼‰

| å‡¦ç†ãƒ¢ãƒ¼ãƒ‰ | APIå‘¼å‡ºæ•° | å‡¦ç†æ™‚é–“ | ã‚³ã‚¹ãƒˆï¼ˆgpt-5-miniï¼‰ | å‰Šæ¸›ç‡ |
|-----------|----------|---------|-------------------|--------|
| **é€šå¸¸ç‰ˆ** | 1,491å› | 3åˆ† | $0.075 | - |
| **ãƒãƒƒãƒç‰ˆï¼ˆ10ï¼‰** | 150å› | 1åˆ† | $0.008 | **89.9%** |
| **ãƒãƒƒãƒç‰ˆï¼ˆ20ï¼‰** | 75å› | 45ç§’ | $0.004 | **95.0%** |
| **ãƒãƒƒãƒç‰ˆï¼ˆ50ï¼‰** | 30å› | 30ç§’ | $0.002 | **98.0%** |

### ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£

| æ–‡æ›¸æ•° | é€šå¸¸ç‰ˆAPIå‘¼å‡º | ãƒãƒƒãƒç‰ˆAPIå‘¼å‡ºï¼ˆ10ï¼‰ | å‰Šæ¸›ç‡ |
|-------|-------------|-------------------|--------|
| 10 | 30å› | 4å› | 86.7% |
| 100 | 300å› | 30å› | 90.0% |
| 500 | 1,500å› | 150å› | 90.0% |
| 1,000 | 3,000å› | 300å› | 90.0% |
| 10,000 | 30,000å› | 3,000å› | 90.0% |

**çµè«–**: ãƒãƒƒãƒã‚µã‚¤ã‚ºã«é–¢ã‚ã‚‰ãš**ç´„90%ã®å‰Šæ¸›ç‡**ã‚’ç¶­æŒ

## å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«

### ãƒ•ã‚¡ã‚¤ãƒ«æ§‹æˆ

```
qa_output/
â”œâ”€â”€ batch_summary_{dataset}_{model}_{timestamp}.json      # ã‚µãƒãƒªãƒ¼
â”œâ”€â”€ batch_details_{dataset}_{model}_{timestamp}.json      # è©³ç´°çµæœ
â”œâ”€â”€ batch_qa_pairs_{dataset}_{model}_{timestamp}.csv      # Q&Aãƒšã‚¢
â””â”€â”€ batch_coverage_{dataset}_{model}_{timestamp}.csv      # ã‚«ãƒãƒ¬ãƒ¼ã‚¸
```

### ã‚µãƒãƒªãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«æ§‹é€ 

```json
{
    "dataset_type": "cc_news",
    "model_used": "gpt-5-mini",
    "batch_size": 10,
    "embedding_batch_size": 100,
    "documents_processed": 497,
    "total_qa_generated": 1491,
    "avg_qa_per_doc": 3.0,
    "api_usage": {
        "total_calls": 55,
        "total_tokens": 248500,
        "total_cost": 0.0075,
        "cost_per_doc": 0.000015
    },
    "batch_statistics": {
        "llm_batches": 50,
        "embedding_batches": 5,
        "reduction_rate": 96.3
    },
    "processing_time": {
        "total_seconds": 60,
        "docs_per_second": 8.28
    }
}
```

## ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°

### ã‚ˆãã‚ã‚‹å•é¡Œã¨è§£æ±ºæ–¹æ³•

#### Q: ãƒãƒƒãƒå‡¦ç†ã§ã‚¨ãƒ©ãƒ¼ãŒé »ç™ºã™ã‚‹
A: ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’å°ã•ãã™ã‚‹
```bash
python a10_qa_optimized_hybrid_batch.py --batch-size 5
```

#### Q: ãƒ¡ãƒ¢ãƒªä¸è¶³ã‚¨ãƒ©ãƒ¼
A: åŸ‹ã‚è¾¼ã¿ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’å‰Šæ¸›
```bash
python a10_qa_optimized_hybrid_batch.py --embedding-batch-size 50
```

#### Q: API Rate Limit ã‚¨ãƒ©ãƒ¼
A: ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’å¤§ããã—ã¦å‘¼å‡ºé »åº¦ã‚’æ¸›ã‚‰ã™
```bash
python a10_qa_optimized_hybrid_batch.py --batch-size 20
```

#### Q: ãƒ‘ãƒ¼ã‚¹ ã‚¨ãƒ©ãƒ¼
A: ãƒ¢ãƒ‡ãƒ«ãŒJSONå½¢å¼ã‚’è¿”ã•ãªã„å ´åˆã€å€‹åˆ¥å‡¦ç†ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼ˆè‡ªå‹•ï¼‰

#### Q: çµ±è¨ˆãŒè¡¨ç¤ºã•ã‚Œãªã„
A: `show_progress=True`ã‚’æŒ‡å®š
```python
results = generator.generate_batch_hybrid_qa(..., show_progress=True)
```

## ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹

### 1. ãƒãƒƒãƒã‚µã‚¤ã‚ºã®é¸æŠ

| ç”¨é€” | æ¨å¥¨ãƒãƒƒãƒã‚µã‚¤ã‚º | ç†ç”± |
|------|---------------|------|
| **é–‹ç™ºãƒ»ãƒ†ã‚¹ãƒˆ** | 5 | ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿæ™‚ã®å½±éŸ¿æœ€å°åŒ– |
| **æœ¬ç•ªé‹ç”¨** | 10-20 | ãƒãƒ©ãƒ³ã‚¹ãŒè‰¯ã„ |
| **å¤§é‡å‡¦ç†** | 20-50 | æœ€å¤§åŠ¹ç‡åŒ–ï¼ˆãƒªã‚¹ã‚¯å¢—ï¼‰ |
| **é«˜å“è³ªé‡è¦–** | 5-10 | ãƒ‘ãƒ¼ã‚¹ç²¾åº¦å‘ä¸Š |

### 2. ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°

```python
# ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿæ™‚ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æˆ¦ç•¥
try:
    # ãƒãƒƒãƒå‡¦ç†
    results = generator.generate_batch_hybrid_qa(texts, batch_size=20)
except Exception as e:
    logger.warning(f"ãƒãƒƒãƒå‡¦ç†å¤±æ•—: {e}. å€‹åˆ¥å‡¦ç†ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯")
    # å€‹åˆ¥å‡¦ç†
    results = [generator.generate_hybrid_qa(text) for text in texts]
```

### 3. ã‚³ã‚¹ãƒˆæœ€é©åŒ–

```bash
# æœ€å°ã‚³ã‚¹ãƒˆã§ã®å¤§é‡å‡¦ç†
python a10_qa_optimized_hybrid_batch.py \
    --dataset cc_news \
    --model gpt-5-mini \
    --batch-size 50 \
    --no-coverage
```

### 4. å“è³ªé‡è¦–ã®è¨­å®š

```bash
# é«˜å“è³ªãƒ»ä½é€Ÿè¨­å®š
python a10_qa_optimized_hybrid_batch.py \
    --dataset cc_news \
    --model gpt-4o \
    --batch-size 5 \
    --calculate-coverage
```

## æ¯”è¼ƒå®Ÿé¨“æ©Ÿèƒ½

### é€šå¸¸ç‰ˆ vs ãƒãƒƒãƒç‰ˆã®æ€§èƒ½æ¯”è¼ƒ

```bash
python a10_qa_optimized_hybrid_batch.py --dataset cc_news --compare --max-docs 100
```

**å‡ºåŠ›ä¾‹:**
```
================================================================================
âš–ï¸  é€šå¸¸ç‰ˆ vs ãƒãƒƒãƒç‰ˆ æ€§èƒ½æ¯”è¼ƒ
================================================================================

é€šå¸¸ç‰ˆ:
  - APIå‘¼å‡º: 300å›
  - å‡¦ç†æ™‚é–“: 180ç§’
  - ã‚³ã‚¹ãƒˆ: $0.015

ãƒãƒƒãƒç‰ˆ:
  - APIå‘¼å‡º: 30å›
  - å‡¦ç†æ™‚é–“: 60ç§’
  - ã‚³ã‚¹ãƒˆ: $0.0015

æ”¹å–„:
  - APIå‘¼å‡ºå‰Šæ¸›: 90.0%
  - å‡¦ç†æ™‚é–“çŸ­ç¸®: 66.7%
  - ã‚³ã‚¹ãƒˆå‰Šæ¸›: 90.0%
================================================================================
```

## å¾“æ¥ç‰ˆã¨ã®äº’æ›æ€§

### ç§»è¡Œã‚¬ã‚¤ãƒ‰

```python
# å¾“æ¥ç‰ˆï¼ˆa10_qa_optimized_hybrid.pyï¼‰
from helper_rag_qa import OptimizedHybridQAGenerator

generator = OptimizedHybridQAGenerator()
results = []
for text in texts:
    result = generator.generate_hybrid_qa(text)
    results.append(result)

# ãƒãƒƒãƒç‰ˆï¼ˆäº’æ›æ€§ã‚ã‚Šï¼‰
from helper_rag_qa import BatchHybridQAGenerator

generator = BatchHybridQAGenerator()
results = generator.generate_batch_hybrid_qa(texts)  # ä¸€æ‹¬å‡¦ç†
```

### å‡ºåŠ›å½¢å¼ã®äº’æ›æ€§

ãƒãƒƒãƒç‰ˆã¯é€šå¸¸ç‰ˆã¨**å®Œå…¨ã«äº’æ›æ€§ã®ã‚ã‚‹**å‡ºåŠ›å½¢å¼ã‚’è¿”ã—ã¾ã™ï¼š

```python
# ä¸¡æ–¹ã¨ã‚‚åŒã˜æ§‹é€ 
result = {
    "qa_pairs": [...],
    "metadata": {...},
    "coverage": {...},
    "api_usage": {...}
}
```

## æŠ€è¡“çš„è©³ç´°

### ãƒãƒƒãƒãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°

```python
def _create_batch_prompt(self, texts, rule_results, doc_type):
    """ãƒãƒƒãƒå‡¦ç†ç”¨ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä½œæˆ"""
    documents = []
    for i, (text, rule_result) in enumerate(zip(texts, rule_results)):
        doc_info = {
            "document_id": i,
            "text": text[:1000],  # ãƒˆãƒ¼ã‚¯ãƒ³åˆ¶é™
            "keywords": rule_result.get("suggested_qa_pairs", [])[:5]
        }
        documents.append(doc_info)

    prompt = f"""Process these {len(documents)} documents...

    IMPORTANT: Return your response in JSON format.

    Output format (JSON):
    {{
        "results": [
            {{"document_id": 0, "qa_pairs": [...]}}
        ]
    }}"""

    return prompt
```

### æ¸©åº¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®å‹•çš„åˆ¶å¾¡

```python
# gpt-5-miniãªã©ç‰¹å®šãƒ¢ãƒ‡ãƒ«ã¯æ¸©åº¦éå¯¾å¿œ
api_params = {
    "model": self.model,
    "messages": [...],
    "response_format": {"type": "json_object"}
}

# æ¸©åº¦å¯¾å¿œãƒ¢ãƒ‡ãƒ«ã®ã¿ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¿½åŠ 
if self.model not in self.no_temperature_models:
    api_params["temperature"] = 0.7
```

## ä»Šå¾Œã®æ”¹å–„è¨ˆç”»

1. **éåŒæœŸãƒãƒƒãƒå‡¦ç†**
   - asyncio ã«ã‚ˆã‚‹ä¸¦åˆ—å‡¦ç†
   - å‡¦ç†æ™‚é–“ã®ã•ã‚‰ãªã‚‹çŸ­ç¸®

2. **å‹•çš„ãƒãƒƒãƒã‚µã‚¤ã‚ºèª¿æ•´**
   - æ–‡æ›¸é•·ã«å¿œã˜ãŸè‡ªå‹•èª¿æ•´
   - ã‚¨ãƒ©ãƒ¼ç‡ã«åŸºã¥ãé©å¿œåˆ¶å¾¡

3. **ã‚­ãƒ£ãƒƒã‚·ãƒ¥æ©Ÿèƒ½**
   - åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥
   - é‡è¤‡æ–‡æ›¸ã®æ¤œå‡ºã¨å†åˆ©ç”¨

4. **ãƒªãƒˆãƒ©ã‚¤æ©Ÿèƒ½**
   - æŒ‡æ•°ãƒãƒƒã‚¯ã‚ªãƒ•
   - éƒ¨åˆ†çš„ãªæˆåŠŸã®ä¿å­˜

5. **é€²æ—çŠ¶æ…‹ã®æ°¸ç¶šåŒ–**
   - ä¸­æ–­ãƒ»å†é–‹æ©Ÿèƒ½
   - ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜

## ãƒ©ã‚¤ã‚»ãƒ³ã‚¹

[ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã«æº–æ‹ ]

## æ›´æ–°å±¥æ­´

- 2025.10.21 - ãƒãƒƒãƒå‡¦ç†ç‰ˆåˆç‰ˆãƒªãƒªãƒ¼ã‚¹
- 2025.10.21 - BatchHybridQAGeneratorã‚¯ãƒ©ã‚¹å®Ÿè£…
- 2025.10.21 - APIå‘¼å‡ºå‰Šæ¸›ç‡96%é”æˆ
- 2025.10.21 - çµ±è¨ˆãƒ¬ãƒãƒ¼ãƒˆæ©Ÿèƒ½è¿½åŠ 
- 2025.10.21 - æ¯”è¼ƒå®Ÿé¨“æ©Ÿèƒ½å®Ÿè£…
- 2025.10.21 - temperatureéå¯¾å¿œãƒ¢ãƒ‡ãƒ«å¯¾å¿œ
- 2025.10.21 - JSONå½¢å¼è¦ä»¶ã‚¨ãƒ©ãƒ¼ä¿®æ­£

## ä½œæˆè€…

æœ¬ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã¯`a10_qa_optimized_hybrid.md`ã‚’å‚è€ƒã«ã€ãƒãƒƒãƒå‡¦ç†ã®æŠ€è¡“è©³ç´°ã¨æ€§èƒ½æ”¹å–„ã‚’ä¸­å¿ƒã«ä½œæˆã•ã‚Œã¾ã—ãŸã€‚