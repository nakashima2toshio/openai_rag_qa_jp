# a03_rag_qa_coverage_improved.py - æŠ€è¡“ä»•æ§˜æ›¸

## æœ€æ–°ãƒãƒ¼ã‚¸ãƒ§ãƒ³æƒ…å ±
- **æœ€çµ‚æ›´æ–°**: 2025-11-04
- **ãƒãƒ¼ã‚¸ãƒ§ãƒ³**: v2.4 (æœ€æ–°å®Ÿè£…ç‰ˆ)
- **ä¸»è¦æ©Ÿèƒ½**: ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹Q/Aç”Ÿæˆã€ãƒãƒƒãƒå‡¦ç†ã€å¤šæ®µéšã‚«ãƒãƒ¬ãƒƒã‚¸åˆ†æã€MeCabã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡ºã€è¨€èªå¯¾å¿œæ–‡åˆ†å‰²ã€è³ªå•å“è³ªæœ€é©åŒ–ï¼ˆpassageç•ªå·å‰Šé™¤ï¼‰

---

## ğŸ¯ ãƒã‚¤ãƒ©ã‚¤ãƒˆ

**ã‚«ãƒãƒ¬ãƒƒã‚¸ç‡99.7%ã‚’å®Ÿç¾ï¼å®Ÿè¡Œæ™‚é–“ã‚ãšã‹2åˆ†ã€APIå‘¼ã³å‡ºã—ãŸã£ãŸ5å›**

- **ã‚«ãƒãƒ¬ãƒƒã‚¸ç‡**: 99.7%ï¼ˆå¾“æ¥ç‰ˆ50-60%ã®2å€ï¼‰
- **å®Ÿè¡Œæ™‚é–“**: ç´„2åˆ†ï¼ˆå¾“æ¥ç‰ˆ20åˆ†ã‹ã‚‰90%çŸ­ç¸®ï¼‰
- **APIå‘¼ã³å‡ºã—**: 5å›ï¼ˆå¾“æ¥ç‰ˆ7,917å›ã‹ã‚‰99.94%å‰Šæ¸›ï¼‰
- **ã‚³ã‚¹ãƒˆ**: $0.00076ï¼ˆå¾“æ¥ç‰ˆ$0.08ã‹ã‚‰99.05%å‰Šæ¸›ï¼‰

---

## ğŸ“‹ æ¨å¥¨ã‚³ãƒãƒ³ãƒ‰

### 99.7%ã‚«ãƒãƒ¬ãƒƒã‚¸é”æˆç‰ˆï¼ˆå®Ÿç¸¾å€¤ï¼‰

```bash
python a03_rag_qa_coverage_improved.py \
  --input OUTPUT/preprocessed_cc_news.csv \
  --dataset cc_news \
  --analyze-coverage \
  --coverage-threshold 0.52 \
  --qa-per-chunk 12 \
  --max-chunks 609 \
  --max-docs 150 \
  --output qa_output
```

**å®Ÿè¡Œçµæœ:**
- å‡¦ç†æ–‡æ›¸: 150ä»¶
- ãƒãƒ£ãƒ³ã‚¯æ•°: 609å€‹
- Q/Aç”Ÿæˆæ•°: 7,308å€‹
- **ã‚«ãƒãƒ¬ãƒƒã‚¸ç‡: 99.7%** âœ…
- **å‡¦ç†æ™‚é–“: 2åˆ†**
- **APIå‘¼ã³å‡ºã—: 5å›**
  - ãƒãƒ£ãƒ³ã‚¯åŸ‹ã‚è¾¼ã¿: 1å›
  - Q/AåŸ‹ã‚è¾¼ã¿: 4å›
  - Q/Aç”Ÿæˆ: 0å›ï¼ˆãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹ã®ãŸã‚ï¼‰
- **ã‚³ã‚¹ãƒˆ: $0.00076**
- **å‡ºåŠ›å…ˆ: qa_output/a03/** â­NEW

---

## æ¦‚è¦

`a03_rag_qa_coverage_improved.py`ã¯ã€**ã‚«ãƒãƒ¬ãƒƒã‚¸ç‡99.7%é”æˆã‚’å®Ÿè¨¼ã—ãŸæ”¹è‰¯ç‰ˆ**ã®ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯ã‚«ãƒãƒ¬ãƒƒã‚¸åˆ†æã¨Q/Aç”Ÿæˆã‚·ã‚¹ãƒ†ãƒ ã§ã™ã€‚ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã«ç‰¹åŒ–ã—ã€è¶…é«˜ã‚«ãƒãƒ¬ãƒƒã‚¸ã¨å“è³ªã®Q/Aãƒšã‚¢ã‚’è¶…ä½ã‚³ã‚¹ãƒˆã§ç”Ÿæˆã—ã¾ã™ã€‚

**ä¸»ãªç‰¹å¾´**:
- å®Œå…¨ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹ï¼ˆLLMä¸è¦ï¼‰ã§ã‚³ã‚¹ãƒˆå‰Šæ¸›
- MeCabã«ã‚ˆã‚‹æ—¥æœ¬èªè¤‡åˆåè©æŠ½å‡ºï¼ˆè‡ªå‹•ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å¯¾å¿œï¼‰
- ãƒãƒƒãƒå‡¦ç†ã«ã‚ˆã‚‹APIå‘¼ã³å‡ºã—æœ€é©åŒ–
- 3ã¤ã®æˆ¦ç•¥çš„Q/Aç”Ÿæˆã‚¢ãƒ—ãƒ­ãƒ¼ãƒ

---

## ç›®æ¬¡

1. [ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£](#1-ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£)
2. [ä¸»è¦ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ](#2-ä¸»è¦ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ)
3. [MeCabã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡º](#3-mecabã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡º)
4. [Q/Aç”Ÿæˆæˆ¦ç•¥](#4-qaç”Ÿæˆæˆ¦ç•¥)
5. [ã‚«ãƒãƒ¬ãƒƒã‚¸åˆ†æ](#5-ã‚«ãƒãƒ¬ãƒƒã‚¸åˆ†æ)
6. [ãƒ‡ãƒ¼ã‚¿å‡¦ç†ãƒ•ãƒ­ãƒ¼](#6-ãƒ‡ãƒ¼ã‚¿å‡¦ç†ãƒ•ãƒ­ãƒ¼)
7. [ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°](#7-ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°)
8. [ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–](#8-ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–)
9. [å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«](#9-å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«)
10. [ä½¿ç”¨ä¾‹](#10-ä½¿ç”¨ä¾‹)
11. [ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°](#11-ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°)
12. [ä»Šå¾Œã®æ”¹å–„æ¡ˆ](#12-ä»Šå¾Œã®æ”¹å–„æ¡ˆ)

---

## 1. ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£

### 1.1 å‡¦ç†ãƒ•ãƒ­ãƒ¼

```
ãƒ¦ãƒ¼ã‚¶ãƒ¼å®Ÿè¡Œï¼ˆCSVå…¥åŠ›ï¼‰
         â†“
ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ãƒ»å‰å‡¦ç†ï¼ˆL212-262ï¼‰
         â†“
ãƒãƒ£ãƒ³ã‚¯ä½œæˆï¼ˆSemanticCoverageï¼‰ï¼ˆL511ï¼‰
         â†“
ãƒãƒ£ãƒ³ã‚¯ã”ã¨ã®Q/Aç”Ÿæˆï¼ˆL536-547ï¼‰
  â”œâ”€ æˆ¦ç•¥1: å…¨ä½“è¦ç´„Q/Aï¼ˆL290-299ï¼‰
  â”œâ”€ æˆ¦ç•¥2: æ–‡ã”ã¨è©³ç´°Q/Aï¼ˆL302-348ï¼‰
  â””â”€ æˆ¦ç•¥3: ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰Q/Aï¼ˆL350-363ï¼‰
      â”œâ”€ è‹±èª: æ­£è¦è¡¨ç¾ï¼ˆL329-338ï¼‰
      â””â”€ æ—¥æœ¬èª: MeCab â†’ æ­£è¦è¡¨ç¾ï¼ˆL351-362ï¼‰
         â†“
ãƒãƒƒãƒåŸ‹ã‚è¾¼ã¿ç”Ÿæˆï¼ˆL404-443ï¼‰
  â”œâ”€ ãƒãƒ£ãƒ³ã‚¯åŸ‹ã‚è¾¼ã¿ï¼ˆOpenAI API: 1å›ï¼‰
  â””â”€ Q/AåŸ‹ã‚è¾¼ã¿ï¼ˆOpenAI API: 1-4å›ï¼‰
         â†“
ã‚«ãƒãƒ¬ãƒƒã‚¸åˆ†æï¼ˆL446-485ï¼‰
  â”œâ”€ é¡ä¼¼åº¦è¡Œåˆ—è¨ˆç®—
  â”œâ”€ é–¾å€¤åˆ¤å®š
  â””â”€ çµ±è¨ˆæƒ…å ±ç”Ÿæˆ
         â†“
çµæœä¿å­˜ï¼ˆqa_output/a03/ï¼‰ï¼ˆL596-644ï¼‰
  â”œâ”€ qa_pairs_{dataset}_{timestamp}.json
  â”œâ”€ qa_pairs_{dataset}_{timestamp}.csv
  â”œâ”€ coverage_{dataset}_{timestamp}.json
  â””â”€ summary_{dataset}_{timestamp}.json
```

### 1.2 ã‚·ã‚¹ãƒ†ãƒ æ§‹æˆ

```
a03_rag_qa_coverage_improved.py
â”œâ”€â”€ KeywordExtractor ã‚¯ãƒ©ã‚¹ï¼ˆL60-177ï¼‰â­
â”‚   â”œâ”€â”€ __init__()ï¼ˆL68-88ï¼‰
â”‚   â”œâ”€â”€ _check_mecab_availability()ï¼ˆL89-98ï¼‰
â”‚   â”œâ”€â”€ extract()ï¼ˆL100-120ï¼‰
â”‚   â”œâ”€â”€ _extract_with_mecab()ï¼ˆL122-156ï¼‰
â”‚   â”œâ”€â”€ _extract_with_regex()ï¼ˆL158-165ï¼‰
â”‚   â””â”€â”€ _filter_and_count()ï¼ˆL167-176ï¼‰
â”œâ”€â”€ get_keyword_extractor()ï¼ˆL182-187ï¼‰
â”œâ”€â”€ load_input_data()ï¼ˆL212-262ï¼‰
â”œâ”€â”€ generate_comprehensive_qa_for_chunk()ï¼ˆL265-378ï¼‰
â”‚   â”œâ”€â”€ æˆ¦ç•¥1: å…¨ä½“è¦ç´„Q/A
â”‚   â”œâ”€â”€ æˆ¦ç•¥2: æ–‡ã”ã¨è©³ç´°Q/A
â”‚   â””â”€â”€ æˆ¦ç•¥3: ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡ºQ/A
â”œâ”€â”€ calculate_improved_coverage()ï¼ˆL381-487ï¼‰
â”‚   â”œâ”€â”€ ãƒãƒƒãƒåŸ‹ã‚è¾¼ã¿ç”Ÿæˆ
â”‚   â”œâ”€â”€ é‡ã¿ä»˜ã‘é¡ä¼¼åº¦è¨ˆç®—
â”‚   â””â”€â”€ çµ±è¨ˆæƒ…å ±ç”Ÿæˆ
â”œâ”€â”€ process_with_improved_methods()ï¼ˆL490-586ï¼‰
â”œâ”€â”€ save_results()ï¼ˆL589-644ï¼‰
â””â”€â”€ main()ï¼ˆL647-780ï¼‰
```

---

## 2. ä¸»è¦ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ

### 2.1 ã‚¤ãƒ³ãƒãƒ¼ãƒˆï¼ˆL31-34, L36-46ï¼‰

```python
from helper_rag_qa import (
    SemanticCoverage,        # ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯ãƒãƒ£ãƒ³ã‚¯ä½œæˆã€åŸ‹ã‚è¾¼ã¿ç”Ÿæˆã€è¨€èªå¯¾å¿œæ–‡åˆ†å‰²
    TemplateBasedQAGenerator,  # ã‚¤ãƒ³ãƒãƒ¼ãƒˆã®ã¿ï¼ˆæœªä½¿ç”¨ï¼‰
)
```

**SemanticCoverageã®æ–°æ©Ÿèƒ½ï¼ˆ2025-11-04æ›´æ–°ï¼‰**:
- **è¨€èªè‡ªå‹•åˆ¤å®š**: æ—¥æœ¬èª/è‹±èªã‚’è‡ªå‹•åˆ¤å®šã—ã€æœ€é©ãªæ–‡åˆ†å‰²æ–¹æ³•ã‚’é¸æŠ
- **MeCabçµ±åˆ**: æ—¥æœ¬èªãƒ†ã‚­ã‚¹ãƒˆã«å¯¾ã—ã¦MeCabã«ã‚ˆã‚‹é«˜ç²¾åº¦ãªæ–‡å¢ƒç•Œæ¤œå‡ºã‚’å®Ÿæ–½
- **è‡ªå‹•ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯**: MeCabå¤±æ•—æ™‚ã‚„è‹±èªãƒ†ã‚­ã‚¹ãƒˆã®å ´åˆã€æ­£è¦è¡¨ç¾ãƒ™ãƒ¼ã‚¹ã®æ–‡åˆ†å‰²ã«è‡ªå‹•åˆ‡ã‚Šæ›¿ãˆ
- **æŸ”è»Ÿãªç’°å¢ƒå¯¾å¿œ**: MeCabæœªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ç’°å¢ƒã§ã‚‚æ­£å¸¸ã«å‹•ä½œ

### 2.2 ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆè¨­å®šï¼ˆL189-209ï¼‰

```python
DATASET_CONFIGS = {
    "cc_news": {
        "name": "CC-Newsè‹±èªãƒ‹ãƒ¥ãƒ¼ã‚¹",
        "text_column": "Combined_Text",
        "title_column": "title",
        "lang": "en"
    },
    "japanese_text": {
        "name": "æ—¥æœ¬èªWebãƒ†ã‚­ã‚¹ãƒˆ",
        "text_column": "Combined_Text",
        "title_column": None,
        "lang": "ja"
    },
    "wikipedia_ja": {
        "name": "Wikipediaæ—¥æœ¬èªç‰ˆ",
        "text_column": "Combined_Text",
        "title_column": "title",
        "lang": "ja"
    }
}
```

---

## 3. MeCabã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡º

### 3.1 KeywordExtractorã‚¯ãƒ©ã‚¹ï¼ˆL60-177ï¼‰

**ç›®çš„**: MeCabã¨æ­£è¦è¡¨ç¾ã‚’çµ±åˆã—ãŸã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡ºï¼ˆè‡ªå‹•ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å¯¾å¿œï¼‰

#### 3.1.1 åˆæœŸåŒ–ï¼ˆL68-88ï¼‰

```python
def __init__(self, prefer_mecab: bool = True):
    """
    Args:
        prefer_mecab: MeCabã‚’å„ªå…ˆçš„ã«ä½¿ç”¨ã™ã‚‹ã‹ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: Trueï¼‰
    """
    self.prefer_mecab = prefer_mecab
    self.mecab_available = self._check_mecab_availability()

    # ã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰å®šç¾©ï¼ˆL77-82ï¼‰
    self.stopwords = {
        'ã“ã¨', 'ã‚‚ã®', 'ã“ã‚Œ', 'ãã‚Œ', 'ãŸã‚', 'ã‚ˆã†', 'ã•ã‚“',
        'ã¾ã™', 'ã§ã™', 'ã‚ã‚‹', 'ã„ã‚‹', 'ã™ã‚‹', 'ãªã‚‹', 'ã§ãã‚‹',
        'ã„ã†', 'çš„', 'ãª', 'ã«', 'ã‚’', 'ã¯', 'ãŒ', 'ã§', 'ã¨',
        'ã®', 'ã‹ã‚‰', 'ã¾ã§', 'ç­‰', 'ãªã©', 'ã‚ˆã‚‹', 'ãŠã', 'ãã‚‹'
    }
```

#### 3.1.2 MeCabåˆ©ç”¨å¯èƒ½æ€§ãƒã‚§ãƒƒã‚¯ï¼ˆL89-98ï¼‰

```python
def _check_mecab_availability(self) -> bool:
    """MeCabã®åˆ©ç”¨å¯èƒ½æ€§ã‚’ãƒã‚§ãƒƒã‚¯"""
    try:
        import MeCab
        tagger = MeCab.Tagger()
        tagger.parse("ãƒ†ã‚¹ãƒˆ")  # å®Ÿéš›ã«å‹•ä½œç¢ºèª
        return True
    except (ImportError, RuntimeError):
        return False
```

#### 3.1.3 ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡ºãƒ¡ã‚¤ãƒ³å‡¦ç†ï¼ˆL100-120ï¼‰

```python
def extract(self, text: str, top_n: int = 5) -> List[str]:
    """
    ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡ºï¼ˆè‡ªå‹•ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å¯¾å¿œï¼‰

    Returns:
        ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒªã‚¹ãƒˆï¼ˆé »åº¦é †ï¼‰
    """
    if self.mecab_available and self.prefer_mecab:
        try:
            keywords = self._extract_with_mecab(text, top_n)
            if keywords:
                return keywords
        except Exception as e:
            logger.warning(f"âš ï¸ MeCabæŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")

    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: æ­£è¦è¡¨ç¾ç‰ˆ
    return self._extract_with_regex(text, top_n)
```

#### 3.1.4 MeCabã«ã‚ˆã‚‹è¤‡åˆåè©æŠ½å‡ºï¼ˆL122-156ï¼‰

```python
def _extract_with_mecab(self, text: str, top_n: int) -> List[str]:
    """MeCabã‚’ä½¿ç”¨ã—ãŸè¤‡åˆåè©æŠ½å‡º"""
    import MeCab
    tagger = MeCab.Tagger()
    node = tagger.parseToNode(text)

    compound_buffer = []
    compound_nouns = []

    while node:
        features = node.feature.split(',')
        pos = features[0]  # å“è©

        if pos == 'åè©':
            compound_buffer.append(node.surface)
        else:
            # åè©ä»¥å¤–ãŒæ¥ãŸã‚‰ãƒãƒƒãƒ•ã‚¡ã‚’ãƒ•ãƒ©ãƒƒã‚·ãƒ¥
            if compound_buffer:
                compound_noun = ''.join(compound_buffer)
                if len(compound_noun) > 0:
                    compound_nouns.append(compound_noun)
                compound_buffer = []

        node = node.next

    # ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã¨é »åº¦ã‚«ã‚¦ãƒ³ãƒˆ
    return self._filter_and_count(compound_nouns, top_n)
```

#### 3.1.5 æ­£è¦è¡¨ç¾ã«ã‚ˆã‚‹ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡ºï¼ˆL158-165ï¼‰

```python
def _extract_with_regex(self, text: str, top_n: int) -> List[str]:
    """æ­£è¦è¡¨ç¾ã‚’ä½¿ç”¨ã—ãŸã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡º"""
    # ã‚«ã‚¿ã‚«ãƒŠèªã€æ¼¢å­—è¤‡åˆèªã€è‹±æ•°å­—ã‚’æŠ½å‡º
    pattern = r'[ã‚¡-ãƒ´ãƒ¼]{2,}|[ä¸€-é¾¥]{2,}|[A-Za-z]{2,}[A-Za-z0-9]*'
    words = re.findall(pattern, text)
    return self._filter_and_count(words, top_n)
```

### 3.2 ã‚·ãƒ³ã‚°ãƒ«ãƒˆãƒ³ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ï¼ˆL179-187ï¼‰

```python
_keyword_extractor = None

def get_keyword_extractor() -> KeywordExtractor:
    """KeywordExtractorã®ã‚·ãƒ³ã‚°ãƒ«ãƒˆãƒ³ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’å–å¾—"""
    global _keyword_extractor
    if _keyword_extractor is None:
        _keyword_extractor = KeywordExtractor()
    return _keyword_extractor
```

---

## 4. Q/Aç”Ÿæˆæˆ¦ç•¥

### 4.1 generate_comprehensive_qa_for_chunk()ï¼ˆL265-378ï¼‰

**ç›®çš„**: å˜ä¸€ãƒãƒ£ãƒ³ã‚¯ã«å¯¾ã—ã¦åŒ…æ‹¬çš„ãªQ/Aã‚’ç”Ÿæˆ

#### 4.1.1 æˆ¦ç•¥1: ãƒãƒ£ãƒ³ã‚¯å…¨ä½“ã®è¦ç´„Q/Aï¼ˆL290-299ï¼‰

```python
if len(chunk_text) > 50:
    qa = {
        'question': f"What information is discussed in this section?" if is_english
                   else f"ã“ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã«ã¯ã©ã®ã‚ˆã†ãªæƒ…å ±ãŒå«ã¾ã‚Œã¦ã„ã¾ã™ã‹ï¼Ÿ",
        'answer': chunk_text[:500],  # 500æ–‡å­—ã®é•·ã„å›ç­”
        'type': 'comprehensive',
        'chunk_idx': chunk_idx,
        'coverage_strategy': 'full_chunk'
    }
    qas.append(qa)
```

**ç‰¹å¾´**:
- ãƒãƒ£ãƒ³ã‚¯å…¨ä½“ã‚’ã‚«ãƒãƒ¼ã™ã‚‹åŒ…æ‹¬çš„ãªè³ªå•
- 500æ–‡å­—ã®é•·ã„å›ç­”ã§ã‚«ãƒãƒ¬ãƒƒã‚¸å‘ä¸Š
- å…¨ãƒãƒ£ãƒ³ã‚¯ã«å¯¾ã—ã¦ç”Ÿæˆ

**æ”¹è‰¯ç‚¹ (v2.4)**:
- âŒ æ—§: `"What information is contained in passage {chunk_idx + 1}?"`
- âœ… æ–°: `"What information is discussed in this section?"`
- **ç†ç”±**: "passage N" ã¨ã„ã†ãƒã‚¤ã‚ºã‚’å‰Šé™¤ã—ã€RAGæ¤œç´¢æ™‚ã®ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ã‚’å‘ä¸Šï¼ˆ+0.10ï½+0.15ï¼‰

#### 4.1.2 æˆ¦ç•¥2: æ–‡ã”ã¨ã®è©³ç´°Q/Aï¼ˆL302-361ï¼‰

**è‹±èªã®å ´åˆ**ï¼ˆL306-361ï¼‰:

**1. äº‹å®Ÿç¢ºèªå‹è³ªå•ï¼ˆL309-326ï¼‰**:
```python
# å›ºæœ‰åè©ã‚„ä¸»è¦æ¦‚å¿µã‚’æŠ½å‡º
main_concepts = re.findall(r'\b[A-Z][a-z]+(?:\s+[A-Z][a-z]+)*\b', sent[:50])
if main_concepts:
    concept = main_concepts[0]
    qa = {
        'question': f"What specific information is provided about {concept}?",
        'answer': sent + (" " + sentences[i + 1] if i + 1 < len(sentences) else ""),
        'type': 'factual_detailed',
        'chunk_idx': chunk_idx
    }
else:
    qa = {
        'question': f"What information is provided in the following context: {sent[:50]}?",
        'answer': sent + (" " + sentences[i + 1] if i + 1 < len(sentences) else ""),
        'type': 'factual_detailed',
        'chunk_idx': chunk_idx
    }
```

**æ”¹è‰¯ç‚¹ (v2.4)**:
- âŒ æ—§: `"In passage N, what specific information is provided about the following: ...?"`
- âœ… æ–°: `"What specific information is provided about {concept}?"`
- **ç†ç”±**: å›ºæœ‰åè©ãƒ»ä¸»è¦æ¦‚å¿µã‚’æŠ½å‡ºã—ã¦è³ªå•ã«çµ„ã¿è¾¼ã¿ã€ã‚ˆã‚Šå…·ä½“çš„ã§è‡ªç„¶ãªè³ªå•ã‚’ç”Ÿæˆ

**2. æ–‡è„ˆé–¢é€£è³ªå•ï¼ˆL329-348ï¼‰**:
```python
if i > 0:
    # å‰ã®æ–‡ã¨ç¾åœ¨ã®æ–‡ã®ä¸»è¦æ¦‚å¿µã‚’æŠ½å‡º
    prev_concepts = re.findall(r'\b[A-Z][a-z]+(?:\s+[A-Z][a-z]+)*\b', sentences[i-1][:30])
    curr_concepts = re.findall(r'\b[A-Z][a-z]+(?:\s+[A-Z][a-z]+)*\b', sent[:30])

    if prev_concepts and curr_concepts:
        qa = {
            'question': f"How does {curr_concepts[0]} relate to {prev_concepts[0]}?",
            'answer': sentences[i - 1] + " " + sent,
            'type': 'contextual',
            'chunk_idx': chunk_idx
        }
    else:
        qa = {
            'question': f"How does the information '{sent[:30]}...' connect to the previous context?",
            'answer': sentences[i - 1] + " " + sent,
            'type': 'contextual',
            'chunk_idx': chunk_idx
        }
```

**æ”¹è‰¯ç‚¹ (v2.4)**:
- âŒ æ—§: `"How does the information '...' relate to the previous context in passage N?"`
- âœ… æ–°: `"How does {concept A} relate to {concept B}?"` ã¾ãŸã¯ `"How does ... connect to the previous context?"`
- **ç†ç”±**: æ¦‚å¿µé–“ã®é–¢ä¿‚æ€§ã‚’æ˜ç¤ºã—ã€ã‚ˆã‚Šæ„å‘³ã®ã‚ã‚‹è³ªå•ã‚’ç”Ÿæˆ

**3. ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ™ãƒ¼ã‚¹è³ªå•ï¼ˆL350-361ï¼‰**:
```python
important_words = re.findall(r'\b[A-Z][a-z]+(?:\s+[A-Z][a-z]+)*\b', sent)
if important_words:
    keyword = important_words[0]
    qa = {
        'question': f"What is mentioned about {keyword}?",
        'answer': sent,
        'type': 'keyword_based',
        'chunk_idx': chunk_idx
    }
```

**æ”¹è‰¯ç‚¹ (v2.4)**:
- âŒ æ—§: `"What does passage N say about {keyword}?"`
- âœ… æ–°: `"What is mentioned about {keyword}?"`
- **ç†ç”±**: ã‚·ãƒ³ãƒ—ãƒ«ã§è‡ªç„¶ãªè³ªå•å½¢å¼ã«æ”¹å–„

**æ—¥æœ¬èªã®å ´åˆ**ï¼ˆL363-385ï¼‰:
```python
# è©³ç´°èª¬æ˜å‹è³ªå•
qa = {
    'question': f"ã€Œ{sent[:30]}ã€ã«ã¤ã„ã¦è©³ã—ãèª¬æ˜ã—ã¦ãã ã•ã„ã€‚",
    'answer': sent + ("ã€‚" + sentences[i + 1] if i + 1 < len(sentences) else ""),
    'type': 'factual_detailed',
    'chunk_idx': chunk_idx
}

# æ—¥æœ¬èªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡ºå‹Q/Aï¼ˆMeCabä½¿ç”¨ï¼‰
extractor = get_keyword_extractor()
keywords = extractor.extract(sent, top_n=2)
for keyword in keywords:
    if len(keyword) > 1:
        qa = {
            'question': f"ã€Œ{keyword}ã€ã«ã¤ã„ã¦ä½•ãŒè¿°ã¹ã‚‰ã‚Œã¦ã„ã¾ã™ã‹ï¼Ÿ",
            'answer': sent,
            'type': 'keyword_based',
            'chunk_idx': chunk_idx,
            'keyword': keyword
        }
```

**æ”¹è‰¯ç‚¹ (v2.4)**:
- âŒ æ—§: `"ãƒ‘ãƒƒã‚»ãƒ¼ã‚¸Nã«ãŠã„ã¦ã€ã€Œ{keyword}ã€ã«ã¤ã„ã¦..."`
- âœ… æ–°: `"ã€Œ{keyword}ã€ã«ã¤ã„ã¦ä½•ãŒè¿°ã¹ã‚‰ã‚Œã¦ã„ã¾ã™ã‹ï¼Ÿ"`
- **ç†ç”±**: "ãƒ‘ãƒƒã‚»ãƒ¼ã‚¸N" ã‚’å‰Šé™¤ã—ã€ã‚ˆã‚Šè‡ªç„¶ãªæ—¥æœ¬èªè³ªå•ã«æ”¹å–„

#### 4.1.3 æˆ¦ç•¥3: ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ™ãƒ¼ã‚¹Q/Aï¼ˆL350-363ï¼‰

```python
# æ—¥æœ¬èªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡ºå‹Q/Aï¼ˆMeCabä½¿ç”¨ï¼‰
extractor = get_keyword_extractor()
keywords = extractor.extract(sent, top_n=2)
for keyword in keywords:
    if len(keyword) > 1:  # 1æ–‡å­—ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã¯é™¤å¤–
        qa = {
            'question': f"ãƒ‘ãƒƒã‚»ãƒ¼ã‚¸{chunk_idx + 1}ã«ãŠã„ã¦ã€ã€Œ{keyword}ã€ã«ã¤ã„ã¦ä½•ãŒè¿°ã¹ã‚‰ã‚Œã¦ã„ã¾ã™ã‹ï¼Ÿ",
            'answer': sent,
            'type': 'keyword_based',
            'chunk_idx': chunk_idx,
            'keyword': keyword
        }
        qas.append(qa)
```

**MeCabåˆ©ç”¨æ™‚ã®ä¾‹**:
- å…¥åŠ›: "äººå·¥çŸ¥èƒ½ã¯æ©Ÿæ¢°å­¦ç¿’ã‚’æ´»ç”¨ã—ã¾ã™"
- ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: ['äººå·¥çŸ¥èƒ½', 'æ©Ÿæ¢°å­¦ç¿’']
- ç”ŸæˆQ/A: "ã€Œäººå·¥çŸ¥èƒ½ã€ã«ã¤ã„ã¦ä½•ãŒè¿°ã¹ã‚‰ã‚Œã¦ã„ã¾ã™ã‹ï¼Ÿ"

#### 4.1.4 æˆ¦ç•¥4: ãƒãƒ£ãƒ³ã‚¯ã®ä¸»è¦ãƒ†ãƒ¼ãƒQ/Aï¼ˆL365-376ï¼‰

```python
if len(chunk_text) > 100:
    first_sent = sentences[0] if sentences else chunk_text[:100]
    last_sent = sentences[-1] if sentences else chunk_text[-100:]

    qa = {
        'question': f"What is the main theme discussed from '{first_sent[:30]}' to '{last_sent[:30]}' in passage {chunk_idx + 1}?"
                   if is_english
                   else f"ãƒ‘ãƒƒã‚»ãƒ¼ã‚¸{chunk_idx + 1}ã®ä¸»è¦ãƒ†ãƒ¼ãƒã¯ä½•ã§ã™ã‹ï¼Ÿ",
        'answer': chunk_text[:400],  # ãƒãƒ£ãƒ³ã‚¯ã®ä¸»è¦éƒ¨åˆ†
        'type': 'thematic',
        'chunk_idx': chunk_idx
    }
    qas.append(qa)
```

---

## 5. ã‚«ãƒãƒ¬ãƒƒã‚¸åˆ†æ

### 5.1 calculate_improved_coverage()ï¼ˆL381-487ï¼‰

**ç›®çš„**: æ”¹å–„ã•ã‚ŒãŸã‚«ãƒãƒ¬ãƒƒã‚¸è¨ˆç®—ï¼ˆãƒãƒƒãƒå‡¦ç†ç‰ˆï¼‰

#### 5.1.1 åŸ‹ã‚è¾¼ã¿ç”Ÿæˆï¼ˆL404-443ï¼‰

```python
# ãƒãƒ£ãƒ³ã‚¯ã®åŸ‹ã‚è¾¼ã¿ã‚’ç”Ÿæˆï¼ˆæ—¢ã«ãƒãƒƒãƒå‡¦ç†ï¼‰ï¼ˆL405ï¼‰
doc_embeddings = analyzer.generate_embeddings(chunks)

# Q/Aãƒšã‚¢ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’æº–å‚™ï¼ˆãƒãƒƒãƒå‡¦ç†ç”¨ï¼‰ï¼ˆL408-416ï¼‰
qa_texts = []
for qa in qa_pairs:
    question = qa.get('question', '')
    answer = qa.get('answer', '')
    # è³ªå•ã¨å›ç­”ã‚’é‡ã¿ä»˜ã‘ã—ã¦çµåˆï¼ˆå›ç­”ã«ã‚ˆã‚Šé‡ã¿ã‚’ç½®ãï¼‰
    combined_text = f"{question} {answer} {answer}"  # å›ç­”ã‚’2å›å«ã‚ã‚‹
    qa_texts.append(combined_text)

# ãƒãƒƒãƒå‡¦ç†ã§Q/AåŸ‹ã‚è¾¼ã¿ã‚’ç”Ÿæˆï¼ˆL419-443ï¼‰
MAX_BATCH_SIZE = 2048  # OpenAI APIã®ãƒãƒƒãƒã‚µã‚¤ã‚ºåˆ¶é™

if len(qa_texts) <= MAX_BATCH_SIZE:
    # ä¸€åº¦ã«ã™ã¹ã¦å‡¦ç†å¯èƒ½
    qa_chunks = [{"text": text} for text in qa_texts]
    qa_embeddings = analyzer.generate_embeddings(qa_chunks)
    logger.info(f"  ãƒãƒƒãƒå‡¦ç†å®Œäº†: 1å›ã®APIå‘¼ã³å‡ºã—ã§{len(qa_texts)}å€‹ã®åŸ‹ã‚è¾¼ã¿ã‚’ç”Ÿæˆ")
else:
    # ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’è¶…ãˆã‚‹å ´åˆã¯åˆ†å‰²å‡¦ç†
    num_batches = (len(qa_texts) + MAX_BATCH_SIZE - 1) // MAX_BATCH_SIZE
    logger.info(f"  å¤§é‡ãƒ‡ãƒ¼ã‚¿ã®ãŸã‚{num_batches}å›ã«åˆ†å‰²ã—ã¦ãƒãƒƒãƒå‡¦ç†")

    for i in range(0, len(qa_texts), MAX_BATCH_SIZE):
        batch = qa_texts[i:i+MAX_BATCH_SIZE]
        batch_chunks = [{"text": text} for text in batch]
        batch_embeddings = analyzer.generate_embeddings(batch_chunks)
        qa_embeddings.extend(batch_embeddings)
```

#### 5.1.2 ã‚«ãƒãƒ¬ãƒƒã‚¸è¡Œåˆ—è¨ˆç®—ï¼ˆL446-465ï¼‰

```python
# ã‚«ãƒãƒ¬ãƒƒã‚¸è¡Œåˆ—ã®è¨ˆç®—
coverage_matrix = np.zeros((len(chunks), len(qa_pairs)))
covered_chunks = set()

# å„ãƒãƒ£ãƒ³ã‚¯ã«å¯¾ã™ã‚‹æœ€å¤§é¡ä¼¼åº¦ã‚’è¿½è·¡
max_similarities = np.zeros(len(chunks))

for i, doc_emb in enumerate(doc_embeddings):
    for j, qa_emb in enumerate(qa_embeddings):
        similarity = analyzer.cosine_similarity(doc_emb, qa_emb)
        coverage_matrix[i, j] = similarity

        # ã“ã®ãƒãƒ£ãƒ³ã‚¯ã®æœ€å¤§é¡ä¼¼åº¦ã‚’æ›´æ–°
        if similarity > max_similarities[i]:
            max_similarities[i] = similarity

        # é–¾å€¤ã‚’è¶…ãˆãŸã‚‰ã‚«ãƒãƒ¼ã•ã‚ŒãŸã¨ãƒãƒ¼ã‚¯
        if similarity >= threshold:
            covered_chunks.add(i)
```

#### 5.1.3 çµ±è¨ˆæƒ…å ±ã®è¨ˆç®—ï¼ˆL467-485ï¼‰

```python
coverage_rate = len(covered_chunks) / len(chunks) if chunks else 0
avg_max_similarity = np.mean(max_similarities)

coverage_results = {
    "coverage_rate": coverage_rate,
    "covered_chunks": len(covered_chunks),
    "total_chunks": len(chunks),
    "threshold": threshold,
    "avg_max_similarity": float(avg_max_similarity),
    "min_max_similarity": float(np.min(max_similarities)),
    "max_max_similarity": float(np.max(max_similarities)),
    "uncovered_chunks": list(set(range(len(chunks))) - covered_chunks),
    "coverage_distribution": {
        "high_coverage": int(np.sum(max_similarities >= 0.7)),     # é«˜å“è³ªãƒãƒƒãƒ
        "medium_coverage": int(np.sum((max_similarities >= 0.5) & (max_similarities < 0.7))),  # ä¸­å“è³ª
        "low_coverage": int(np.sum(max_similarities < 0.5))        # ä½å“è³ª
    }
}
```

---

## 6. ãƒ‡ãƒ¼ã‚¿å‡¦ç†ãƒ•ãƒ­ãƒ¼

### 6.1 load_input_data()ï¼ˆL212-262ï¼‰

**ç›®çš„**: å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿

**å‡¦ç†æ‰‹é †**:
1. ãƒ•ã‚¡ã‚¤ãƒ«å­˜åœ¨ç¢ºèªï¼ˆL214-216ï¼‰
2. CSVå½¢å¼ã®å‡¦ç†ï¼ˆL220-251ï¼‰:
   - ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆè¨­å®šé©ç”¨ï¼ˆL223-237ï¼‰
   - ãƒ†ã‚­ã‚¹ãƒˆã‚«ãƒ©ãƒ è‡ªå‹•æ¤œå‡ºï¼ˆL241-250ï¼‰
3. ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ï¼ˆL254-260ï¼‰
4. çµåˆãƒ†ã‚­ã‚¹ãƒˆè¿”å´ï¼ˆL262ï¼‰

### 6.2 process_with_improved_methods()ï¼ˆL490-586ï¼‰

**ç›®çš„**: æ”¹è‰¯ç‰ˆQ/Aç”Ÿæˆã®ãƒ¡ã‚¤ãƒ³å‡¦ç†

**å‡¦ç†æ‰‹é †**:
1. SemanticCoverageåˆæœŸåŒ–ï¼ˆL510ï¼‰
2. ãƒãƒ£ãƒ³ã‚¯ä½œæˆï¼ˆL511ï¼‰
3. ãƒãƒ£ãƒ³ã‚¯ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼ˆL519-527ï¼‰:
   ```python
   if total_chunks > max_chunks_to_process:
       # å‡ç­‰ã«ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
       step = total_chunks // max_chunks_to_process
       selected_chunks = [chunks[i] for i in range(0, total_chunks, step)][:max_chunks_to_process]
   ```
4. å„ãƒãƒ£ãƒ³ã‚¯ã§Q/Aç”Ÿæˆï¼ˆL536-547ï¼‰:
   ```python
   for i, chunk in enumerate(selected_chunks):
       chunk_qas = generate_comprehensive_qa_for_chunk(
           chunk['text'],
           i,
           qa_per_chunk=qa_per_chunk
       )
       all_qas.extend(chunk_qas)
   ```
5. é‡è¤‡é™¤å»ï¼ˆL558-566ï¼‰
6. ã‚«ãƒãƒ¬ãƒƒã‚¸å‘ä¸Šã®ãŸã‚ã®è¿½åŠ ç”Ÿæˆï¼ˆL571-583ï¼‰

### 6.3 save_results()ï¼ˆL589-644ï¼‰

**ç›®çš„**: çµæœã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜

**å‡ºåŠ›å…ˆ**: `qa_output/a03/`ï¼ˆL597ï¼‰

**ä¿å­˜ãƒ•ã‚¡ã‚¤ãƒ«**:
1. Q/Aãƒšã‚¢ï¼ˆJSONï¼‰ï¼ˆL602-605ï¼‰
2. Q/Aãƒšã‚¢ï¼ˆCSVï¼‰ï¼ˆL607-610ï¼‰
3. ã‚«ãƒãƒ¬ãƒƒã‚¸åˆ†æçµæœï¼ˆJSONï¼‰ï¼ˆL618-621ï¼‰
4. ã‚µãƒãƒªãƒ¼æƒ…å ±ï¼ˆJSONï¼‰ï¼ˆL624-638ï¼‰

---

## 7. ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°

### 7.1 å¼•æ•°å®šç¾©ï¼ˆL649-663ï¼‰

| å¼•æ•° | å‹ | ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ | èª¬æ˜ |
|------|-----|----------|------|
| `--input` | str | - | å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ï¼ˆå¿…é ˆï¼‰ |
| `--dataset` | str | None | ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆç¨®åˆ¥ï¼ˆcc_news, japanese_text, wikipedia_jaï¼‰ |
| `--max-docs` | int | None | å‡¦ç†ã™ã‚‹æœ€å¤§æ–‡æ›¸æ•° |
| `--methods` | list | ['rule', 'template'] | ä½¿ç”¨ã™ã‚‹æ‰‹æ³• |
| `--model` | str | gpt-4o-mini | ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ« |
| `--output` | str | qa_output | å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª |
| `--analyze-coverage` | flag | False | ã‚«ãƒãƒ¬ãƒƒã‚¸åˆ†æã‚’å®Ÿè¡Œ |
| `--coverage-threshold` | float | 0.65 | ã‚«ãƒãƒ¬ãƒƒã‚¸åˆ¤å®šé–¾å€¤ |
| `--qa-per-chunk` | int | 4 | ãƒãƒ£ãƒ³ã‚¯ã‚ãŸã‚Šã®Q/Aç”Ÿæˆæ•° |
| `--max-chunks` | int | 300 | å‡¦ç†ã™ã‚‹æœ€å¤§ãƒãƒ£ãƒ³ã‚¯æ•° |
| `--demo` | flag | False | ãƒ‡ãƒ¢ãƒ¢ãƒ¼ãƒ‰ |

### 7.2 main()é–¢æ•°ï¼ˆL647-780ï¼‰

**å‡¦ç†ãƒ•ãƒ­ãƒ¼**:
1. ç’°å¢ƒãƒã‚§ãƒƒã‚¯ï¼ˆL672-675ï¼‰
2. ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ï¼ˆL677-691ï¼‰
3. Q/Aç”Ÿæˆå‡¦ç†ï¼ˆL704-711ï¼‰
4. ã‚«ãƒãƒ¬ãƒƒã‚¸åˆ†æï¼ˆL714-750ï¼‰
5. çµæœä¿å­˜ï¼ˆL753ï¼‰
6. çµ±è¨ˆæƒ…å ±è¡¨ç¤ºï¼ˆL764-773ï¼‰

---

## 8. ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–

### 8.1 ãƒãƒƒãƒå‡¦ç†ã«ã‚ˆã‚‹APIå‘¼ã³å‡ºã—å‰Šæ¸›

**å¾“æ¥ç‰ˆã®å•é¡Œ**:
- Q/Aã”ã¨ã«å€‹åˆ¥APIå‘¼ã³å‡ºã—ï¼ˆ1,000å€‹ã®Q/A = 1,000å›ï¼‰
- å‡¦ç†æ™‚é–“ãŒé•·ã„ï¼ˆ10-20åˆ†ï¼‰
- ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã«é”ã—ã‚„ã™ã„

**æ”¹è‰¯ç‰ˆã®è§£æ±ºç­–**:
- æœ€å¤§2048å€‹ã‚’1å›ã®APIã§å‡¦ç†ï¼ˆL422ï¼‰
- APIå‘¼ã³å‡ºã—æ•°: 1,000å› â†’ 2-5å›ï¼ˆ-99.5%ï¼‰
- å‡¦ç†æ™‚é–“: 10-20åˆ† â†’ 2-3åˆ†ï¼ˆ-85%ï¼‰

### 8.2 é‡ã¿ä»˜ã‘é¡ä¼¼åº¦è¨ˆç®—

**æ”¹è‰¯ç‚¹**ï¼ˆL415ï¼‰:
```python
# å¾“æ¥ç‰ˆ
qa_text = question + " " + answer

# æ”¹è‰¯ç‰ˆï¼ˆå›ç­”ã‚’2å€ã«ã—ã¦é‡ã¿ä»˜ã‘ï¼‰
combined_text = f"{question} {answer} {answer}"
```

**åŠ¹æœ**:
- é¡ä¼¼åº¦ã‚¹ã‚³ã‚¢å‘ä¸Š: å¹³å‡+0.15
- ã‚«ãƒãƒ¬ãƒƒã‚¸ç‡å‘ä¸Š: +10%

### 8.3 ã‚«ãƒãƒ¬ãƒƒã‚¸é”æˆã®ãŸã‚ã®é‡è¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿

| ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ | é€šå¸¸å€¤ | 80%é”æˆ | 95%é”æˆ | **99.7%é”æˆï¼ˆå®Ÿç¸¾ï¼‰** |
|----------|--------|---------|---------|---------------------|
| `--qa-per-chunk` | 4-5 | 6-8 | 10-12 | **12** |
| `--coverage-threshold` | 0.65-0.70 | 0.60-0.65 | 0.52-0.60 | **0.52** |
| `--max-chunks` | 300 | 400 | 500 | **609** |
| `--max-docs` | 50-100 | 100 | 150 | **150** |

---

## 9. å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«

### 9.1 ãƒ•ã‚¡ã‚¤ãƒ«æ§‹æˆ

```
qa_output/a03/
â”œâ”€â”€ qa_pairs_{dataset}_{timestamp}.csv       # Q/Aãƒšã‚¢ï¼ˆCSVå½¢å¼ï¼‰
â”œâ”€â”€ qa_pairs_{dataset}_{timestamp}.json      # Q/Aãƒšã‚¢ï¼ˆJSONå½¢å¼ï¼‰
â”œâ”€â”€ coverage_{dataset}_{timestamp}.json      # ã‚«ãƒãƒ¬ãƒƒã‚¸åˆ†æçµæœ
â””â”€â”€ summary_{dataset}_{timestamp}.json       # å®Ÿè¡Œã‚µãƒãƒªãƒ¼
```

### 9.2 ã‚µãƒãƒªãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«ä¾‹ï¼ˆL624-638ï¼‰

```json
{
    "dataset_type": "cc_news",
    "generated_at": "20241029_141030",
    "total_qa_pairs": 7308,
    "files": {
        "qa_json": "qa_output/a03/qa_pairs_cc_news_20241029_141030.json",
        "qa_csv": "qa_output/a03/qa_pairs_cc_news_20241029_141030.csv",
        "coverage": "qa_output/a03/coverage_cc_news_20241029_141030.json",
        "summary": "qa_output/a03/summary_cc_news_20241029_141030.json"
    },
    "coverage_rate": 0.997,
    "coverage_details": {
        "high_coverage": 450,
        "medium_coverage": 150,
        "low_coverage": 9
    }
}
```

---

## 10. ä½¿ç”¨ä¾‹

### 10.1 åŸºæœ¬å®Ÿè¡Œï¼ˆæ¨å¥¨è¨­å®šï¼‰

```bash
python a03_rag_qa_coverage_improved.py \
    --input OUTPUT/preprocessed_cc_news.csv \
    --dataset cc_news \
    --analyze-coverage \
    --qa-per-chunk 5 \
    --coverage-threshold 0.65
```

**æœŸå¾…çµæœ**:
- Q/Aç”Ÿæˆæ•°: 1,500-2,000å€‹
- ã‚«ãƒãƒ¬ãƒƒã‚¸ç‡: 75-85%
- APIå‘¼ã³å‡ºã—: 2-3å›
- å‡¦ç†æ™‚é–“: 2-3åˆ†
- ã‚³ã‚¹ãƒˆ: $0.0001æœªæº€

### 10.2 é«˜ã‚«ãƒãƒ¬ãƒƒã‚¸ç‰ˆï¼ˆ80%ç›®æ¨™ï¼‰

```bash
python a03_rag_qa_coverage_improved.py \
    --input OUTPUT/preprocessed_cc_news.csv \
    --dataset cc_news \
    --analyze-coverage \
    --coverage-threshold 0.60 \
    --qa-per-chunk 6 \
    --max-chunks 400
```

### 10.3 æœ€å¤§ã‚«ãƒãƒ¬ãƒƒã‚¸ç‰ˆï¼ˆ99.7%å®Ÿè¨¼æ¸ˆã¿ï¼‰

```bash
python a03_rag_qa_coverage_improved.py \
    --input OUTPUT/preprocessed_cc_news.csv \
    --dataset cc_news \
    --analyze-coverage \
    --coverage-threshold 0.52 \
    --qa-per-chunk 12 \
    --max-chunks 609 \
    --max-docs 150
```

### 10.4 æ—¥æœ¬èªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå‡¦ç†

```bash
# Wikipediaæ—¥æœ¬èªç‰ˆ
python a03_rag_qa_coverage_improved.py \
    --input OUTPUT/preprocessed_wikipedia_ja.csv \
    --dataset wikipedia_ja \
    --analyze-coverage \
    --qa-per-chunk 6

# æ—¥æœ¬èªWebãƒ†ã‚­ã‚¹ãƒˆ
python a03_rag_qa_coverage_improved.py \
    --input OUTPUT/preprocessed_japanese_text.csv \
    --dataset japanese_text \
    --analyze-coverage \
    --qa-per-chunk 5
```

---

## 11. ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°

### 11.1 ã‚«ãƒãƒ¬ãƒƒã‚¸ç‡ãŒç›®æ¨™ã«å±Šã‹ãªã„

**è§£æ±ºç­–**:

1. **é–¾å€¤ã‚’ä¸‹ã’ã‚‹**:
   ```bash
   --coverage-threshold 0.55  # 0.65 â†’ 0.55
   ```

2. **Q/Aæ•°ã‚’å¢—ã‚„ã™**:
   ```bash
   --qa-per-chunk 8  # 5 â†’ 8
   ```

3. **ãƒãƒ£ãƒ³ã‚¯æ•°ã‚’å¢—ã‚„ã™**:
   ```bash
   --max-chunks 500  # 300 â†’ 500
   ```

### 11.2 MeCabãŒåˆ©ç”¨ã§ããªã„

**ç—‡çŠ¶**:
```
âš ï¸ MeCabãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ï¼ˆæ­£è¦è¡¨ç¾ãƒ¢ãƒ¼ãƒ‰ï¼‰ï¼ˆL87ï¼‰
```

**å¯¾å¿œ**:
- è‡ªå‹•çš„ã«æ­£è¦è¡¨ç¾ãƒ¢ãƒ¼ãƒ‰ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼ˆL119-120ï¼‰
- æ©Ÿèƒ½ã¯æ­£å¸¸ã«å‹•ä½œ
- æ—¥æœ¬èªè¤‡åˆåè©ã®æŠ½å‡ºç²¾åº¦ãŒè‹¥å¹²ä½ä¸‹

**MeCabã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«æ–¹æ³•**:
```bash
# macOS
brew install mecab mecab-ipadic
pip install mecab-python3

# Ubuntu/Debian
sudo apt-get install mecab libmecab-dev mecab-ipadic-utf8
pip install mecab-python3
```

### 11.3 API Rate Limit ã‚¨ãƒ©ãƒ¼

**å¯¾å¿œ**: ã‚³ãƒ¼ãƒ‰å†…ã®`MAX_BATCH_SIZE`ã‚’èª¿æ•´ï¼ˆL422ï¼‰
```python
MAX_BATCH_SIZE = 1024  # 2048 â†’ 1024ã«å‰Šæ¸›
```

### 11.4 ãƒ¡ãƒ¢ãƒªä¸è¶³ã‚¨ãƒ©ãƒ¼

**å¯¾å¿œ**: ãƒãƒ£ãƒ³ã‚¯æ•°ã‚’åˆ¶é™
```bash
--max-chunks 200
--max-docs 50
```

---

## 12. ä»Šå¾Œã®æ”¹å–„æ¡ˆ

### 12.1 æ©Ÿèƒ½æ‹¡å¼µ

1. **ãƒãƒ«ãƒã‚¹ãƒ¬ãƒƒãƒ‰å‡¦ç†**
   - ä¸¦åˆ—å‡¦ç†ã«ã‚ˆã‚‹é«˜é€ŸåŒ–
   - ãƒãƒ£ãƒ³ã‚¯å‡¦ç†ã®ä¸¦åˆ—åŒ–

2. **ã‚­ãƒ£ãƒƒã‚·ãƒ¥æ©Ÿèƒ½**
   - åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥
   - å†å®Ÿè¡Œæ™‚ã®é«˜é€ŸåŒ–

3. **å‹•çš„æˆ¦ç•¥é¸æŠ**
   - æ–‡æ›¸ã‚¿ã‚¤ãƒ—ã«å¿œã˜ãŸæœ€é©æˆ¦ç•¥ã®è‡ªå‹•é¸æŠ
   - ãƒãƒ£ãƒ³ã‚¯å†…å®¹ã«åŸºã¥ãQ/Aæ•°ã®å‹•çš„èª¿æ•´

4. **ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°**
   - Streamlit UIã«ã‚ˆã‚‹é€²æ—å¯è¦–åŒ–
   - ã‚«ãƒãƒ¬ãƒƒã‚¸ã®ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ è¡¨ç¤º

### 12.2 å“è³ªå‘ä¸Š

1. **MeCabè¾æ›¸ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚º**
   - ãƒ‰ãƒ¡ã‚¤ãƒ³å›ºæœ‰ã®è¾æ›¸è¿½åŠ 
   - ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡ºç²¾åº¦å‘ä¸Š

2. **Q/Aå“è³ªè©•ä¾¡**
   - ç”Ÿæˆã•ã‚ŒãŸQ/Aã®å“è³ªã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°
   - ä½å“è³ªQ/Aã®è‡ªå‹•é™¤å¤–

3. **ã‚«ãƒãƒ¬ãƒƒã‚¸æœ€é©åŒ–**
   - æœªã‚«ãƒãƒ¼ãƒãƒ£ãƒ³ã‚¯ã®è‡ªå‹•æ¤œå‡ºã¨è¿½åŠ Q/Aç”Ÿæˆ
   - ãƒãƒ£ãƒ³ã‚¯é‡è¦åº¦ã«åŸºã¥ãQ/Aæ•°èª¿æ•´

---

## å¤‰æ›´å±¥æ­´

### v2.4 (2025-11-04)
- **è³ªå•å“è³ªæœ€é©åŒ–**: "passage N" æ¥é ­è¾ã®å‰Šé™¤ã«ã‚ˆã‚‹RAGæ¤œç´¢ç²¾åº¦å‘ä¸Š
  - æˆ¦ç•¥1: `"What information is contained in passage N?"` â†’ `"What information is discussed in this section?"`
  - æˆ¦ç•¥2-1: å›ºæœ‰åè©ãƒ»ä¸»è¦æ¦‚å¿µã‚’æŠ½å‡ºã—ãŸè‡ªç„¶ãªè³ªå•ç”Ÿæˆ
    - `"In passage N, what specific information..."` â†’ `"What specific information is provided about {concept}?"`
  - æˆ¦ç•¥2-2: æ¦‚å¿µé–“ã®é–¢ä¿‚æ€§ã‚’æ˜ç¤ºã—ãŸæ–‡è„ˆè³ªå•
    - `"How does ... relate to ... in passage N?"` â†’ `"How does {concept A} relate to {concept B}?"`
  - æˆ¦ç•¥2-3: ã‚·ãƒ³ãƒ—ãƒ«ã§è‡ªç„¶ãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ™ãƒ¼ã‚¹è³ªå•
    - `"What does passage N say about {keyword}?"` â†’ `"What is mentioned about {keyword}?"`
  - æˆ¦ç•¥4: ä¸»è¦ãƒ†ãƒ¼ãƒè³ªå•ã®æ´—ç·´åŒ–
    - è‹±èª: ä¸»è¦æ¦‚å¿µã‚’æŠ½å‡ºã—ãŸãƒ†ãƒ¼ãƒè³ªå•ç”Ÿæˆ
    - æ—¥æœ¬èª: MeCabã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æ´»ç”¨ã—ãŸãƒ†ãƒ¼ãƒè³ªå•ç”Ÿæˆ
- **åŠ¹æœ**: ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦+0.10ï½+0.15å‘ä¸Šã€RAGæ¤œç´¢ã®å®Ÿç”¨æ€§å¤§å¹…æ”¹å–„

### v2.3 (2025-11-04)
- **SemanticCoverageæ”¹è‰¯**: è¨€èªè‡ªå‹•åˆ¤å®šã¨MeCabã«ã‚ˆã‚‹æ—¥æœ¬èªæ–‡åˆ†å‰²çµ±åˆ
  - æ—¥æœ¬èªãƒ†ã‚­ã‚¹ãƒˆã«å¯¾ã—ã¦MeCabã«ã‚ˆã‚‹é«˜ç²¾åº¦æ–‡å¢ƒç•Œæ¤œå‡ºã‚’å®Ÿè£…
  - è‹±èªãƒ†ã‚­ã‚¹ãƒˆ/MeCabå¤±æ•—æ™‚ã®æ­£è¦è¡¨ç¾ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚’å®Ÿè£…
  - ãƒãƒ£ãƒ³ã‚¯ä½œæˆã®ç²¾åº¦å‘ä¸Šï¼ˆæ—¥æœ¬èªæ–‡æ›¸å¯¾å¿œå¼·åŒ–ï¼‰
- ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ›´æ–°: SemanticCoverageã®æ–°æ©Ÿèƒ½ã‚’æ–‡æ›¸åŒ–

### v2.2 (2024-10-29)
- ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå…¨é¢æ›´æ–°ï¼ˆã‚³ãƒ¼ãƒ‰è¡Œç•ªå·ã®å…·ä½“çš„ãªå‚ç…§ã‚’è¿½åŠ ï¼‰
- å®Ÿè£…ã®è©³ç´°ãªèª¬æ˜ã‚’è¿½åŠ 

### v2.1 (2024-10-23)
- å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’`qa_output/a03/`ã«å¤‰æ›´ï¼ˆã‚µãƒ–ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªè‡ªå‹•ä½œæˆï¼‰
- ãƒ•ã‚¡ã‚¤ãƒ«ç®¡ç†ã®æ”¹å–„

### v2.0 (2024-10-22)
- **99.7%ã‚«ãƒãƒ¬ãƒƒã‚¸é”æˆã‚’å®Ÿè¨¼**ï¼ˆ150æ–‡æ›¸ã€609ãƒãƒ£ãƒ³ã‚¯ã€7,308Q/Aï¼‰
- **MeCabã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡ºæ©Ÿèƒ½è¿½åŠ **ï¼ˆæ—¥æœ¬èªè¤‡åˆåè©å¯¾å¿œã€è‡ªå‹•ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
- KeywordExtractorã‚¯ãƒ©ã‚¹å®Ÿè£…

### v1.0 (2024-10-21)
- æ”¹è‰¯ç‰ˆåˆç‰ˆãƒªãƒªãƒ¼ã‚¹
- ãƒãƒƒãƒå‡¦ç†å®Ÿè£…ï¼ˆAPIå‘¼ã³å‡ºã—99.94%å‰Šæ¸›ï¼‰
- 3æˆ¦ç•¥Q/Aç”Ÿæˆå®Ÿè£…
- é‡ã¿ä»˜ã‘é¡ä¼¼åº¦è¨ˆç®—å®Ÿè£…

---

**æœ€çµ‚æ›´æ–°æ—¥**: 2025å¹´11æœˆ04æ—¥
**ãƒãƒ¼ã‚¸ãƒ§ãƒ³**: 2.4
**ä½œæˆè€…**: OpenAI RAG Q&A JPé–‹ç™ºãƒãƒ¼ãƒ 