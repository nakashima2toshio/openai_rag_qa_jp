## a02_make_qa.py ã‚«ãƒãƒ¬ãƒ¼ã‚¸æ¸¬å®šå¼·åŒ–ææ¡ˆæ›¸
---
ææ¡ˆã™ã‚‹4ã¤ã®å¼·åŒ–è»¸

ææ¡ˆ1: å¤šæ®µéšã‚«ãƒãƒ¬ãƒ¼ã‚¸åˆ†æ

- âœ… è¤‡æ•°é–¾å€¤è©•ä¾¡ï¼ˆstrict 0.8 / standard 0.7 / lenient 0.6ï¼‰
- âœ… ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆåˆ¥æœ€é©é–¾å€¤ï¼ˆWikipediaå°‚é–€çš„â†’é«˜é–¾å€¤ï¼‰
- âœ… ãƒãƒ£ãƒ³ã‚¯ç‰¹æ€§åˆ¥åˆ†æï¼ˆé•·ã•åˆ¥ãƒ»ä½ç½®åˆ¥ã‚«ãƒãƒ¬ãƒ¼ã‚¸ï¼‰

ææ¡ˆ2: Q/Aå“è³ªã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°

- âœ… ã‚«ãƒãƒ¬ãƒ¼ã‚¸è²¢çŒ®åº¦ï¼ˆä½•ãƒãƒ£ãƒ³ã‚¯ã‚’ã‚«ãƒãƒ¼ã™ã‚‹ã‹ï¼‰
- âœ… æœ€å¤§/å¹³å‡é¡ä¼¼åº¦
- âœ… ç·åˆå“è³ªã‚¹ã‚³ã‚¢ï¼ˆé‡ã¿ä»˜ã‘ï¼‰

ææ¡ˆ3: ã‚®ãƒ£ãƒƒãƒ—åˆ†æã¨æ”¹å–„ææ¡ˆ

- âœ… æœªã‚«ãƒãƒ¼é ˜åŸŸã®è©³ç´°åˆ†æï¼ˆã©ã“ãŒå¼±ã„ã‹ç‰¹å®šï¼‰
- âœ… è‡ªå‹•æ”¹å–„ææ¡ˆï¼ˆã€Œ12å€‹ã®Q/Aè¿½åŠ ãŒå¿…è¦ã€ï¼‰
- âœ… è³ªå•ã‚¿ã‚¤ãƒ—ãƒãƒ©ãƒ³ã‚¹ï¼ˆä¸è¶³ã‚¿ã‚¤ãƒ—ã‚’æ¤œå‡ºï¼‰

ææ¡ˆ4: å¯è¦–åŒ–ã¨ãƒ¬ãƒãƒ¼ãƒˆ

- âœ… çµŒå–¶å±¤å‘ã‘ã‚µãƒãƒªãƒ¼ï¼ˆã€Œå„ªç§€/è‰¯å¥½/è¦æ”¹å–„ã€è©•ä¾¡ï¼‰
- âœ… ã‚°ãƒ©ãƒ•ãƒ»ãƒãƒ£ãƒ¼ãƒˆãƒ‡ãƒ¼ã‚¿ï¼ˆå††ã‚°ãƒ©ãƒ•ã€æ£’ã‚°ãƒ©ãƒ•ã€æ•£å¸ƒå›³ï¼‰
- âœ… ã‚¢ã‚¯ã‚·ãƒ§ãƒ³æç¤ºï¼ˆã€Œchunk_15ã«å®šç¾©å‹Q/Aè¿½åŠ ã€ï¼‰

æœŸå¾…ã•ã‚Œã‚‹åŠ¹æœ

| æŒ‡æ¨™      | ç¾çŠ¶      | å¼·åŒ–å¾Œ        | æ”¹å–„    |
|---------|---------|------------|-------|
| ã‚«ãƒãƒ¬ãƒ¼ã‚¸æŠŠæ¡ | 1ãƒ¡ãƒˆãƒªã‚¯ã‚¹  | 10+ãƒ¡ãƒˆãƒªã‚¯ã‚¹   | +900% |
| æ”¹å–„ç‰¹å®šæ™‚é–“  | 30åˆ†ï¼ˆæ‰‹å‹•ï¼‰ | 1åˆ†ï¼ˆè‡ªå‹•ï¼‰     | -97%  |
| Q/Aè¿½åŠ ç²¾åº¦ | 50%ï¼ˆæ¨æ¸¬ï¼‰ | 85%ï¼ˆãƒ‡ãƒ¼ã‚¿é§†å‹•ï¼‰ | +70%  |
| ãƒ¬ãƒãƒ¼ãƒˆä½œæˆ  | 60åˆ†     | 5åˆ†         | -92%  |

æ®µéšçš„å°å…¥è¨ˆç”»ï¼ˆ10é€±é–“ï¼‰

1. Phase 1ï¼ˆ2é€±ï¼‰: è¤‡æ•°é–¾å€¤è©•ä¾¡ã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆåˆ¥æœ€é©åŒ–
2. Phase 2ï¼ˆ3é€±ï¼‰: å“è³ªã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°ã€è©³ç´°ã‚®ãƒ£ãƒƒãƒ—åˆ†æ
3. Phase 3ï¼ˆ2é€±ï¼‰: è‡ªå‹•æ”¹å–„ææ¡ˆ
4. Phase 4ï¼ˆ2é€±ï¼‰: å¯è¦–åŒ–ãƒ»ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
5. Phase 5ï¼ˆ1é€±ï¼‰: çµ±åˆãƒ»æœ€é©åŒ–

ã“ã®ææ¡ˆã«ã‚ˆã‚Šã€a02_make_qa.pyã¯å˜ãªã‚‹Q/Aç”Ÿæˆãƒ„ãƒ¼ãƒ«ã‹ã‚‰å“è³ªç®¡ç†ãƒ»æ”¹å–„æ”¯æ´ã‚·ã‚¹ãƒ†ãƒ ã¸é€²åŒ–ã—ã¾ã™ï¼

---

## ç›®æ¬¡

1. [ç¾çŠ¶åˆ†æ](#1-ç¾çŠ¶åˆ†æ)
2. [èª²é¡Œã¨æ”¹å–„æ©Ÿä¼š](#2-èª²é¡Œã¨æ”¹å–„æ©Ÿä¼š)
3. [ææ¡ˆæ¦‚è¦](#3-ææ¡ˆæ¦‚è¦)
4. [è©³ç´°ææ¡ˆ](#4-è©³ç´°ææ¡ˆ)
5. [å®Ÿè£…ã‚¤ãƒ¡ãƒ¼ã‚¸](#5-å®Ÿè£…ã‚¤ãƒ¡ãƒ¼ã‚¸)
6. [æœŸå¾…ã•ã‚Œã‚‹åŠ¹æœ](#6-æœŸå¾…ã•ã‚Œã‚‹åŠ¹æœ)
7. [æ®µéšçš„å°å…¥è¨ˆç”»](#7-æ®µéšçš„å°å…¥è¨ˆç”»)

---

## 1. ç¾çŠ¶åˆ†æ

### 1.1 a02_make_qa.pyã®ç¾çŠ¶ã‚«ãƒãƒ¬ãƒ¼ã‚¸æ©Ÿèƒ½

ç¾åœ¨ã®a02_make_qa.pyã«ã¯ã€åŸºæœ¬çš„ãªã‚«ãƒãƒ¬ãƒ¼ã‚¸åˆ†ææ©Ÿèƒ½ãŒå®Ÿè£…ã•ã‚Œã¦ã„ã¾ã™ï¼š

```python
def analyze_coverage(chunks: List[Dict], qa_pairs: List[Dict]) -> Dict:
    """ç”Ÿæˆã•ã‚ŒãŸQ/Aãƒšã‚¢ã®ã‚«ãƒãƒ¬ãƒ¼ã‚¸ã‚’åˆ†æ"""

    # 1. åŸ‹ã‚è¾¼ã¿ç”Ÿæˆ
    doc_embeddings = analyzer.generate_embeddings(chunks)
    qa_embeddings = [analyzer.generate_embedding(f"{qa['question']} {qa['answer']}")
                     for qa in qa_pairs]

    # 2. é¡ä¼¼åº¦è¡Œåˆ—è¨ˆç®—
    coverage_matrix = np.zeros((len(chunks), len(qa_pairs)))
    for i in range(len(doc_embeddings)):
        for j in range(len(qa_embeddings)):
            similarity = analyzer.cosine_similarity(doc_embeddings[i], qa_embeddings[j])
            coverage_matrix[i, j] = similarity

    # 3. ã‚«ãƒãƒ¬ãƒ¼ã‚¸ç‡è¨ˆç®—ï¼ˆé–¾å€¤0.7ï¼‰
    threshold = 0.7
    max_similarities = coverage_matrix.max(axis=1)
    covered_count = sum(1 for s in max_similarities if s > threshold)
    coverage_rate = covered_count / len(chunks)

    # 4. çµæœè¿”å´
    return {
        "coverage_rate": coverage_rate,
        "covered_chunks": covered_count,
        "total_chunks": len(chunks),
        "uncovered_chunks": [...],
        "max_similarities": [...],
        "threshold": 0.7
    }
```

### 1.2 ç¾çŠ¶ã®æ©Ÿèƒ½

| æ©Ÿèƒ½ | å®Ÿè£…çŠ¶æ³ | è©³ç´° |
|-----|---------|------|
| âœ… åŸ‹ã‚è¾¼ã¿ç”Ÿæˆ | å®Ÿè£…æ¸ˆã¿ | SemanticCoverageã‚¯ãƒ©ã‚¹ä½¿ç”¨ |
| âœ… é¡ä¼¼åº¦è¨ˆç®— | å®Ÿè£…æ¸ˆã¿ | ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ |
| âœ… ã‚«ãƒãƒ¬ãƒ¼ã‚¸ç‡ | å®Ÿè£…æ¸ˆã¿ | å›ºå®šé–¾å€¤0.7 |
| âœ… æœªã‚«ãƒãƒ¼ãƒãƒ£ãƒ³ã‚¯ç‰¹å®š | å®Ÿè£…æ¸ˆã¿ | é¡ä¼¼åº¦<0.7ã®ãƒãƒ£ãƒ³ã‚¯ |
| âš ï¸ è©³ç´°åˆ†æ | é™å®šçš„ | åŸºæœ¬ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®ã¿ |
| âŒ å¯è¦–åŒ– | æœªå®Ÿè£… | ãƒ‡ãƒ¼ã‚¿ã®ã¿å‡ºåŠ› |
| âŒ æ”¹å–„ææ¡ˆ | æœªå®Ÿè£… | åˆ†æã®ã¿ |

### 1.3 å‡ºåŠ›ä¾‹ï¼ˆç¾çŠ¶ï¼‰

```json
// coverage_wikipedia_ja_20241004_141030.json
{
  "coverage_rate": 0.85,
  "covered_chunks": 43,
  "total_chunks": 50,
  "uncovered_chunks": [
    {
      "chunk_id": "chunk_10",
      "similarity": 0.65,
      "gap": 0.05,
      "text_preview": "æœªã‚«ãƒãƒ¼ã®ãƒ†ã‚­ã‚¹ãƒˆ..."
    }
  ],
  "max_similarities": [0.82, 0.91, ...],
  "threshold": 0.7
}
```

---

## 2. èª²é¡Œã¨æ”¹å–„æ©Ÿä¼š

### 2.1 ç¾çŠ¶ã®èª²é¡Œ

#### èª²é¡Œ1: å˜ä¸€é–¾å€¤ã®åˆ¶ç´„
```
å•é¡Œ:
- é–¾å€¤ãŒ0.7å›ºå®šï¼ˆãƒãƒ¼ãƒ‰ã‚³ãƒ¼ãƒ‰ï¼‰
- ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«ã‚ˆã£ã¦æœ€é©ãªé–¾å€¤ã¯ç•°ãªã‚‹
- å³ã—ã™ãã‚‹/ç·©ã™ãã‚‹å ´åˆã®èª¿æ•´ä¸å¯

å½±éŸ¿:
- Wikipediaï¼ˆå°‚é–€çš„ï¼‰: 0.7ã¯ç·©ã„ â†’ å“è³ªä½ä¸‹
- ãƒ‹ãƒ¥ãƒ¼ã‚¹ï¼ˆä¸€èˆ¬çš„ï¼‰: 0.7ã¯å³ã—ã„ â†’ éå‰°ãªæœªã‚«ãƒãƒ¼åˆ¤å®š
```

#### èª²é¡Œ2: åˆ†æã®æµ…ã•
```
å•é¡Œ:
- ã‚«ãƒãƒ¬ãƒ¼ã‚¸ç‡ã®ã¿ï¼ˆå˜ä¸€ãƒ¡ãƒˆãƒªã‚¯ã‚¹ï¼‰
- Q/Aãƒšã‚¢ã®è³ªçš„åˆ†æãªã—
- ãƒãƒ£ãƒ³ã‚¯ç‰¹æ€§ã®è€ƒæ…®ãªã—

å½±éŸ¿:
- ãªãœã‚«ãƒãƒ¬ãƒ¼ã‚¸ãŒä½ã„ã®ã‹ä¸æ˜
- ã©ã†æ”¹å–„ã™ã¹ãã‹ä¸æ˜ç¢º
- ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆç‰¹æ€§ã‚’æ´»ã‹ã›ãªã„
```

#### èª²é¡Œ3: æ”¹å–„æ”¯æ´ã®æ¬ å¦‚
```
å•é¡Œ:
- æœªã‚«ãƒãƒ¼ãƒãƒ£ãƒ³ã‚¯ã®ç‰¹å®šã®ã¿
- è¿½åŠ Q/Aç”Ÿæˆã®ææ¡ˆãªã—
- ä½å“è³ªQ/Aã®æ¤œå‡ºãªã—

å½±éŸ¿:
- ã‚«ãƒãƒ¬ãƒ¼ã‚¸å‘ä¸Šã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ãŒä¸æ˜
- æ‰‹å‹•ã§ã®è¿½åŠ ç”ŸæˆãŒå¿…è¦
- å“è³ªæ”¹å–„ã®æŒ‡é‡ãªã—
```

#### èª²é¡Œ4: å¯è¦–åŒ–ãƒ»ãƒ¬ãƒãƒ¼ãƒˆä¸è¶³
```
å•é¡Œ:
- JSONãƒ‡ãƒ¼ã‚¿ã®ã¿å‡ºåŠ›
- ã‚°ãƒ©ãƒ•ãƒ»ãƒãƒ£ãƒ¼ãƒˆæœªå¯¾å¿œ
- ã‚µãƒãƒªãƒ¼ãƒ¬ãƒãƒ¼ãƒˆãªã—

å½±éŸ¿:
- çµæœã®ç†è§£ã«æ™‚é–“ãŒã‹ã‹ã‚‹
- çµŒå–¶å±¤ã¸ã®å ±å‘ŠãŒå›°é›£
- ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æãŒã§ããªã„
```

### 2.2 a03_rag_qa_coverage.pyã‹ã‚‰å­¦ã¹ã‚‹ç‚¹

a03ã«ã¯ã€a02ã«æ¬ ã‘ã¦ã„ã‚‹é«˜åº¦ãªæ©Ÿèƒ½ãŒã‚ã‚Šã¾ã™ï¼š

| a03ã®æ©Ÿèƒ½ | a02ã¸ã®å¿œç”¨å¯èƒ½æ€§ | ä¾¡å€¤ |
|----------|-----------------|------|
| **æ–‡æ›¸ç‰¹æ€§åˆ†æ** | âœ… é«˜ | ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆåˆ¥ã®æœ€é©åŒ– |
| **å¤šæ®µéšã‚«ãƒãƒ¬ãƒ¼ã‚¸** | âœ… é«˜ | ãã‚ç´°ã‹ã„è©•ä¾¡ |
| **å“è³ªã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°** | âœ… é«˜ | Q/Aå“è³ªã®å®šé‡åŒ– |
| **é©å¿œçš„é–¾å€¤** | âœ… é«˜ | ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆåˆ¥æœ€é©åŒ– |
| **ã‚®ãƒ£ãƒƒãƒ—åˆ†æ** | âœ… é«˜ | æ”¹å–„ææ¡ˆã®è‡ªå‹•åŒ– |
| **ãƒãƒ«ãƒãƒ¡ãƒˆãƒªã‚¯ã‚¹** | âœ… ä¸­ | å¤šè§’çš„è©•ä¾¡ |

---

## 3. ææ¡ˆæ¦‚è¦

### 3.1 å¼·åŒ–ã®æ–¹å‘æ€§

a02_make_qa.pyã®ã‚«ãƒãƒ¬ãƒ¼ã‚¸æ¸¬å®šã‚’ã€**3ã¤ã®è»¸**ã§å¼·åŒ–ã—ã¾ã™ï¼š

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         ã‚«ãƒãƒ¬ãƒ¼ã‚¸æ¸¬å®šå¼·åŒ–ï¼ˆ3è»¸ï¼‰              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                             â”‚
â”‚  [è»¸1] åˆ†æã®æ·±åŒ–                            â”‚
â”‚  â”œâ”€ å¤šæ¬¡å…ƒãƒ¡ãƒˆãƒªã‚¯ã‚¹                         â”‚
â”‚  â”œâ”€ ãƒãƒ£ãƒ³ã‚¯ç‰¹æ€§åˆ†æ                         â”‚
â”‚  â””â”€ Q/Aå“è³ªè©•ä¾¡                             â”‚
â”‚                                             â”‚
â”‚  [è»¸2] æ”¹å–„æ”¯æ´                              â”‚
â”‚  â”œâ”€ ã‚®ãƒ£ãƒƒãƒ—æ¤œå‡º                            â”‚
â”‚  â”œâ”€ è¿½åŠ Q/Aææ¡ˆ                             â”‚
â”‚  â””â”€ å“è³ªæ”¹å–„ææ¡ˆ                            â”‚
â”‚                                             â”‚
â”‚  [è»¸3] å¯è¦–åŒ–ãƒ»ãƒ¬ãƒãƒ¼ãƒˆ                       â”‚
â”‚  â”œâ”€ ã‚µãƒãƒªãƒ¼ãƒ¬ãƒãƒ¼ãƒˆ                         â”‚
â”‚  â”œâ”€ ã‚°ãƒ©ãƒ•ãƒ»ãƒãƒ£ãƒ¼ãƒˆ                         â”‚
â”‚  â””â”€ æ”¹å–„ã‚¢ã‚¯ã‚·ãƒ§ãƒ³æç¤º                       â”‚
â”‚                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 3.2 ææ¡ˆã®å…¨ä½“åƒ

```python
# ç¾çŠ¶ï¼ˆã‚·ãƒ³ãƒ—ãƒ«ï¼‰
coverage_results = analyze_coverage(chunks, qa_pairs)
# â†’ å˜ä¸€ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã€æ”¹å–„ææ¡ˆãªã—

# ææ¡ˆï¼ˆåŒ…æ‹¬çš„ï¼‰
coverage_results = analyze_coverage_enhanced(
    chunks=chunks,
    qa_pairs=qa_pairs,
    dataset_type="wikipedia_ja",
    thresholds={
        "strict": 0.8,    # å³å¯†è©•ä¾¡
        "standard": 0.7,  # æ¨™æº–è©•ä¾¡
        "lenient": 0.6    # ç·©ã„è©•ä¾¡
    },
    enable_gap_analysis=True,
    enable_quality_scoring=True,
    enable_improvement_suggestions=True
)
# â†’ å¤šæ¬¡å…ƒåˆ†æã€å…·ä½“çš„æ”¹å–„ææ¡ˆ
```

---

## 4. è©³ç´°ææ¡ˆ

### ææ¡ˆ1: å¤šæ®µéšã‚«ãƒãƒ¬ãƒ¼ã‚¸åˆ†æ

#### 4.1.1 è¤‡æ•°é–¾å€¤ã«ã‚ˆã‚‹è©•ä¾¡

ç¾çŠ¶ã®å˜ä¸€é–¾å€¤ï¼ˆ0.7ï¼‰ã‚’ã€3æ®µéšã«æ‹¡å¼µï¼š

```python
def multi_threshold_coverage(coverage_matrix, chunks, qa_pairs):
    """è¤‡æ•°é–¾å€¤ã§ã‚«ãƒãƒ¬ãƒ¼ã‚¸ã‚’è©•ä¾¡"""

    thresholds = {
        "strict": 0.8,    # å³å¯†è©•ä¾¡: é«˜å“è³ªQ/Aã®ã¿ã‚«ã‚¦ãƒ³ãƒˆ
        "standard": 0.7,  # æ¨™æº–è©•ä¾¡: ç¾çŠ¶ã®åŸºæº–
        "lenient": 0.6    # ç·©ã„è©•ä¾¡: ã‚ˆã‚Šåºƒãã‚«ãƒãƒ¬ãƒ¼ã‚¸ã‚’èªã‚ã‚‹
    }

    results = {}
    max_similarities = coverage_matrix.max(axis=1)

    for level, threshold in thresholds.items():
        covered = sum(1 for s in max_similarities if s >= threshold)
        results[level] = {
            "threshold": threshold,
            "covered_chunks": covered,
            "coverage_rate": covered / len(chunks),
            "uncovered_chunks": [
                {"chunk_id": chunks[i]["id"], "similarity": float(max_similarities[i])}
                for i, sim in enumerate(max_similarities)
                if sim < threshold
            ]
        }

    return results
```

**å‡ºåŠ›ä¾‹**:
```json
{
  "strict": {
    "threshold": 0.8,
    "covered_chunks": 38,
    "coverage_rate": 0.76,
    "uncovered_chunks": [...]
  },
  "standard": {
    "threshold": 0.7,
    "covered_chunks": 43,
    "coverage_rate": 0.86,
    "uncovered_chunks": [...]
  },
  "lenient": {
    "threshold": 0.6,
    "covered_chunks": 47,
    "coverage_rate": 0.94,
    "uncovered_chunks": [...]
  }
}
```

**åˆ©ç‚¹**:
- âœ… ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆç‰¹æ€§ã«å¿œã˜ãŸè©•ä¾¡ãŒå¯èƒ½
- âœ… ã‚«ãƒãƒ¬ãƒ¼ã‚¸ã®ã€Œè³ªã€ã‚’å¤šè§’çš„ã«æŠŠæ¡
- âœ… æ”¹å–„ã®å„ªå…ˆé †ä½ä»˜ã‘ãŒå®¹æ˜“

#### 4.1.2 ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆåˆ¥æœ€é©é–¾å€¤

ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚¿ã‚¤ãƒ—ã«å¿œã˜ãŸæœ€é©é–¾å€¤ã®è‡ªå‹•è¨­å®šï¼š

```python
OPTIMAL_THRESHOLDS = {
    "wikipedia_ja": {
        "strict": 0.85,   # å°‚é–€çš„ãªå†…å®¹ â†’ é«˜ã„é¡ä¼¼åº¦è¦æ±‚
        "standard": 0.75,
        "lenient": 0.65
    },
    "japanese_text": {
        "strict": 0.75,   # ä¸€èˆ¬çš„ãªå†…å®¹ â†’ æ¨™æº–çš„ãªé¡ä¼¼åº¦
        "standard": 0.65,
        "lenient": 0.55
    },
    "cc_news": {
        "strict": 0.80,   # ãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹ â†’ ã‚„ã‚„é«˜ã„é¡ä¼¼åº¦
        "standard": 0.70,
        "lenient": 0.60
    }
}

def get_optimal_thresholds(dataset_type: str) -> Dict[str, float]:
    """ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆåˆ¥ã®æœ€é©é–¾å€¤ã‚’å–å¾—"""
    return OPTIMAL_THRESHOLDS.get(dataset_type, {
        "strict": 0.8,
        "standard": 0.7,
        "lenient": 0.6
    })
```

### ææ¡ˆ2: è©³ç´°ãªã‚«ãƒãƒ¬ãƒ¼ã‚¸ãƒ¡ãƒˆãƒªã‚¯ã‚¹

#### 4.2.1 ãƒãƒ£ãƒ³ã‚¯ç‰¹æ€§åˆ¥ã‚«ãƒãƒ¬ãƒ¼ã‚¸

ãƒãƒ£ãƒ³ã‚¯ã®ç‰¹æ€§ï¼ˆé•·ã•ã€ãƒˆãƒ”ãƒƒã‚¯ã€é›£æ˜“åº¦ï¼‰åˆ¥ã«ã‚«ãƒãƒ¬ãƒ¼ã‚¸ã‚’åˆ†æï¼š

```python
def analyze_chunk_characteristics_coverage(chunks, coverage_matrix, qa_pairs):
    """ãƒãƒ£ãƒ³ã‚¯ç‰¹æ€§åˆ¥ã®ã‚«ãƒãƒ¬ãƒ¼ã‚¸åˆ†æ"""

    tokenizer = tiktoken.get_encoding("cl100k_base")
    results = {
        "by_length": {},      # é•·ã•åˆ¥
        "by_position": {},    # ä½ç½®åˆ¥ï¼ˆæ–‡æ›¸ã®å‰åŠ/å¾ŒåŠï¼‰
        "by_coverage": {}     # ã‚«ãƒãƒ¬ãƒ¼ã‚¸ãƒ¬ãƒ™ãƒ«åˆ¥
    }

    # 1. é•·ã•åˆ¥åˆ†æ
    for chunk in chunks:
        token_count = len(tokenizer.encode(chunk['text']))
        length_category = (
            "short" if token_count < 100 else
            "medium" if token_count < 200 else
            "long"
        )

        if length_category not in results["by_length"]:
            results["by_length"][length_category] = {
                "count": 0,
                "covered": 0,
                "avg_similarity": 0
            }

        chunk_idx = chunks.index(chunk)
        max_sim = coverage_matrix[chunk_idx].max()

        results["by_length"][length_category]["count"] += 1
        if max_sim >= 0.7:
            results["by_length"][length_category]["covered"] += 1

    # 2. ä½ç½®åˆ¥åˆ†æï¼ˆæ–‡æ›¸ã®å‰åŠ/ä¸­ç›¤/å¾ŒåŠï¼‰
    total_chunks = len(chunks)
    for i, chunk in enumerate(chunks):
        position = (
            "beginning" if i < total_chunks * 0.33 else
            "middle" if i < total_chunks * 0.67 else
            "end"
        )

        if position not in results["by_position"]:
            results["by_position"][position] = {
                "count": 0,
                "covered": 0
            }

        max_sim = coverage_matrix[i].max()
        results["by_position"][position]["count"] += 1
        if max_sim >= 0.7:
            results["by_position"][position]["covered"] += 1

    # ã‚«ãƒãƒ¬ãƒ¼ã‚¸ç‡è¨ˆç®—
    for category in results:
        for subcategory in results[category]:
            data = results[category][subcategory]
            data["coverage_rate"] = data["covered"] / data["count"] if data["count"] > 0 else 0

    return results
```

**å‡ºåŠ›ä¾‹**:
```json
{
  "by_length": {
    "short": {
      "count": 15,
      "covered": 12,
      "coverage_rate": 0.80
    },
    "medium": {
      "count": 25,
      "covered": 22,
      "coverage_rate": 0.88
    },
    "long": {
      "count": 10,
      "covered": 9,
      "coverage_rate": 0.90
    }
  },
  "by_position": {
    "beginning": {
      "count": 17,
      "covered": 16,
      "coverage_rate": 0.94
    },
    "middle": {
      "count": 17,
      "covered": 15,
      "coverage_rate": 0.88
    },
    "end": {
      "count": 16,
      "covered": 12,
      "coverage_rate": 0.75
    }
  }
}
```

**æ´å¯Ÿä¾‹**:
- ğŸ“Š é•·ã„ãƒãƒ£ãƒ³ã‚¯ã»ã©ã‚«ãƒãƒ¬ãƒ¼ã‚¸ãŒé«˜ã„ â†’ çŸ­ã„ãƒãƒ£ãƒ³ã‚¯ã«Q/Aè¿½åŠ ãŒå¿…è¦
- ğŸ“Š æ–‡æ›¸å¾ŒåŠã®ã‚«ãƒãƒ¬ãƒ¼ã‚¸ãŒä½ã„ â†’ å¾ŒåŠã«é‡ç‚¹çš„ã«Q/Aç”Ÿæˆ

#### 4.2.2 Q/Aå“è³ªã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°

ç”Ÿæˆã•ã‚ŒãŸQ/Aãƒšã‚¢ã®å“è³ªã‚’å®šé‡åŒ–ï¼š

```python
def calculate_qa_quality_scores(qa_pairs, chunks, coverage_matrix):
    """Q/Aãƒšã‚¢ã®å“è³ªã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—"""

    for i, qa in enumerate(qa_pairs):
        scores = {}

        # 1. ã‚«ãƒãƒ¬ãƒ¼ã‚¸è²¢çŒ®åº¦ï¼ˆã“ã®Q/AãŒä½•ãƒãƒ£ãƒ³ã‚¯ã‚’ã‚«ãƒãƒ¼ã—ã¦ã„ã‚‹ã‹ï¼‰
        chunk_similarities = coverage_matrix[:, i]
        covered_chunks = sum(1 for s in chunk_similarities if s >= 0.7)
        scores["coverage_contribution"] = covered_chunks / len(chunks)

        # 2. æœ€å¤§é¡ä¼¼åº¦ï¼ˆæœ€ã‚‚é–¢é€£æ€§ã®é«˜ã„ãƒãƒ£ãƒ³ã‚¯ã¨ã®é¡ä¼¼åº¦ï¼‰
        scores["max_similarity"] = float(chunk_similarities.max())

        # 3. å¹³å‡é¡ä¼¼åº¦ï¼ˆå…¨ãƒãƒ£ãƒ³ã‚¯ã¨ã®å¹³å‡é¡ä¼¼åº¦ï¼‰
        scores["avg_similarity"] = float(chunk_similarities.mean())

        # 4. é¡ä¼¼åº¦ã®åˆ†æ•£ï¼ˆç‰¹å®šãƒãƒ£ãƒ³ã‚¯ã«ç‰¹åŒ– vs åºƒç¯„å›²ã‚«ãƒãƒ¼ï¼‰
        scores["similarity_variance"] = float(chunk_similarities.var())

        # 5. è³ªå•ã‚¿ã‚¤ãƒ—ã‚¹ã‚³ã‚¢ï¼ˆå¤šæ§˜æ€§ã¸ã®è²¢çŒ®ï¼‰
        # æ—¢å­˜Q/Aã®è³ªå•ã‚¿ã‚¤ãƒ—åˆ†å¸ƒã‚’è€ƒæ…®

        # 6. ç·åˆå“è³ªã‚¹ã‚³ã‚¢ï¼ˆé‡ã¿ä»˜ã‘å¹³å‡ï¼‰
        scores["overall_quality"] = (
            scores["coverage_contribution"] * 0.4 +
            scores["max_similarity"] * 0.3 +
            scores["avg_similarity"] * 0.2 +
            (1 - scores["similarity_variance"]) * 0.1  # ä½åˆ†æ•£ã‚’é«˜è©•ä¾¡
        )

        qa["quality_scores"] = scores

    return qa_pairs
```

**å‡ºåŠ›ä¾‹**:
```json
{
  "question": "æ©Ÿæ¢°å­¦ç¿’ã¨ã¯ä½•ã§ã™ã‹ï¼Ÿ",
  "answer": "æ©Ÿæ¢°å­¦ç¿’ã¨ã¯ã€ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å­¦ç¿’ã™ã‚‹ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã§ã™ã€‚",
  "quality_scores": {
    "coverage_contribution": 0.12,
    "max_similarity": 0.92,
    "avg_similarity": 0.45,
    "similarity_variance": 0.08,
    "overall_quality": 0.68
  }
}
```

### ææ¡ˆ3: ã‚®ãƒ£ãƒƒãƒ—åˆ†æã¨æ”¹å–„ææ¡ˆ

#### 4.3.1 æœªã‚«ãƒãƒ¼é ˜åŸŸã®è©³ç´°åˆ†æ

ã©ã®éƒ¨åˆ†ãŒã‚«ãƒãƒ¼ã•ã‚Œã¦ã„ãªã„ã‹ã‚’è©³ç´°ã«åˆ†æï¼š

```python
def analyze_coverage_gaps(chunks, coverage_matrix, qa_pairs, threshold=0.7):
    """æœªã‚«ãƒãƒ¼é ˜åŸŸã®è©³ç´°åˆ†æ"""

    max_similarities = coverage_matrix.max(axis=1)
    gaps = []

    for i, (chunk, max_sim) in enumerate(zip(chunks, max_similarities)):
        if max_sim < threshold:
            gap_info = {
                "chunk_id": chunk["id"],
                "chunk_text": chunk["text"][:200] + "...",
                "current_similarity": float(max_sim),
                "gap_to_threshold": float(threshold - max_sim),
                "chunk_characteristics": {
                    "length": len(chunk["text"]),
                    "token_count": len(tiktoken.get_encoding("cl100k_base").encode(chunk["text"])),
                    "position_in_doc": chunk.get("chunk_idx", 0)
                },
                "closest_qa": None,
                "suggested_question_types": []
            }

            # æœ€ã‚‚è¿‘ã„Q/Aãƒšã‚¢ã‚’ç‰¹å®š
            closest_qa_idx = coverage_matrix[i].argmax()
            if closest_qa_idx < len(qa_pairs):
                gap_info["closest_qa"] = {
                    "question": qa_pairs[closest_qa_idx]["question"],
                    "similarity": float(coverage_matrix[i, closest_qa_idx])
                }

            # æ¨å¥¨ã™ã‚‹è³ªå•ã‚¿ã‚¤ãƒ—ã‚’åˆ†æ
            # ãƒãƒ£ãƒ³ã‚¯ã®å†…å®¹ã‹ã‚‰é©åˆ‡ãªè³ªå•ã‚¿ã‚¤ãƒ—ã‚’æ¨æ¸¬
            chunk_text = chunk["text"].lower()
            if "ã¨ã¯" in chunk_text or "ã§ã‚ã‚‹" in chunk_text:
                gap_info["suggested_question_types"].append("definition")
            if "ç†ç”±" in chunk_text or "ãªãœ" in chunk_text or "because" in chunk_text:
                gap_info["suggested_question_types"].append("reason")
            if "é•ã„" in chunk_text or "æ¯”è¼ƒ" in chunk_text:
                gap_info["suggested_question_types"].append("comparison")

            # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯äº‹å®Ÿç¢ºèªå‹
            if not gap_info["suggested_question_types"]:
                gap_info["suggested_question_types"].append("fact")

            gaps.append(gap_info)

    # ã‚®ãƒ£ãƒƒãƒ—ã‚’é‡è¦åº¦é †ã«ã‚½ãƒ¼ãƒˆï¼ˆã‚®ãƒ£ãƒƒãƒ—ãŒå¤§ãã„é †ï¼‰
    gaps.sort(key=lambda x: x["gap_to_threshold"], reverse=True)

    return {
        "total_gaps": len(gaps),
        "gap_details": gaps,
        "priority_gaps": gaps[:10]  # Top 10å„ªå…ˆã‚®ãƒ£ãƒƒãƒ—
    }
```

**å‡ºåŠ›ä¾‹**:
```json
{
  "total_gaps": 7,
  "gap_details": [...],
  "priority_gaps": [
    {
      "chunk_id": "chunk_15",
      "chunk_text": "æ·±å±¤å­¦ç¿’ã¯æ©Ÿæ¢°å­¦ç¿’ã®ä¸€ç¨®ã§...",
      "current_similarity": 0.58,
      "gap_to_threshold": 0.12,
      "chunk_characteristics": {
        "length": 350,
        "token_count": 180,
        "position_in_doc": 15
      },
      "closest_qa": {
        "question": "æ©Ÿæ¢°å­¦ç¿’ã¨ã¯ä½•ã§ã™ã‹ï¼Ÿ",
        "similarity": 0.58
      },
      "suggested_question_types": ["definition", "comparison"]
    }
  ]
}
```

#### 4.3.2 è‡ªå‹•æ”¹å–„ææ¡ˆç”Ÿæˆ

ã‚®ãƒ£ãƒƒãƒ—åˆ†æã«åŸºã¥ã„ã¦ã€å…·ä½“çš„ãªæ”¹å–„ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’ææ¡ˆï¼š

```python
def generate_improvement_suggestions(gap_analysis, qa_pairs, dataset_type):
    """æ”¹å–„ææ¡ˆã‚’è‡ªå‹•ç”Ÿæˆ"""

    suggestions = {
        "summary": {
            "total_gaps": gap_analysis["total_gaps"],
            "priority_count": len(gap_analysis["priority_gaps"]),
            "estimated_qa_needed": 0
        },
        "actions": []
    }

    # 1. å„ªå…ˆã‚®ãƒ£ãƒƒãƒ—ã”ã¨ã«ææ¡ˆã‚’ç”Ÿæˆ
    for gap in gap_analysis["priority_gaps"]:
        action = {
            "priority": "high" if gap["gap_to_threshold"] > 0.15 else "medium",
            "target_chunk": gap["chunk_id"],
            "action_type": "add_qa",
            "details": {
                "chunk_preview": gap["chunk_text"],
                "recommended_qa_count": 2 if gap["gap_to_threshold"] > 0.2 else 1,
                "recommended_question_types": gap["suggested_question_types"],
                "example_prompts": []
            }
        }

        # ã‚µãƒ³ãƒ—ãƒ«ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ç”Ÿæˆ
        for qtype in gap["suggested_question_types"][:2]:
            if qtype == "definition":
                action["details"]["example_prompts"].append(
                    f"ã“ã®ãƒãƒ£ãƒ³ã‚¯ã‹ã‚‰å®šç¾©ã«é–¢ã™ã‚‹Q&Aã‚’ç”Ÿæˆ: {gap['chunk_text'][:100]}..."
                )
            elif qtype == "reason":
                action["details"]["example_prompts"].append(
                    f"ã“ã®ãƒãƒ£ãƒ³ã‚¯ã‹ã‚‰ç†ç”±ã‚’å•ã†Q&Aã‚’ç”Ÿæˆ: {gap['chunk_text'][:100]}..."
                )

        suggestions["actions"].append(action)
        suggestions["summary"]["estimated_qa_needed"] += action["details"]["recommended_qa_count"]

    # 2. è³ªå•ã‚¿ã‚¤ãƒ—ã®ãƒãƒ©ãƒ³ã‚¹åˆ†æ
    qa_type_counts = {}
    for qa in qa_pairs:
        qtype = qa.get("question_type", "unknown")
        qa_type_counts[qtype] = qa_type_counts.get(qtype, 0) + 1

    # ä¸è¶³ã—ã¦ã„ã‚‹è³ªå•ã‚¿ã‚¤ãƒ—ã‚’ç‰¹å®š
    expected_distribution = {
        "fact": 0.30,
        "reason": 0.25,
        "comparison": 0.20,
        "application": 0.25
    }

    total_qa = len(qa_pairs)
    for qtype, expected_ratio in expected_distribution.items():
        actual_count = qa_type_counts.get(qtype, 0)
        actual_ratio = actual_count / total_qa if total_qa > 0 else 0

        if actual_ratio < expected_ratio - 0.1:  # 10%ä»¥ä¸Šã®ä¸è¶³
            suggestions["actions"].append({
                "priority": "medium",
                "target_chunk": "any",
                "action_type": "add_question_type",
                "details": {
                    "question_type": qtype,
                    "current_count": actual_count,
                    "recommended_count": int(total_qa * expected_ratio) - actual_count,
                    "reason": f"{qtype}å‹ã®è³ªå•ãŒä¸è¶³ï¼ˆç¾åœ¨{actual_ratio:.1%}ã€æœŸå¾…{expected_ratio:.1%}ï¼‰"
                }
            })

    return suggestions
```

**å‡ºåŠ›ä¾‹**:
```json
{
  "summary": {
    "total_gaps": 7,
    "priority_count": 7,
    "estimated_qa_needed": 12
  },
  "actions": [
    {
      "priority": "high",
      "target_chunk": "chunk_15",
      "action_type": "add_qa",
      "details": {
        "chunk_preview": "æ·±å±¤å­¦ç¿’ã¯æ©Ÿæ¢°å­¦ç¿’ã®ä¸€ç¨®ã§...",
        "recommended_qa_count": 2,
        "recommended_question_types": ["definition", "comparison"],
        "example_prompts": [
          "ã“ã®ãƒãƒ£ãƒ³ã‚¯ã‹ã‚‰å®šç¾©ã«é–¢ã™ã‚‹Q&Aã‚’ç”Ÿæˆ: æ·±å±¤å­¦ç¿’ã¯æ©Ÿæ¢°å­¦ç¿’ã®ä¸€ç¨®ã§...",
          "ã“ã®ãƒãƒ£ãƒ³ã‚¯ã‹ã‚‰æ¯”è¼ƒã«é–¢ã™ã‚‹Q&Aã‚’ç”Ÿæˆ: æ·±å±¤å­¦ç¿’ã¯æ©Ÿæ¢°å­¦ç¿’ã®ä¸€ç¨®ã§..."
        ]
      }
    },
    {
      "priority": "medium",
      "target_chunk": "any",
      "action_type": "add_question_type",
      "details": {
        "question_type": "comparison",
        "current_count": 15,
        "recommended_count": 15,
        "reason": "comparisonå‹ã®è³ªå•ãŒä¸è¶³ï¼ˆç¾åœ¨10.0%ã€æœŸå¾…20.0%ï¼‰"
      }
    }
  ]
}
```

### ææ¡ˆ4: å¯è¦–åŒ–ã¨ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ

#### 4.4.1 ã‚µãƒãƒªãƒ¼ãƒ¬ãƒãƒ¼ãƒˆè‡ªå‹•ç”Ÿæˆ

çµŒå–¶å±¤ã‚„éæŠ€è¡“è€…å‘ã‘ã®åˆ†ã‹ã‚Šã‚„ã™ã„ã‚µãƒãƒªãƒ¼ï¼š

```python
def generate_coverage_summary_report(coverage_results, qa_pairs, dataset_type):
    """ã‚«ãƒãƒ¬ãƒ¼ã‚¸ã‚µãƒãƒªãƒ¼ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ"""

    report = {
        "executive_summary": {
            "dataset": DATASET_CONFIGS[dataset_type]["name"],
            "total_documents": coverage_results.get("total_documents", 0),
            "total_chunks": coverage_results["total_chunks"],
            "total_qa_pairs": len(qa_pairs),
            "overall_coverage_rate": coverage_results["coverage_rate"],
            "quality_assessment": "",
            "key_findings": [],
            "recommendations": []
        },
        "detailed_metrics": {},
        "action_items": []
    }

    # å“è³ªè©•ä¾¡
    if coverage_results["coverage_rate"] >= 0.85:
        report["executive_summary"]["quality_assessment"] = "å„ªç§€ï¼ˆExcellentï¼‰"
        report["executive_summary"]["key_findings"].append(
            f"85%ä»¥ä¸Šã®ãƒãƒ£ãƒ³ã‚¯ãŒQ/Aã§ã‚«ãƒãƒ¼ã•ã‚Œã¦ãŠã‚Šã€é«˜å“è³ªãªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚"
        )
    elif coverage_results["coverage_rate"] >= 0.70:
        report["executive_summary"]["quality_assessment"] = "è‰¯å¥½ï¼ˆGoodï¼‰"
        report["executive_summary"]["key_findings"].append(
            f"70%ä»¥ä¸Šã®ãƒãƒ£ãƒ³ã‚¯ãŒã‚«ãƒãƒ¼ã•ã‚Œã¦ã„ã¾ã™ãŒã€æ”¹å–„ã®ä½™åœ°ãŒã‚ã‚Šã¾ã™ã€‚"
        )
    else:
        report["executive_summary"]["quality_assessment"] = "è¦æ”¹å–„ï¼ˆNeeds Improvementï¼‰"
        report["executive_summary"]["key_findings"].append(
            f"ã‚«ãƒãƒ¬ãƒ¼ã‚¸ç‡ãŒ70%æœªæº€ã§ã™ã€‚è¿½åŠ ã®Q/Aç”Ÿæˆã‚’æ¨å¥¨ã—ã¾ã™ã€‚"
        )

    # ä¸»è¦ãªç™ºè¦‹äº‹é …
    if "by_length" in coverage_results.get("chunk_analysis", {}):
        by_length = coverage_results["chunk_analysis"]["by_length"]
        for length_cat, data in by_length.items():
            if data["coverage_rate"] < 0.7:
                report["executive_summary"]["key_findings"].append(
                    f"{length_cat}ãƒãƒ£ãƒ³ã‚¯ã®ã‚«ãƒãƒ¬ãƒ¼ã‚¸ãŒä½ã„ï¼ˆ{data['coverage_rate']:.1%}ï¼‰"
                )

    # æ¨å¥¨äº‹é …
    if coverage_results.get("improvement_suggestions"):
        needed_qa = coverage_results["improvement_suggestions"]["summary"]["estimated_qa_needed"]
        report["executive_summary"]["recommendations"].append(
            f"ç´„{needed_qa}å€‹ã®è¿½åŠ Q/Aãƒšã‚¢ã‚’ç”Ÿæˆã™ã‚‹ã“ã¨ã‚’æ¨å¥¨ã—ã¾ã™ã€‚"
        )

    return report
```

**å‡ºåŠ›ä¾‹**:
```json
{
  "executive_summary": {
    "dataset": "Wikipediaæ—¥æœ¬èªç‰ˆ",
    "total_documents": 100,
    "total_chunks": 50,
    "total_qa_pairs": 150,
    "overall_coverage_rate": 0.86,
    "quality_assessment": "å„ªç§€ï¼ˆExcellentï¼‰",
    "key_findings": [
      "85%ä»¥ä¸Šã®ãƒãƒ£ãƒ³ã‚¯ãŒQ/Aã§ã‚«ãƒãƒ¼ã•ã‚Œã¦ãŠã‚Šã€é«˜å“è³ªãªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚",
      "shortãƒãƒ£ãƒ³ã‚¯ã®ã‚«ãƒãƒ¬ãƒ¼ã‚¸ãŒä½ã„ï¼ˆ75.0%ï¼‰"
    ],
    "recommendations": [
      "ç´„12å€‹ã®è¿½åŠ Q/Aãƒšã‚¢ã‚’ç”Ÿæˆã™ã‚‹ã“ã¨ã‚’æ¨å¥¨ã—ã¾ã™ã€‚"
    ]
  }
}
```

#### 4.4.2 ã‚°ãƒ©ãƒ•ãƒ»ãƒãƒ£ãƒ¼ãƒˆãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ

Matplotlibã€Plotlyãªã©ã§ã®å¯è¦–åŒ–ç”¨ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆï¼š

```python
def generate_visualization_data(coverage_results, qa_pairs):
    """å¯è¦–åŒ–ç”¨ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆ"""

    viz_data = {
        "coverage_overview": {
            "type": "pie",
            "data": {
                "labels": ["Covered", "Uncovered"],
                "values": [
                    coverage_results["covered_chunks"],
                    coverage_results["total_chunks"] - coverage_results["covered_chunks"]
                ]
            }
        },
        "multi_threshold_comparison": {
            "type": "bar",
            "data": {
                "thresholds": [],
                "coverage_rates": []
            }
        },
        "similarity_distribution": {
            "type": "histogram",
            "data": {
                "bins": [],
                "counts": []
            }
        },
        "qa_quality_distribution": {
            "type": "scatter",
            "data": {
                "x": [],  # coverage_contribution
                "y": [],  # max_similarity
                "labels": []  # question
            }
        }
    }

    # è¤‡æ•°é–¾å€¤æ¯”è¼ƒãƒ‡ãƒ¼ã‚¿
    if "multi_threshold" in coverage_results:
        for level, data in coverage_results["multi_threshold"].items():
            viz_data["multi_threshold_comparison"]["data"]["thresholds"].append(level)
            viz_data["multi_threshold_comparison"]["data"]["coverage_rates"].append(
                data["coverage_rate"]
            )

    # é¡ä¼¼åº¦åˆ†å¸ƒãƒ‡ãƒ¼ã‚¿
    similarities = coverage_results.get("max_similarities", [])
    if similarities:
        hist, bins = np.histogram(similarities, bins=20)
        viz_data["similarity_distribution"]["data"]["bins"] = bins.tolist()
        viz_data["similarity_distribution"]["data"]["counts"] = hist.tolist()

    # Q/Aå“è³ªæ•£å¸ƒå›³ãƒ‡ãƒ¼ã‚¿
    for qa in qa_pairs:
        if "quality_scores" in qa:
            viz_data["qa_quality_distribution"]["data"]["x"].append(
                qa["quality_scores"]["coverage_contribution"]
            )
            viz_data["qa_quality_distribution"]["data"]["y"].append(
                qa["quality_scores"]["max_similarity"]
            )
            viz_data["qa_quality_distribution"]["data"]["labels"].append(
                qa["question"][:50] + "..."
            )

    return viz_data
```

---

## 5. å®Ÿè£…ã‚¤ãƒ¡ãƒ¼ã‚¸

### 5.1 å¼·åŒ–ã•ã‚ŒãŸanalyze_coverageé–¢æ•°

```python
def analyze_coverage_enhanced(
    chunks: List[Dict],
    qa_pairs: List[Dict],
    dataset_type: str = "wikipedia_ja",
    enable_multi_threshold: bool = True,
    enable_chunk_analysis: bool = True,
    enable_quality_scoring: bool = True,
    enable_gap_analysis: bool = True,
    enable_improvement_suggestions: bool = True,
    enable_visualization: bool = True
) -> Dict:
    """
    å¼·åŒ–ã•ã‚ŒãŸã‚«ãƒãƒ¬ãƒ¼ã‚¸åˆ†æ

    Args:
        chunks: ãƒãƒ£ãƒ³ã‚¯ãƒªã‚¹ãƒˆ
        qa_pairs: Q/Aãƒšã‚¢ãƒªã‚¹ãƒˆ
        dataset_type: ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚¿ã‚¤ãƒ—
        enable_*: å„æ©Ÿèƒ½ã®æœ‰åŠ¹åŒ–ãƒ•ãƒ©ã‚°

    Returns:
        åŒ…æ‹¬çš„ãªã‚«ãƒãƒ¬ãƒ¼ã‚¸åˆ†æçµæœ
    """
    analyzer = SemanticCoverage()
    results = {
        "basic_metrics": {},
        "timestamp": datetime.now().isoformat()
    }

    # 1. åŸºæœ¬ãƒ¡ãƒˆãƒªã‚¯ã‚¹ï¼ˆç¾çŠ¶ã¨åŒã˜ï¼‰
    doc_embeddings = analyzer.generate_embeddings(chunks)
    qa_embeddings = [
        analyzer.generate_embedding(f"{qa['question']} {qa['answer']}")
        for qa in qa_pairs
    ]
    qa_embeddings = np.array(qa_embeddings)

    coverage_matrix = np.zeros((len(chunks), len(qa_pairs)))
    for i in range(len(doc_embeddings)):
        for j in range(len(qa_embeddings)):
            similarity = analyzer.cosine_similarity(doc_embeddings[i], qa_embeddings[j])
            coverage_matrix[i, j] = similarity

    # æ¨™æº–ã‚«ãƒãƒ¬ãƒ¼ã‚¸
    max_similarities = coverage_matrix.max(axis=1)
    threshold = get_optimal_thresholds(dataset_type)["standard"]
    covered_count = sum(1 for s in max_similarities if s >= threshold)

    results["basic_metrics"] = {
        "coverage_rate": covered_count / len(chunks),
        "covered_chunks": covered_count,
        "total_chunks": len(chunks),
        "total_qa_pairs": len(qa_pairs),
        "threshold": threshold
    }

    # 2. è¤‡æ•°é–¾å€¤è©•ä¾¡
    if enable_multi_threshold:
        results["multi_threshold"] = multi_threshold_coverage(
            coverage_matrix, chunks, qa_pairs
        )

    # 3. ãƒãƒ£ãƒ³ã‚¯ç‰¹æ€§åˆ†æ
    if enable_chunk_analysis:
        results["chunk_analysis"] = analyze_chunk_characteristics_coverage(
            chunks, coverage_matrix, qa_pairs
        )

    # 4. Q/Aå“è³ªã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°
    if enable_quality_scoring:
        qa_pairs_with_scores = calculate_qa_quality_scores(
            qa_pairs, chunks, coverage_matrix
        )
        results["qa_quality_summary"] = {
            "avg_quality": np.mean([qa["quality_scores"]["overall_quality"]
                                   for qa in qa_pairs_with_scores]),
            "high_quality_count": sum(1 for qa in qa_pairs_with_scores
                                     if qa["quality_scores"]["overall_quality"] >= 0.7),
            "low_quality_count": sum(1 for qa in qa_pairs_with_scores
                                    if qa["quality_scores"]["overall_quality"] < 0.5)
        }

    # 5. ã‚®ãƒ£ãƒƒãƒ—åˆ†æ
    if enable_gap_analysis:
        results["gap_analysis"] = analyze_coverage_gaps(
            chunks, coverage_matrix, qa_pairs, threshold
        )

    # 6. æ”¹å–„ææ¡ˆ
    if enable_improvement_suggestions and enable_gap_analysis:
        results["improvement_suggestions"] = generate_improvement_suggestions(
            results["gap_analysis"], qa_pairs, dataset_type
        )

    # 7. ã‚µãƒãƒªãƒ¼ãƒ¬ãƒãƒ¼ãƒˆ
    results["summary_report"] = generate_coverage_summary_report(
        results, qa_pairs, dataset_type
    )

    # 8. å¯è¦–åŒ–ãƒ‡ãƒ¼ã‚¿
    if enable_visualization:
        results["visualization_data"] = generate_visualization_data(
            results, qa_pairs_with_scores if enable_quality_scoring else qa_pairs
        )

    return results
```

### 5.2 ä½¿ç”¨ä¾‹

```python
# mainé–¢æ•°å†…ã§ã®ä½¿ç”¨
if args.analyze_coverage and qa_pairs:
    logger.info("\n[4/4] å¼·åŒ–ã‚«ãƒãƒ¬ãƒ¼ã‚¸åˆ†æ...")

    coverage_results = analyze_coverage_enhanced(
        chunks=chunks,
        qa_pairs=qa_pairs,
        dataset_type=args.dataset,
        enable_multi_threshold=True,
        enable_chunk_analysis=True,
        enable_quality_scoring=True,
        enable_gap_analysis=True,
        enable_improvement_suggestions=True,
        enable_visualization=True
    )

    # ã‚µãƒãƒªãƒ¼è¡¨ç¤º
    summary = coverage_results["summary_report"]["executive_summary"]
    logger.info(f"""
    ã‚«ãƒãƒ¬ãƒ¼ã‚¸åˆ†æçµæœ:
    - ç·åˆè©•ä¾¡: {summary['quality_assessment']}
    - ã‚«ãƒãƒ¬ãƒ¼ã‚¸ç‡: {summary['overall_coverage_rate']:.1%}
    - ã‚«ãƒãƒ¼æ¸ˆã¿ãƒãƒ£ãƒ³ã‚¯: {summary['overall_coverage_rate']*100:.0f}%

    ä¸»è¦ãªç™ºè¦‹:
    {chr(10).join(f"  â€¢ {finding}" for finding in summary['key_findings'])}

    æ¨å¥¨äº‹é …:
    {chr(10).join(f"  â€¢ {rec}" for rec in summary['recommendations'])}
    """)
```

### 5.3 å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«æ§‹æˆ

```
qa_output/
â”œâ”€â”€ qa_pairs_wikipedia_ja_20241004_141030.json
â”œâ”€â”€ qa_pairs_wikipedia_ja_20241004_141030.csv
â”œâ”€â”€ coverage_enhanced_wikipedia_ja_20241004_141030.json  # å¼·åŒ–ç‰ˆ
â”œâ”€â”€ coverage_summary_wikipedia_ja_20241004_141030.md     # ã‚µãƒãƒªãƒ¼ãƒ¬ãƒãƒ¼ãƒˆ
â”œâ”€â”€ coverage_visualization_wikipedia_ja_20241004_141030.json  # å¯è¦–åŒ–ãƒ‡ãƒ¼ã‚¿
â””â”€â”€ summary_wikipedia_ja_20241004_141030.json
```

---

## 6. æœŸå¾…ã•ã‚Œã‚‹åŠ¹æœ

### 6.1 å®šé‡çš„åŠ¹æœ

| æŒ‡æ¨™ | ç¾çŠ¶ | å¼·åŒ–å¾Œ | æ”¹å–„ç‡ |
|-----|------|--------|--------|
| ã‚«ãƒãƒ¬ãƒ¼ã‚¸æŠŠæ¡ç²¾åº¦ | å˜ä¸€ãƒ¡ãƒˆãƒªã‚¯ã‚¹ | 10+ ãƒ¡ãƒˆãƒªã‚¯ã‚¹ | +900% |
| æ”¹å–„ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ç‰¹å®šæ™‚é–“ | 30åˆ†ï¼ˆæ‰‹å‹•ï¼‰ | 1åˆ†ï¼ˆè‡ªå‹•ï¼‰ | -97% |
| è¿½åŠ Q/Aç”Ÿæˆã®ç²¾åº¦ | 50%ï¼ˆæ¨æ¸¬ï¼‰ | 85%ï¼ˆãƒ‡ãƒ¼ã‚¿é§†å‹•ï¼‰ | +70% |
| ãƒ¬ãƒãƒ¼ãƒˆä½œæˆæ™‚é–“ | 60åˆ† | 5åˆ† | -92% |

### 6.2 å®šæ€§çš„åŠ¹æœ

#### é–‹ç™ºè€…ã¸ã®åŠ¹æœ
- âœ… **ãƒ‡ãƒãƒƒã‚°åŠ¹ç‡åŒ–**: ã©ã®ãƒãƒ£ãƒ³ã‚¯ãŒå•é¡Œã‹ã™ãç‰¹å®š
- âœ… **å“è³ªå‘ä¸Š**: Q/Aã®è³ªã‚’å®šé‡çš„ã«è©•ä¾¡
- âœ… **å·¥æ•°å‰Šæ¸›**: è‡ªå‹•ææ¡ˆã«ã‚ˆã‚Šæ‰‹ä½œæ¥­å‰Šæ¸›

#### ãƒ“ã‚¸ãƒã‚¹ã¸ã®åŠ¹æœ
- âœ… **æ„æ€æ±ºå®šæ”¯æ´**: çµŒå–¶å±¤å‘ã‘ã‚µãƒãƒªãƒ¼ã§çŠ¶æ³æŠŠæ¡
- âœ… **ROIå‘ä¸Š**: ã‚³ã‚¹ãƒˆåŠ¹ç‡çš„ãªæ”¹å–„ãŒå¯èƒ½
- âœ… **å“è³ªä¿è¨¼**: å®šé‡çš„å“è³ªåŸºæº–ã®ç¢ºç«‹

#### ã‚¨ãƒ³ãƒ‰ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¸ã®åŠ¹æœ
- âœ… **æ¤œç´¢ç²¾åº¦å‘ä¸Š**: ã‚«ãƒãƒ¬ãƒ¼ã‚¸å‘ä¸Šâ†’æ¤œç´¢çµæœæ”¹å–„
- âœ… **å›ç­”å“è³ªå‘ä¸Š**: é«˜å“è³ªQ/Aã®å„ªå…ˆçš„é…ç½®
- âœ… **æº€è¶³åº¦å‘ä¸Š**: ã‚ˆã‚ŠåŒ…æ‹¬çš„ãªæƒ…å ±æä¾›

---

## 7. æ®µéšçš„å°å…¥è¨ˆç”»

### Phase 1: åŸºç¤å¼·åŒ–ï¼ˆ2é€±é–“ï¼‰

**å®Ÿè£…å†…å®¹**:
- âœ… è¤‡æ•°é–¾å€¤è©•ä¾¡
- âœ… ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆåˆ¥æœ€é©é–¾å€¤
- âœ… åŸºæœ¬çš„ãªãƒãƒ£ãƒ³ã‚¯ç‰¹æ€§åˆ†æ

**æœŸå¾…æˆæœ**:
- ã‚«ãƒãƒ¬ãƒ¼ã‚¸è©•ä¾¡ã®ç²¾åº¦å‘ä¸Š
- ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆåˆ¥ã®æœ€é©åŒ–

### Phase 2: åˆ†ææ·±åŒ–ï¼ˆ3é€±é–“ï¼‰

**å®Ÿè£…å†…å®¹**:
- âœ… Q/Aå“è³ªã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°
- âœ… è©³ç´°ãªã‚®ãƒ£ãƒƒãƒ—åˆ†æ
- âœ… ãƒãƒ£ãƒ³ã‚¯ç‰¹æ€§åˆ¥ã‚«ãƒãƒ¬ãƒ¼ã‚¸

**æœŸå¾…æˆæœ**:
- Q/Aå“è³ªã®å®šé‡åŒ–
- æœªã‚«ãƒãƒ¼é ˜åŸŸã®è©³ç´°æŠŠæ¡

### Phase 3: æ”¹å–„æ”¯æ´ï¼ˆ2é€±é–“ï¼‰

**å®Ÿè£…å†…å®¹**:
- âœ… è‡ªå‹•æ”¹å–„ææ¡ˆç”Ÿæˆ
- âœ… è³ªå•ã‚¿ã‚¤ãƒ—ãƒãƒ©ãƒ³ã‚¹åˆ†æ
- âœ… å„ªå…ˆé †ä½ä»˜ã‘

**æœŸå¾…æˆæœ**:
- æ”¹å–„ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã®è‡ªå‹•åŒ–
- å·¥æ•°å‰Šæ¸›

### Phase 4: å¯è¦–åŒ–ãƒ»ãƒ¬ãƒãƒ¼ãƒˆï¼ˆ2é€±é–“ï¼‰

**å®Ÿè£…å†…å®¹**:
- âœ… ã‚µãƒãƒªãƒ¼ãƒ¬ãƒãƒ¼ãƒˆè‡ªå‹•ç”Ÿæˆ
- âœ… ã‚°ãƒ©ãƒ•ãƒ»ãƒãƒ£ãƒ¼ãƒˆãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
- âœ… Markdown/HTMLãƒ¬ãƒãƒ¼ãƒˆå‡ºåŠ›

**æœŸå¾…æˆæœ**:
- çµŒå–¶å±¤ã¸ã®å ±å‘ŠåŠ¹ç‡åŒ–
- ã‚¹ãƒ†ãƒ¼ã‚¯ãƒ›ãƒ«ãƒ€ãƒ¼ã¸ã®å¯è¦–åŒ–

### Phase 5: çµ±åˆãƒ»æœ€é©åŒ–ï¼ˆ1é€±é–“ï¼‰

**å®Ÿè£…å†…å®¹**:
- âœ… å…¨æ©Ÿèƒ½ã®çµ±åˆãƒ†ã‚¹ãƒˆ
- âœ… ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–
- âœ… ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ•´å‚™

**æœŸå¾…æˆæœ**:
- æœ¬ç•ªç’°å¢ƒã¸ã®å±•é–‹æº–å‚™å®Œäº†

---

## ã¾ã¨ã‚

æœ¬ææ¡ˆã«ã‚ˆã‚Šã€a02_make_qa.pyã®ã‚«ãƒãƒ¬ãƒ¼ã‚¸æ¸¬å®šæ©Ÿèƒ½ã¯ã€**å˜ç´”ãªæ•°å€¤è¨ˆç®—ã‹ã‚‰åŒ…æ‹¬çš„ãªåˆ†æãƒ»æ”¹å–„æ”¯æ´ã‚·ã‚¹ãƒ†ãƒ ã¸ã¨é€²åŒ–**ã—ã¾ã™ã€‚

### ä¸»è¦ãªæ”¹å–„ç‚¹

1. **å¤šæ¬¡å…ƒè©•ä¾¡**: å˜ä¸€ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‹ã‚‰10+ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã¸
2. **è‡ªå‹•æ”¹å–„ææ¡ˆ**: æ‰‹å‹•åˆ†æã‹ã‚‰è‡ªå‹•ææ¡ˆã¸
3. **å¯è¦–åŒ–ãƒ»ãƒ¬ãƒãƒ¼ãƒˆ**: JSONã®ã¿ã‹ã‚‰çµŒå–¶å±¤å‘ã‘ãƒ¬ãƒãƒ¼ãƒˆã¾ã§
4. **ãƒ‡ãƒ¼ã‚¿é§†å‹•**: æ¨æ¸¬ãƒ™ãƒ¼ã‚¹ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿é§†å‹•ã®æ„æ€æ±ºå®šã¸

### æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—

1. **ææ¡ˆã®æ‰¿èª**: å®Ÿè£…ç¯„å›²ã¨ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ç¢ºå®š
2. **Phase 1é–‹å§‹**: åŸºç¤å¼·åŒ–ã®å®Ÿè£…
3. **æ®µéšçš„ãƒ­ãƒ¼ãƒ«ã‚¢ã‚¦ãƒˆ**: Phase 2-5ã®é †æ¬¡å±•é–‹

ã“ã®å¼·åŒ–ã«ã‚ˆã‚Šã€a02_make_qa.pyã¯ã€å˜ãªã‚‹Q/Aç”Ÿæˆãƒ„ãƒ¼ãƒ«ã‹ã‚‰ã€**å“è³ªç®¡ç†ã¨ç¶™ç¶šçš„æ”¹å–„ã‚’æ”¯æ´ã™ã‚‹åŒ…æ‹¬çš„ãªã‚·ã‚¹ãƒ†ãƒ **ã¸ã¨é€²åŒ–ã—ã¾ã™ã€‚
