# a00_check.py - Q&A正解率検証ツール

## 概要
QAペアとQdrantコレクションの回答を比較し、コサイン類似度による正解率を評価するStreamlitアプリケーション。

## 目的
- RAGシステムの検索精度を定量的に評価
- 異なる生成手法（LLM、ルールベース、ハイブリッド）の性能比較
- Q&Aデータセットと実際の検索結果の一致度を測定

## 起動方法
```bash
streamlit run a00_check.py --server.port=8501
```

## 主な機能

### 1. データセット選択
2つのQ&Aデータセットから選択可能：

#### CC News データセット
- **ファイル**: `qa_output/a02_qa_pairs_cc_news.csv`
- **件数**: 4,646件
- **対応コレクション**:
  - `raw_cc_news` - Rawデータ (660件)
  - `qa_cc_news_a02_llm` - LLMベース (9,290件)
  - `qa_cc_news_a03_rule` - ルールベース (8,556件)
  - `qa_cc_news_a10_hybrid` - ハイブリッド (4,334件)

#### Livedoor データセット
- **ファイル**: `qa_output/a02_qa_pairs_livedoor.csv`
- **件数**: 965件
- **対応コレクション**:
  - `raw_livedoor` - Rawデータ (20,193件)
  - `qa_livedoor_a02_20_llm` - LLMベース (964件)
  - `qa_livedoor_a03_rule` - ルールベース (466件)
  - `qa_livedoor_a10_hybrid` - ハイブリッド (2,175件)

### 2. 検証プロセス

#### ステップ1: データ読み込み
```python
# CSVから question と answer を読み込み
qa_question = row['question']  # 質問
qa_answer = row['answer']      # 正解答
```

#### ステップ2: Qdrant検索
```python
# questionでQdrantコレクションを検索
ans_output, search_score = search_qdrant(qa_question, collection_name, ...)
# ans_output: 検索回答
# search_score: 検索スコア
```

#### ステップ3: 類似度計算
```python
# 正解答と検索回答のコサイン類似度を計算
similarity = calculate_cosine_similarity(qa_answer, ans_output, openai_client)
```

### 3. 結果表示

#### サマリー統計
スコア分布と平均値を表示：

| スコア範囲 | 件数 | 構成比(%) |
|------------|------|-----------|
| 80%以上    | XX件 | XX.X%     |
| 60%以上    | XX件 | XX.X%     |
| 40%以上    | XX件 | XX.X%     |
| 39%以下    | XX件 | XX.X%     |
| 平均スコア | -    | XX.X%     |

#### 詳細結果テーブル
各Q&Aペアの検証結果を表示：

| 質問 | 正解答 | 検索回答 | 類似度(%) | 検索スコア |
|------|--------|----------|-----------|------------|
| ... | ... | ... | ... | ... |

### 4. フィルタリング機能
- **類似度範囲**: スライダーで最小・最大類似度を指定
- **並び順**: 類似度の昇順/降順でソート
- **CSV出力**: 検証結果をCSVファイルとしてダウンロード

## 技術仕様

### 使用モデル
- **Embedding**: text-embedding-3-small
- **次元数**: 1536

### 環境変数
```bash
OPENAI_API_KEY=your-api-key
QDRANT_URL=http://localhost:6333  # デフォルト
```

### 依存ライブラリ
- streamlit
- pandas
- numpy
- qdrant-client
- openai
- scikit-learn

## UI構成

### サイドバー（左ペイン）
1. **Q/Aデータセット選択**
   - CC News または Livedoor を選択
2. **検証対象コレクション**
   - 選択したデータセットに対応するコレクションを選択
3. **処理オプション**
   - 処理件数制限（デフォルト: 100件）
4. **検証開始ボタン**

### メインエリア（右ペイン）
1. **データ情報**（左カラム）
   - データセット名
   - Q&Aペア総数
   - 選択コレクション
   - 処理対象件数

2. **検証結果サマリー**（右カラム）
   - スコア分布表
   - 平均スコア

3. **詳細結果**
   - フィルタリングコントロール
   - 結果テーブル
   - CSVダウンロードボタン
   - 統計詳細（平均、中央値、標準偏差、最高値）

## 評価指標

### コサイン類似度
- **計算方法**: OpenAI Embeddingを使用してベクトル化し、コサイン類似度を計算
- **範囲**: 0-100%
- **解釈**:
  - 80%以上: 非常に高い一致度
  - 60-80%: 高い一致度
  - 40-60%: 中程度の一致度
  - 40%未満: 低い一致度

### 検索スコア
- Qdrantが返す検索結果の信頼度スコア
- 値が高いほど、クエリとの関連性が高い

## 使用例

### 基本的な使用フロー
1. Streamlitアプリを起動
2. サイドバーでデータセットを選択
3. 検証対象コレクションを選択
4. 処理件数を設定（テスト時は少なめに）
5. 「検証開始」ボタンをクリック
6. 結果を確認し、必要に応じてCSVでダウンロード

### 性能比較の例
同じデータセットで異なるコレクション（LLM、ルール、ハイブリッド）を検証し、平均スコアを比較することで、各手法の性能を評価できる。

## 注意事項

### パフォーマンス
- 大量のデータ処理時はOpenAI APIの呼び出し回数に注意
- 各Q&Aペアで2回のembedding生成（正解答と検索回答）が発生
- 処理件数を制限してテスト実行を推奨

### エラーハンドリング
- Qdrantコレクションが存在しない場合はエラー表示
- OpenAI APIエラー時は該当ペアの類似度を0%として処理継続
- CSVファイルのフォーマットエラー時は読み込み中止

## 関連ファイル
- `a02_make_qa.py` - Q&Aペア生成
- `a03_rag_qa_coverage_improved.py` - カバレッジ分析
- `a10_qa_optimized_hybrid_batch.py` - ハイブリッド方式のQ&A生成
- `a42_qdrant_registration.py` - Qdrantへのデータ登録

## 更新履歴
- 2025-11-13: 初版作成
- 用語統一: 正解答（CSVのanswer）、検索回答（Qdrantの検索結果）
- 2つのデータセット（CC News、Livedoor）に対応
- 詳細結果テーブルに検索回答列を追加